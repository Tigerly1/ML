# -*- coding: utf-8 -*-
"""lab11.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EaFSwKcCrfd85omz-92rRlyK1vO2raKw
"""

import tensorflow as tf
from tensorflow import keras

(X_train, y_train), (X_test, y_test) = tf.keras.datasets.boston_housing.load_data()

print(X_train)

def build_model(n_hidden=1, n_neurons=25, optimizer="sgd", learning_rate=1e5, momentum=0): 
    model = tf.keras.models.Sequential()
    model.add(tf.keras.Input(shape=(X_train[0].shape)))
    for x in range(0, n_hidden):
        model.add(tf.keras.layers.Dense(n_neurons, activation='relu'))
    if optimizer=="sgd":
        optimizer = keras.optimizers.SGD(
        learning_rate=learning_rate, momentum=0.0, nesterov=False, name="SGD")
    elif optimizer=="adam":
        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
    elif optimizer=="nesterov":
        optimizer = keras.optimizers.SGD(
        learning_rate=learning_rate, momentum=0.0, nesterov=True, name="SGD")
    elif optimizer=="momentum":
        optimizer = keras.optimizers.SGD(
        learning_rate=learning_rate, momentum=momentum, nesterov=False, name="SGD")
        
    model.add(tf.keras.layers.Dense(1))
    
    model.compile(loss="MSE",
              optimizer=optimizer,metrics=["MAE"])
   

    return model

import os
root_logdir = os.path.join(os.curdir, "tb_logs")
def get_run_logdir(name, value): 
    import time
    ts = int(time.time())
    run_id = str(ts)+"_"+str(name)+"_"+str(value) 
    return os.path.join(root_logdir, run_id)

es = tf.keras.callbacks.EarlyStopping(patience=10, min_delta=1.0,verbose=1)

import numpy as np
tf.keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)

lr = []
for x in [10e-6,10e-5,10e-4]:
    model = build_model(1,25,"sgd",x)
    tensorboard_cb = tf.keras.callbacks.TensorBoard(get_run_logdir("lr",x))
    model.fit(X_train, y_train, epochs=100,  callbacks=[tensorboard_cb,es], validation_split=0.1)
    ev = model.evaluate(X_test, y_test)
    lr.append((x,ev[0],ev[1]))
import pickle
pickle.dump(lr, open("lr.pkl", "wb"))

hl = []
for x in [0,1,2,3]:
    model = build_model(x,25,"sgd",10e-5)
    tensorboard_cb = tf.keras.callbacks.TensorBoard(get_run_logdir("hl",x))
    model.fit(X_train, y_train, epochs=100,  callbacks=[tensorboard_cb,es], validation_split=0.1) 
    ev = model.evaluate(X_test, y_test)
    hl.append((x,ev[0],ev[1]))
import pickle
pickle.dump(hl, open("hl.pkl", "wb"))

nn = []
for x in [5,25,125]:
    model = build_model(1,x,"sgd",10e-5)
    tensorboard_cb = tf.keras.callbacks.TensorBoard(get_run_logdir("nn",x))
    model.fit(X_train, y_train, epochs=100,  callbacks=[tensorboard_cb,es], validation_split=0.1)
    ev = model.evaluate(X_test, y_test)
    nn.append((x,ev[0],ev[1]))
import pickle
pickle.dump(nn, open("nn.pkl", "wb"))

opt = []
for x in ["sgd","nesterov","momentum", "adam"]:
    model = build_model(1,25,x,10e-5)
    tensorboard_cb = tf.keras.callbacks.TensorBoard(get_run_logdir("opt",x))
    model.fit(X_train, y_train, epochs=100,  callbacks=[tensorboard_cb,es], validation_split=0.1)
    ev = model.evaluate(X_test, y_test)
    opt.append((x,ev[0],ev[1]))
import pickle
pickle.dump(opt, open("opt.pkl", "wb"))

mom = []
for x in [0.1,0.5,0.9]:
    model = build_model(1,25,"sgd",10e-5,x)
    tensorboard_cb = tf.keras.callbacks.TensorBoard(get_run_logdir("mom",x))
    model.fit(X_train, y_train, epochs=100,  callbacks=[tensorboard_cb,es], validation_split=0.1)
    ev = model.evaluate(X_test, y_test)
    opt.append((x,ev[0],ev[1]))
import pickle
pickle.dump(mom, open("mom.pkl", "wb"))

param_distribs = {
  "model__n_hidden": [0,1,2,3],
  "model__n_neurons": [5,25,125],
  "model__learning_rate": [10e-6,10e-5,10e-4],
  "model__optimizer": ["sgd","nesterov","momentum", "adam"],
  "model__momentum": [0.1,0.5,0.9]
}

import scikeras
from scikeras.wrappers import KerasRegressor
es = tf.keras.callbacks.EarlyStopping(patience=10, min_delta=1.0, verbose=1)
keras_reg = KerasRegressor(build_model, callbacks=[es])

from sklearn.model_selection import RandomizedSearchCV
rnd_search_cv = RandomizedSearchCV(keras_reg,
param_distribs,
n_iter=30,
cv=3,
verbose=2)
rnd_search_cv.fit(X_train, y_train, epochs=100, validation_split=0.1)

rnd_search_cv.best_params_

pickle.dump(rnd_search_cv.best_params_, open("rnd_search.pkl", "wb"))