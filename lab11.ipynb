{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b10c20e1-b0af-4a4e-93e1-76b0948d41b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b10c20e1-b0af-4a4e-93e1-76b0948d41b7",
        "outputId": "60999bb4-5984-41a3-d2c5-c07684eb0806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 0us/step\n",
            "65536/57026 [==================================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.boston_housing.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "05326002-c4b5-4eca-8d7c-9ce8665ffce3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05326002-c4b5-4eca-8d7c-9ce8665ffce3",
        "outputId": "b49753a2-d7b4-4a0e-e949-a5d2e53dd4ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.23247e+00 0.00000e+00 8.14000e+00 ... 2.10000e+01 3.96900e+02\n",
            "  1.87200e+01]\n",
            " [2.17700e-02 8.25000e+01 2.03000e+00 ... 1.47000e+01 3.95380e+02\n",
            "  3.11000e+00]\n",
            " [4.89822e+00 0.00000e+00 1.81000e+01 ... 2.02000e+01 3.75520e+02\n",
            "  3.26000e+00]\n",
            " ...\n",
            " [3.46600e-02 3.50000e+01 6.06000e+00 ... 1.69000e+01 3.62250e+02\n",
            "  7.83000e+00]\n",
            " [2.14918e+00 0.00000e+00 1.95800e+01 ... 1.47000e+01 2.61950e+02\n",
            "  1.57900e+01]\n",
            " [1.43900e-02 6.00000e+01 2.93000e+00 ... 1.56000e+01 3.76700e+02\n",
            "  4.38000e+00]]\n"
          ]
        }
      ],
      "source": [
        "print(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "39ad682c-4b25-4003-b271-81ea5d1e9e0b",
      "metadata": {
        "id": "39ad682c-4b25-4003-b271-81ea5d1e9e0b"
      },
      "outputs": [],
      "source": [
        "def build_model(n_hidden=1, n_neurons=25, optimizer=\"sgd\", learning_rate=1e5, momentum=0): \n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.Input(shape=(X_train[0].shape)))\n",
        "    for x in range(0, n_hidden):\n",
        "        model.add(tf.keras.layers.Dense(n_neurons, activation='relu'))\n",
        "    if optimizer==\"sgd\":\n",
        "        optimizer = keras.optimizers.SGD(\n",
        "        learning_rate=learning_rate, momentum=0.0, nesterov=False, name=\"SGD\")\n",
        "    elif optimizer==\"adam\":\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    elif optimizer==\"nesterov\":\n",
        "        optimizer = keras.optimizers.SGD(\n",
        "        learning_rate=learning_rate, momentum=0.0, nesterov=True, name=\"SGD\")\n",
        "    elif optimizer==\"momentum\":\n",
        "        optimizer = keras.optimizers.SGD(\n",
        "        learning_rate=learning_rate, momentum=momentum, nesterov=False, name=\"SGD\")\n",
        "        \n",
        "    model.add(tf.keras.layers.Dense(1))\n",
        "    \n",
        "    model.compile(loss=\"MSE\",\n",
        "              optimizer=optimizer,metrics=[\"MAE\"])\n",
        "   \n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6ba39e63-5a75-4129-b28b-ae0e15267c08",
      "metadata": {
        "id": "6ba39e63-5a75-4129-b28b-ae0e15267c08"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "root_logdir = os.path.join(os.curdir, \"tb_logs\")\n",
        "def get_run_logdir(name, value): \n",
        "    import time\n",
        "    ts = int(time.time())\n",
        "    run_id = str(ts)+\"_\"+str(name)+\"_\"+str(value) \n",
        "    return os.path.join(root_logdir, run_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "293ba554-93a5-49d9-8180-767da67ec022",
      "metadata": {
        "id": "293ba554-93a5-49d9-8180-767da67ec022"
      },
      "outputs": [],
      "source": [
        "es = tf.keras.callbacks.EarlyStopping(patience=10, min_delta=1.0,verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c1d1f447-c43d-4d32-960b-540226a2661c",
      "metadata": {
        "id": "c1d1f447-c43d-4d32-960b-540226a2661c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "tf.keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2e622ceb-fcac-4ec8-8c9a-2bea235a9442",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e622ceb-fcac-4ec8-8c9a-2bea235a9442",
        "outputId": "040a9e65-049a-406c-d3f8-71b9ac70d96a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "12/12 [==============================] - 1s 17ms/step - loss: 117101.7812 - MAE: 123.7935 - val_loss: 108.1399 - val_MAE: 8.5337\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 163.8253 - MAE: 9.5988 - val_loss: 68.6999 - val_MAE: 6.7683\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 113.2871 - MAE: 7.8936 - val_loss: 49.6265 - val_MAE: 5.5180\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 97.5250 - MAE: 7.2386 - val_loss: 44.5197 - val_MAE: 5.2958\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 91.5391 - MAE: 7.0076 - val_loss: 42.7563 - val_MAE: 5.2981\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 87.4378 - MAE: 7.0010 - val_loss: 42.5495 - val_MAE: 5.0830\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 86.9827 - MAE: 6.7954 - val_loss: 41.1674 - val_MAE: 5.1446\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 85.6662 - MAE: 6.7517 - val_loss: 44.3108 - val_MAE: 5.4583\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 87.2038 - MAE: 6.9364 - val_loss: 40.8578 - val_MAE: 5.0159\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 85.3688 - MAE: 6.7019 - val_loss: 44.2814 - val_MAE: 5.4409\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 86.3176 - MAE: 6.8699 - val_loss: 40.8691 - val_MAE: 5.1381\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 86.3014 - MAE: 6.7988 - val_loss: 46.5182 - val_MAE: 5.6370\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 86.0942 - MAE: 6.7629 - val_loss: 43.3289 - val_MAE: 5.3657\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 84.8896 - MAE: 6.8204 - val_loss: 40.6521 - val_MAE: 5.0992\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 84.5033 - MAE: 6.7824 - val_loss: 40.2261 - val_MAE: 5.0088\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 85.2646 - MAE: 6.7375 - val_loss: 41.3100 - val_MAE: 4.9884\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 84.2253 - MAE: 6.5342 - val_loss: 74.1953 - val_MAE: 7.3707\n",
            "Epoch 17: early stopping\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 96.4782 - MAE: 8.2673\n",
            "Epoch 1/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 112177.9609 - MAE: 125.7833 - val_loss: 512.0266 - val_MAE: 21.6644\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 600.7207 - MAE: 22.6158 - val_loss: 509.7014 - val_MAE: 21.6107\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 598.2958 - MAE: 22.5618 - val_loss: 507.3871 - val_MAE: 21.5571\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 595.8815 - MAE: 22.5082 - val_loss: 505.0604 - val_MAE: 21.5030\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 593.4545 - MAE: 22.4543 - val_loss: 502.7385 - val_MAE: 21.4490\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 591.0327 - MAE: 22.3996 - val_loss: 500.4478 - val_MAE: 21.3955\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 588.6422 - MAE: 22.3469 - val_loss: 498.1533 - val_MAE: 21.3418\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 586.2479 - MAE: 22.2935 - val_loss: 495.8609 - val_MAE: 21.2881\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 583.8571 - MAE: 22.2399 - val_loss: 493.6149 - val_MAE: 21.2352\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 581.5112 - MAE: 22.1868 - val_loss: 491.3393 - val_MAE: 21.1816\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 579.1368 - MAE: 22.1328 - val_loss: 489.0913 - val_MAE: 21.1285\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 576.7904 - MAE: 22.0797 - val_loss: 486.8587 - val_MAE: 21.0756\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 574.4598 - MAE: 22.0273 - val_loss: 484.6459 - val_MAE: 21.0230\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 572.1483 - MAE: 21.9745 - val_loss: 482.4273 - val_MAE: 20.9702\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 569.8316 - MAE: 21.9218 - val_loss: 480.2237 - val_MAE: 20.9176\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 567.5311 - MAE: 21.8694 - val_loss: 478.0438 - val_MAE: 20.8654\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 565.2524 - MAE: 21.8177 - val_loss: 475.8259 - val_MAE: 20.8122\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 562.9379 - MAE: 21.7642 - val_loss: 473.6757 - val_MAE: 20.7604\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 560.6900 - MAE: 21.7123 - val_loss: 471.5013 - val_MAE: 20.7080\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 558.4186 - MAE: 21.6599 - val_loss: 469.3616 - val_MAE: 20.6563\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 556.1818 - MAE: 21.6082 - val_loss: 467.1881 - val_MAE: 20.6036\n",
            "Epoch 22/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 553.9112 - MAE: 21.5563 - val_loss: 465.0553 - val_MAE: 20.5518\n",
            "Epoch 23/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 551.6826 - MAE: 21.5042 - val_loss: 462.9467 - val_MAE: 20.5004\n",
            "Epoch 24/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 549.4780 - MAE: 21.4524 - val_loss: 460.8335 - val_MAE: 20.4488\n",
            "Epoch 25/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 547.2690 - MAE: 21.4010 - val_loss: 458.7302 - val_MAE: 20.3973\n",
            "Epoch 26/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 545.0695 - MAE: 21.3495 - val_loss: 456.6427 - val_MAE: 20.3461\n",
            "Epoch 27/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 542.8873 - MAE: 21.2986 - val_loss: 454.5813 - val_MAE: 20.2954\n",
            "Epoch 28/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 540.7307 - MAE: 21.2478 - val_loss: 452.4994 - val_MAE: 20.2440\n",
            "Epoch 29/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 538.5542 - MAE: 21.1963 - val_loss: 450.4464 - val_MAE: 20.1932\n",
            "Epoch 30/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 536.4068 - MAE: 21.1462 - val_loss: 448.4101 - val_MAE: 20.1428\n",
            "Epoch 31/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 534.2756 - MAE: 21.0959 - val_loss: 446.3604 - val_MAE: 20.0918\n",
            "Epoch 32/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 532.1312 - MAE: 21.0448 - val_loss: 444.3350 - val_MAE: 20.0413\n",
            "Epoch 33/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 530.0120 - MAE: 20.9944 - val_loss: 442.3171 - val_MAE: 19.9909\n",
            "Epoch 34/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 527.8992 - MAE: 20.9438 - val_loss: 440.2874 - val_MAE: 19.9401\n",
            "Epoch 35/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 525.7756 - MAE: 20.8930 - val_loss: 438.2813 - val_MAE: 19.8897\n",
            "Epoch 36/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 523.6766 - MAE: 20.8425 - val_loss: 436.3045 - val_MAE: 19.8400\n",
            "Epoch 37/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 521.6061 - MAE: 20.7930 - val_loss: 434.3184 - val_MAE: 19.7899\n",
            "Epoch 38/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 519.5277 - MAE: 20.7427 - val_loss: 432.3637 - val_MAE: 19.7404\n",
            "Epoch 39/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 517.4799 - MAE: 20.6936 - val_loss: 430.4005 - val_MAE: 19.6906\n",
            "Epoch 40/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 515.4249 - MAE: 20.6438 - val_loss: 428.4732 - val_MAE: 19.6416\n",
            "Epoch 41/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 513.4052 - MAE: 20.5944 - val_loss: 426.5143 - val_MAE: 19.5917\n",
            "Epoch 42/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 511.3537 - MAE: 20.5442 - val_loss: 424.5862 - val_MAE: 19.5424\n",
            "Epoch 43/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 509.3336 - MAE: 20.4957 - val_loss: 422.6642 - val_MAE: 19.4932\n",
            "Epoch 44/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 507.3201 - MAE: 20.4470 - val_loss: 420.7590 - val_MAE: 19.4443\n",
            "Epoch 45/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 505.3236 - MAE: 20.3972 - val_loss: 418.8598 - val_MAE: 19.3954\n",
            "Epoch 46/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 503.3339 - MAE: 20.3484 - val_loss: 416.9846 - val_MAE: 19.3470\n",
            "Epoch 47/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 501.3679 - MAE: 20.2999 - val_loss: 415.0927 - val_MAE: 19.2980\n",
            "Epoch 48/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 499.3853 - MAE: 20.2517 - val_loss: 413.2446 - val_MAE: 19.2501\n",
            "Epoch 49/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 497.4474 - MAE: 20.2033 - val_loss: 411.3893 - val_MAE: 19.2018\n",
            "Epoch 50/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 495.5021 - MAE: 20.1557 - val_loss: 409.5348 - val_MAE: 19.1535\n",
            "Epoch 51/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 493.5568 - MAE: 20.1071 - val_loss: 407.6826 - val_MAE: 19.1050\n",
            "Epoch 52/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 491.6145 - MAE: 20.0589 - val_loss: 405.8469 - val_MAE: 19.0569\n",
            "Epoch 53/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 489.6894 - MAE: 20.0104 - val_loss: 404.0177 - val_MAE: 19.0089\n",
            "Epoch 54/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 487.7704 - MAE: 19.9626 - val_loss: 402.1857 - val_MAE: 18.9606\n",
            "Epoch 55/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 485.8479 - MAE: 19.9140 - val_loss: 400.3729 - val_MAE: 18.9128\n",
            "Epoch 56/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 483.9464 - MAE: 19.8662 - val_loss: 398.5757 - val_MAE: 18.8652\n",
            "Epoch 57/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 482.0597 - MAE: 19.8190 - val_loss: 396.7721 - val_MAE: 18.8173\n",
            "Epoch 58/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 480.1671 - MAE: 19.7710 - val_loss: 394.9947 - val_MAE: 18.7701\n",
            "Epoch 59/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 478.3014 - MAE: 19.7242 - val_loss: 393.2136 - val_MAE: 18.7226\n",
            "Epoch 60/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 476.4312 - MAE: 19.6765 - val_loss: 391.4424 - val_MAE: 18.6752\n",
            "Epoch 61/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 474.5721 - MAE: 19.6293 - val_loss: 389.6817 - val_MAE: 18.6280\n",
            "Epoch 62/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 472.7233 - MAE: 19.5820 - val_loss: 387.9430 - val_MAE: 18.5813\n",
            "Epoch 63/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 470.8971 - MAE: 19.5356 - val_loss: 386.1934 - val_MAE: 18.5341\n",
            "Epoch 64/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 469.0601 - MAE: 19.4880 - val_loss: 384.4697 - val_MAE: 18.4876\n",
            "Epoch 65/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 467.2495 - MAE: 19.4414 - val_loss: 382.7586 - val_MAE: 18.4412\n",
            "Epoch 66/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 465.4508 - MAE: 19.3955 - val_loss: 381.0303 - val_MAE: 18.3943\n",
            "Epoch 67/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 463.6353 - MAE: 19.3491 - val_loss: 379.3206 - val_MAE: 18.3478\n",
            "Epoch 68/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 461.8382 - MAE: 19.3022 - val_loss: 377.6075 - val_MAE: 18.3010\n",
            "Epoch 69/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 460.0376 - MAE: 19.2554 - val_loss: 375.9084 - val_MAE: 18.2546\n",
            "Epoch 70/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 458.2519 - MAE: 19.2097 - val_loss: 374.2228 - val_MAE: 18.2083\n",
            "Epoch 71/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 456.4803 - MAE: 19.1631 - val_loss: 372.5625 - val_MAE: 18.1627\n",
            "Epoch 72/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 454.7347 - MAE: 19.1169 - val_loss: 370.9090 - val_MAE: 18.1171\n",
            "Epoch 73/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 452.9958 - MAE: 19.0714 - val_loss: 369.2496 - val_MAE: 18.0712\n",
            "Epoch 74/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 451.2508 - MAE: 19.0256 - val_loss: 367.6054 - val_MAE: 18.0257\n",
            "Epoch 75/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 449.5216 - MAE: 18.9802 - val_loss: 365.9727 - val_MAE: 17.9804\n",
            "Epoch 76/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 447.8034 - MAE: 18.9349 - val_loss: 364.3339 - val_MAE: 17.9347\n",
            "Epoch 77/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 446.0797 - MAE: 18.8891 - val_loss: 362.7161 - val_MAE: 17.8896\n",
            "Epoch 78/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 444.3771 - MAE: 18.8440 - val_loss: 361.0944 - val_MAE: 17.8442\n",
            "Epoch 79/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 442.6711 - MAE: 18.7993 - val_loss: 359.4981 - val_MAE: 17.7994\n",
            "Epoch 80/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 440.9903 - MAE: 18.7545 - val_loss: 357.8953 - val_MAE: 17.7543\n",
            "Epoch 81/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 439.3030 - MAE: 18.7095 - val_loss: 356.2913 - val_MAE: 17.7091\n",
            "Epoch 82/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 437.6150 - MAE: 18.6642 - val_loss: 354.7058 - val_MAE: 17.6643\n",
            "Epoch 83/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 435.9453 - MAE: 18.6192 - val_loss: 353.1321 - val_MAE: 17.6197\n",
            "Epoch 84/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 434.2880 - MAE: 18.5749 - val_loss: 351.5617 - val_MAE: 17.5750\n",
            "Epoch 85/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 432.6340 - MAE: 18.5303 - val_loss: 349.9906 - val_MAE: 17.5303\n",
            "Epoch 86/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 430.9804 - MAE: 18.4852 - val_loss: 348.4514 - val_MAE: 17.4863\n",
            "Epoch 87/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 429.3579 - MAE: 18.4418 - val_loss: 346.9059 - val_MAE: 17.4421\n",
            "Epoch 88/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 427.7294 - MAE: 18.3973 - val_loss: 345.3674 - val_MAE: 17.3979\n",
            "Epoch 89/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 426.1083 - MAE: 18.3533 - val_loss: 343.8439 - val_MAE: 17.3541\n",
            "Epoch 90/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 424.5033 - MAE: 18.3099 - val_loss: 342.3343 - val_MAE: 17.3105\n",
            "Epoch 91/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 422.9113 - MAE: 18.2661 - val_loss: 340.8154 - val_MAE: 17.2666\n",
            "Epoch 92/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 421.3098 - MAE: 18.2220 - val_loss: 339.2952 - val_MAE: 17.2225\n",
            "Epoch 93/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 419.7073 - MAE: 18.1776 - val_loss: 337.7924 - val_MAE: 17.1788\n",
            "Epoch 94/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 418.1237 - MAE: 18.1344 - val_loss: 336.3228 - val_MAE: 17.1360\n",
            "Epoch 95/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 416.5729 - MAE: 18.0914 - val_loss: 334.8378 - val_MAE: 17.0926\n",
            "Epoch 96/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 415.0070 - MAE: 18.0485 - val_loss: 333.3613 - val_MAE: 17.0494\n",
            "Epoch 97/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 413.4492 - MAE: 18.0049 - val_loss: 331.8820 - val_MAE: 17.0060\n",
            "Epoch 98/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 411.8880 - MAE: 17.9618 - val_loss: 330.3976 - val_MAE: 16.9622\n",
            "Epoch 99/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 410.3225 - MAE: 17.9186 - val_loss: 328.9483 - val_MAE: 16.9195\n",
            "Epoch 100/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 408.7923 - MAE: 17.8757 - val_loss: 327.4956 - val_MAE: 16.8765\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 423.1082 - MAE: 18.4354\n",
            "Epoch 1/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 1821524754432.0000 - MAE: 402203.3750 - val_loss: 76737976.0000 - val_MAE: 8760.0186\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 75152024.0000 - MAE: 8668.8301 - val_loss: 73138352.0000 - val_MAE: 8552.0928\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 71626416.0000 - MAE: 8463.0518 - val_loss: 69707592.0000 - val_MAE: 8349.1045\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 68266208.0000 - MAE: 8262.1523 - val_loss: 66437864.0000 - val_MAE: 8150.9399\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 65063708.0000 - MAE: 8066.0283 - val_loss: 63321524.0000 - val_MAE: 7957.4795\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 62011472.0000 - MAE: 7874.5669 - val_loss: 60351300.0000 - val_MAE: 7768.6074\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 59102348.0000 - MAE: 7687.6323 - val_loss: 57520472.0000 - val_MAE: 7584.2227\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 56329760.0000 - MAE: 7505.1445 - val_loss: 54822448.0000 - val_MAE: 7404.2148\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 53687252.0000 - MAE: 7326.9897 - val_loss: 52250888.0000 - val_MAE: 7228.4746\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 51168616.0000 - MAE: 7153.0625 - val_loss: 49800080.0000 - val_MAE: 7056.9141\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 48768252.0000 - MAE: 6983.2729 - val_loss: 47464176.0000 - val_MAE: 6889.4224\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 46480432.0000 - MAE: 6817.5049 - val_loss: 45237836.0000 - val_MAE: 6725.9048\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 44299940.0000 - MAE: 6655.6680 - val_loss: 43115900.0000 - val_MAE: 6566.2666\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 42221708.0000 - MAE: 6497.6777 - val_loss: 41093560.0000 - val_MAE: 6410.4224\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 40241016.0000 - MAE: 6343.4380 - val_loss: 39166072.0000 - val_MAE: 6258.2769\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 38353232.0000 - MAE: 6192.8569 - val_loss: 37328964.0000 - val_MAE: 6109.7397\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 36553980.0000 - MAE: 6045.8462 - val_loss: 35578168.0000 - val_MAE: 5964.7402\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 34839252.0000 - MAE: 5902.3418 - val_loss: 33909328.0000 - val_MAE: 5823.1680\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 33204824.0000 - MAE: 5762.2314 - val_loss: 32318880.0000 - val_MAE: 5684.9663\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 31647162.0000 - MAE: 5625.4526 - val_loss: 30802976.0000 - val_MAE: 5550.0391\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 30162516.0000 - MAE: 5491.9155 - val_loss: 29358290.0000 - val_MAE: 5418.3257\n",
            "Epoch 22/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28747620.0000 - MAE: 5361.5508 - val_loss: 27981294.0000 - val_MAE: 5289.7305\n",
            "Epoch 23/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 27399026.0000 - MAE: 5234.2837 - val_loss: 26668844.0000 - val_MAE: 5164.1851\n",
            "Epoch 24/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 26113664.0000 - MAE: 5110.0366 - val_loss: 25418010.0000 - val_MAE: 5041.6235\n",
            "Epoch 25/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 24888644.0000 - MAE: 4988.7354 - val_loss: 24225838.0000 - val_MAE: 4921.9712\n",
            "Epoch 26/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 23721076.0000 - MAE: 4870.3164 - val_loss: 23089582.0000 - val_MAE: 4805.1577\n",
            "Epoch 27/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 22608282.0000 - MAE: 4754.7036 - val_loss: 22006588.0000 - val_MAE: 4691.1138\n",
            "Epoch 28/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 21547652.0000 - MAE: 4641.8345 - val_loss: 20974460.0000 - val_MAE: 4579.7837\n",
            "Epoch 29/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 20536840.0000 - MAE: 4531.6538 - val_loss: 19990710.0000 - val_MAE: 4471.0923\n",
            "Epoch 30/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 19573414.0000 - MAE: 4424.0757 - val_loss: 19053088.0000 - val_MAE: 4364.9795\n",
            "Epoch 31/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 18655176.0000 - MAE: 4319.0537 - val_loss: 18159498.0000 - val_MAE: 4261.3911\n",
            "Epoch 32/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 17780050.0000 - MAE: 4216.5347 - val_loss: 17307788.0000 - val_MAE: 4160.2578\n",
            "Epoch 33/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 16945956.0000 - MAE: 4116.4429 - val_loss: 16496045.0000 - val_MAE: 4061.5271\n",
            "Epoch 34/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 16150999.0000 - MAE: 4018.7310 - val_loss: 15722410.0000 - val_MAE: 3965.1440\n",
            "Epoch 35/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 15393369.0000 - MAE: 3923.3413 - val_loss: 14985038.0000 - val_MAE: 3871.0457\n",
            "Epoch 36/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 14671251.0000 - MAE: 3830.2131 - val_loss: 14282212.0000 - val_MAE: 3779.1760\n",
            "Epoch 37/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13982977.0000 - MAE: 3739.2878 - val_loss: 13612394.0000 - val_MAE: 3689.4919\n",
            "Epoch 38/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13327027.0000 - MAE: 3650.5303 - val_loss: 12973954.0000 - val_MAE: 3601.9319\n",
            "Epoch 39/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12701812.0000 - MAE: 3563.8684 - val_loss: 12365492.0000 - val_MAE: 3516.4543\n",
            "Epoch 40/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12105954.0000 - MAE: 3479.2720 - val_loss: 11785525.0000 - val_MAE: 3432.9993\n",
            "Epoch 41/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11538008.0000 - MAE: 3396.6812 - val_loss: 11232832.0000 - val_MAE: 3351.5354\n",
            "Epoch 42/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10996772.0000 - MAE: 3316.0598 - val_loss: 10706028.0000 - val_MAE: 3272.0000\n",
            "Epoch 43/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10480895.0000 - MAE: 3237.3362 - val_loss: 10203941.0000 - val_MAE: 3194.3545\n",
            "Epoch 44/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9989223.0000 - MAE: 3160.4849 - val_loss: 9725391.0000 - val_MAE: 3118.5491\n",
            "Epoch 45/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9520603.0000 - MAE: 3085.4700 - val_loss: 9269293.0000 - val_MAE: 3044.5447\n",
            "Epoch 46/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9073974.0000 - MAE: 3012.2268 - val_loss: 8834568.0000 - val_MAE: 2972.2930\n",
            "Epoch 47/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8648277.0000 - MAE: 2940.7212 - val_loss: 8420271.0000 - val_MAE: 2901.7629\n",
            "Epoch 48/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8242585.0000 - MAE: 2870.9104 - val_loss: 8025353.5000 - val_MAE: 2832.8979\n",
            "Epoch 49/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7855877.5000 - MAE: 2802.7603 - val_loss: 7648988.0000 - val_MAE: 2765.6726\n",
            "Epoch 50/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7487335.5000 - MAE: 2736.2207 - val_loss: 7290285.5000 - val_MAE: 2700.0449\n",
            "Epoch 51/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7136093.0000 - MAE: 2671.2715 - val_loss: 6948415.0000 - val_MAE: 2635.9768\n",
            "Epoch 52/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6801336.0000 - MAE: 2607.8613 - val_loss: 6622568.5000 - val_MAE: 2573.4268\n",
            "Epoch 53/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 6482273.5000 - MAE: 2545.9612 - val_loss: 6312009.5000 - val_MAE: 2512.3628\n",
            "Epoch 54/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6178182.0000 - MAE: 2485.5232 - val_loss: 6016030.5000 - val_MAE: 2452.7512\n",
            "Epoch 55/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5888368.0000 - MAE: 2426.5291 - val_loss: 5733919.5000 - val_MAE: 2394.5518\n",
            "Epoch 56/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5612138.5000 - MAE: 2368.9290 - val_loss: 5465034.0000 - val_MAE: 2337.7322\n",
            "Epoch 57/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5348862.0000 - MAE: 2312.6924 - val_loss: 5208780.5000 - val_MAE: 2282.2661\n",
            "Epoch 58/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5097952.5000 - MAE: 2257.7988 - val_loss: 4964523.0000 - val_MAE: 2228.1116\n",
            "Epoch 59/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4858794.0000 - MAE: 2204.1978 - val_loss: 4731736.0000 - val_MAE: 2175.2458\n",
            "Epoch 60/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4630870.0000 - MAE: 2151.8796 - val_loss: 4509867.5000 - val_MAE: 2123.6350\n",
            "Epoch 61/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4413636.0000 - MAE: 2100.7988 - val_loss: 4298402.0000 - val_MAE: 2073.2488\n",
            "Epoch 62/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4206590.5000 - MAE: 2050.9324 - val_loss: 4096839.0000 - val_MAE: 2024.0544\n",
            "Epoch 63/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4009244.5000 - MAE: 2002.2429 - val_loss: 3904752.5000 - val_MAE: 1976.0339\n",
            "Epoch 64/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3821176.5000 - MAE: 1954.7222 - val_loss: 3721655.7500 - val_MAE: 1929.1482\n",
            "Epoch 65/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3641913.5000 - MAE: 1908.3196 - val_loss: 3547141.5000 - val_MAE: 1883.3743\n",
            "Epoch 66/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3471056.5000 - MAE: 1863.0138 - val_loss: 3380838.7500 - val_MAE: 1838.6940\n",
            "Epoch 67/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3308240.0000 - MAE: 1818.7885 - val_loss: 3222325.7500 - val_MAE: 1795.0718\n",
            "Epoch 68/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3153051.7500 - MAE: 1775.6202 - val_loss: 3071257.2500 - val_MAE: 1752.4882\n",
            "Epoch 69/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3005153.2500 - MAE: 1733.4757 - val_loss: 2927269.0000 - val_MAE: 1710.9138\n",
            "Epoch 70/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2864189.2500 - MAE: 1692.3218 - val_loss: 2790028.2500 - val_MAE: 1670.3248\n",
            "Epoch 71/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2729832.0000 - MAE: 1652.1553 - val_loss: 2659209.2500 - val_MAE: 1630.6951\n",
            "Epoch 72/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2601764.0000 - MAE: 1612.9396 - val_loss: 2534526.2500 - val_MAE: 1592.0061\n",
            "Epoch 73/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2479704.7500 - MAE: 1574.6494 - val_loss: 2415702.7500 - val_MAE: 1554.2394\n",
            "Epoch 74/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2363382.7500 - MAE: 1537.2714 - val_loss: 2302445.2500 - val_MAE: 1517.3669\n",
            "Epoch 75/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2252512.0000 - MAE: 1500.7778 - val_loss: 2194497.2500 - val_MAE: 1481.3691\n",
            "Epoch 76/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2146840.7500 - MAE: 1465.1498 - val_loss: 2091622.8750 - val_MAE: 1446.2296\n",
            "Epoch 77/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 2046137.1250 - MAE: 1430.3749 - val_loss: 1993562.7500 - val_MAE: 1411.9207\n",
            "Epoch 78/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1950148.2500 - MAE: 1396.4198 - val_loss: 1900111.3750 - val_MAE: 1378.4299\n",
            "Epoch 79/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1858671.2500 - MAE: 1363.2668 - val_loss: 1811028.6250 - val_MAE: 1345.7288\n",
            "Epoch 80/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1771473.1250 - MAE: 1330.9038 - val_loss: 1726134.5000 - val_MAE: 1313.8081\n",
            "Epoch 81/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1688376.1250 - MAE: 1299.3109 - val_loss: 1645227.8750 - val_MAE: 1282.6477\n",
            "Epoch 82/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1609183.0000 - MAE: 1268.4724 - val_loss: 1568107.7500 - val_MAE: 1252.2240\n",
            "Epoch 83/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1533697.6250 - MAE: 1238.3644 - val_loss: 1494600.6250 - val_MAE: 1222.5211\n",
            "Epoch 84/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1461750.6250 - MAE: 1208.9653 - val_loss: 1424544.2500 - val_MAE: 1193.5248\n",
            "Epoch 85/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1393182.6250 - MAE: 1180.2681 - val_loss: 1357778.5000 - val_MAE: 1165.2192\n",
            "Epoch 86/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1327836.0000 - MAE: 1152.2573 - val_loss: 1294127.8750 - val_MAE: 1137.5786\n",
            "Epoch 87/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1265539.7500 - MAE: 1124.8970 - val_loss: 1233470.5000 - val_MAE: 1110.5979\n",
            "Epoch 88/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1206174.3750 - MAE: 1098.1969 - val_loss: 1175658.0000 - val_MAE: 1084.2581\n",
            "Epoch 89/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1149594.3750 - MAE: 1072.1270 - val_loss: 1120552.3750 - val_MAE: 1058.5413\n",
            "Epoch 90/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1095664.3750 - MAE: 1046.6710 - val_loss: 1068026.2500 - val_MAE: 1033.4329\n",
            "Epoch 91/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1044260.8750 - MAE: 1021.8245 - val_loss: 1017973.4375 - val_MAE: 1008.9255\n",
            "Epoch 92/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 995278.7500 - MAE: 997.5714 - val_loss: 970272.7500 - val_MAE: 985.0026\n",
            "Epoch 93/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 948598.7500 - MAE: 973.8981 - val_loss: 924802.7500 - val_MAE: 961.6444\n",
            "Epoch 94/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 904102.8750 - MAE: 950.7765 - val_loss: 881450.7500 - val_MAE: 938.8333\n",
            "Epoch 95/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 861681.1250 - MAE: 928.2028 - val_loss: 840143.4375 - val_MAE: 916.5700\n",
            "Epoch 96/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 821261.3125 - MAE: 906.1641 - val_loss: 800772.8750 - val_MAE: 894.8353\n",
            "Epoch 97/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 782737.3750 - MAE: 884.6575 - val_loss: 763253.4375 - val_MAE: 873.6193\n",
            "Epoch 98/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 746025.8750 - MAE: 863.6573 - val_loss: 727499.0625 - val_MAE: 852.9105\n",
            "Epoch 99/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 711042.0000 - MAE: 843.1575 - val_loss: 693407.6250 - val_MAE: 832.6854\n",
            "Epoch 100/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 677686.5000 - MAE: 823.1422 - val_loss: 660919.6250 - val_MAE: 812.9433\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 658428.0000 - MAE: 811.3845\n"
          ]
        }
      ],
      "source": [
        "lr = []\n",
        "for x in [10e-6,10e-5,10e-4]:\n",
        "    model = build_model(1,25,\"sgd\",x)\n",
        "    tensorboard_cb = tf.keras.callbacks.TensorBoard(get_run_logdir(\"lr\",x))\n",
        "    model.fit(X_train, y_train, epochs=100,  callbacks=[tensorboard_cb,es], validation_split=0.1)\n",
        "    ev = model.evaluate(X_test, y_test)\n",
        "    lr.append((x,ev[0],ev[1]))\n",
        "import pickle\n",
        "pickle.dump(lr, open(\"lr.pkl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "fc64b74b-1b0d-4839-b7e6-f0d2095c940d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc64b74b-1b0d-4839-b7e6-f0d2095c940d",
        "outputId": "35125861-c018-4d6e-ff7c-93d0cfb28cb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: inf - MAE: 462933731421691314176.0000 - val_loss: inf - val_MAE: 967319228821248132775936.0000\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "4/4 [==============================] - 0s 3ms/step - loss: nan - MAE: nan\n",
            "Epoch 1/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 1185043.8750 - MAE: 405.3778 - val_loss: 526.3845 - val_MAE: 21.9933\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 615.6886 - MAE: 22.9444 - val_loss: 523.9895 - val_MAE: 21.9388\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 613.1924 - MAE: 22.8895 - val_loss: 521.6056 - val_MAE: 21.8844\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 610.7071 - MAE: 22.8351 - val_loss: 519.2092 - val_MAE: 21.8296\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 608.2090 - MAE: 22.7804 - val_loss: 516.8179 - val_MAE: 21.7747\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 605.7165 - MAE: 22.7250 - val_loss: 514.4584 - val_MAE: 21.7205\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 603.2559 - MAE: 22.6715 - val_loss: 512.0953 - val_MAE: 21.6660\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 600.7915 - MAE: 22.6173 - val_loss: 509.7344 - val_MAE: 21.6115\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 598.3309 - MAE: 22.5629 - val_loss: 507.4208 - val_MAE: 21.5579\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 595.9161 - MAE: 22.5091 - val_loss: 505.0773 - val_MAE: 21.5034\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 593.4724 - MAE: 22.4543 - val_loss: 502.7621 - val_MAE: 21.4495\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 591.0574 - MAE: 22.4005 - val_loss: 500.4626 - val_MAE: 21.3959\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 588.6585 - MAE: 22.3473 - val_loss: 498.1833 - val_MAE: 21.3425\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 586.2792 - MAE: 22.2937 - val_loss: 495.8984 - val_MAE: 21.2889\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 583.8948 - MAE: 22.2402 - val_loss: 493.6287 - val_MAE: 21.2356\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 581.5269 - MAE: 22.1871 - val_loss: 491.3835 - val_MAE: 21.1826\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 579.1814 - MAE: 22.1346 - val_loss: 489.0997 - val_MAE: 21.1287\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 576.7997 - MAE: 22.0804 - val_loss: 486.8846 - val_MAE: 21.0762\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 574.4857 - MAE: 22.0277 - val_loss: 484.6452 - val_MAE: 21.0230\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 572.1480 - MAE: 21.9746 - val_loss: 482.4413 - val_MAE: 20.9705\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 569.8456 - MAE: 21.9221 - val_loss: 480.2032 - val_MAE: 20.9171\n",
            "Epoch 22/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 567.5090 - MAE: 21.8694 - val_loss: 478.0066 - val_MAE: 20.8645\n",
            "Epoch 23/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 565.2153 - MAE: 21.8166 - val_loss: 475.8347 - val_MAE: 20.8124\n",
            "Epoch 24/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 562.9460 - MAE: 21.7640 - val_loss: 473.6583 - val_MAE: 20.7600\n",
            "Epoch 25/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 560.6725 - MAE: 21.7119 - val_loss: 471.4920 - val_MAE: 20.7078\n",
            "Epoch 26/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 558.4086 - MAE: 21.6596 - val_loss: 469.3419 - val_MAE: 20.6558\n",
            "Epoch 27/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 556.1626 - MAE: 21.6080 - val_loss: 467.2185 - val_MAE: 20.6043\n",
            "Epoch 28/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 553.9426 - MAE: 21.5564 - val_loss: 465.0744 - val_MAE: 20.5522\n",
            "Epoch 29/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 551.7026 - MAE: 21.5042 - val_loss: 462.9598 - val_MAE: 20.5007\n",
            "Epoch 30/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 549.4924 - MAE: 21.4534 - val_loss: 460.8622 - val_MAE: 20.4495\n",
            "Epoch 31/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 547.2986 - MAE: 21.4024 - val_loss: 458.7513 - val_MAE: 20.3978\n",
            "Epoch 32/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 545.0916 - MAE: 21.3506 - val_loss: 456.6652 - val_MAE: 20.3466\n",
            "Epoch 33/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 542.9105 - MAE: 21.2994 - val_loss: 454.5868 - val_MAE: 20.2955\n",
            "Epoch 34/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 540.7359 - MAE: 21.2481 - val_loss: 452.4966 - val_MAE: 20.2439\n",
            "Epoch 35/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 538.5503 - MAE: 21.1965 - val_loss: 450.4305 - val_MAE: 20.1928\n",
            "Epoch 36/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 536.3901 - MAE: 21.1453 - val_loss: 448.3943 - val_MAE: 20.1424\n",
            "Epoch 37/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 534.2589 - MAE: 21.0951 - val_loss: 446.3488 - val_MAE: 20.0915\n",
            "Epoch 38/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 532.1198 - MAE: 21.0441 - val_loss: 444.3352 - val_MAE: 20.0413\n",
            "Epoch 39/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 530.0119 - MAE: 20.9943 - val_loss: 442.3132 - val_MAE: 19.9908\n",
            "Epoch 40/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 527.8968 - MAE: 20.9437 - val_loss: 440.3276 - val_MAE: 19.9411\n",
            "Epoch 41/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 525.8176 - MAE: 20.8936 - val_loss: 438.3104 - val_MAE: 19.8905\n",
            "Epoch 42/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 523.7065 - MAE: 20.8427 - val_loss: 436.3245 - val_MAE: 19.8405\n",
            "Epoch 43/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 521.6273 - MAE: 20.7935 - val_loss: 434.3447 - val_MAE: 19.7905\n",
            "Epoch 44/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 519.5547 - MAE: 20.7441 - val_loss: 432.3823 - val_MAE: 19.7409\n",
            "Epoch 45/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 517.4998 - MAE: 20.6935 - val_loss: 430.4261 - val_MAE: 19.6913\n",
            "Epoch 46/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 515.4517 - MAE: 20.6440 - val_loss: 428.4945 - val_MAE: 19.6422\n",
            "Epoch 47/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 513.4278 - MAE: 20.5948 - val_loss: 426.5459 - val_MAE: 19.5925\n",
            "Epoch 48/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 511.3875 - MAE: 20.5459 - val_loss: 424.6419 - val_MAE: 19.5439\n",
            "Epoch 49/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 509.3925 - MAE: 20.4968 - val_loss: 422.7307 - val_MAE: 19.4949\n",
            "Epoch 50/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 507.3901 - MAE: 20.4485 - val_loss: 420.8207 - val_MAE: 19.4459\n",
            "Epoch 51/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 505.3880 - MAE: 20.3992 - val_loss: 418.9129 - val_MAE: 19.3967\n",
            "Epoch 52/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 503.3889 - MAE: 20.3503 - val_loss: 417.0221 - val_MAE: 19.3479\n",
            "Epoch 53/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 501.4074 - MAE: 20.3011 - val_loss: 415.1380 - val_MAE: 19.2992\n",
            "Epoch 54/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 499.4323 - MAE: 20.2526 - val_loss: 413.2513 - val_MAE: 19.2502\n",
            "Epoch 55/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 497.4538 - MAE: 20.2033 - val_loss: 411.3839 - val_MAE: 19.2017\n",
            "Epoch 56/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 495.4966 - MAE: 20.1548 - val_loss: 409.5329 - val_MAE: 19.1534\n",
            "Epoch 57/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 493.5546 - MAE: 20.1069 - val_loss: 407.6753 - val_MAE: 19.1049\n",
            "Epoch 58/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 491.6070 - MAE: 20.0582 - val_loss: 405.8443 - val_MAE: 19.0569\n",
            "Epoch 59/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 489.6864 - MAE: 20.0107 - val_loss: 404.0098 - val_MAE: 19.0087\n",
            "Epoch 60/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 487.7616 - MAE: 19.9623 - val_loss: 402.1856 - val_MAE: 18.9606\n",
            "Epoch 61/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 485.8483 - MAE: 19.9144 - val_loss: 400.3721 - val_MAE: 18.9128\n",
            "Epoch 62/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 483.9453 - MAE: 19.8665 - val_loss: 398.5810 - val_MAE: 18.8653\n",
            "Epoch 63/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 482.0656 - MAE: 19.8194 - val_loss: 396.7790 - val_MAE: 18.8175\n",
            "Epoch 64/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 480.1749 - MAE: 19.7711 - val_loss: 395.0033 - val_MAE: 18.7703\n",
            "Epoch 65/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 478.3112 - MAE: 19.7238 - val_loss: 393.2408 - val_MAE: 18.7233\n",
            "Epoch 66/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 476.4597 - MAE: 19.6772 - val_loss: 391.4608 - val_MAE: 18.6757\n",
            "Epoch 67/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 474.5913 - MAE: 19.6302 - val_loss: 389.6996 - val_MAE: 18.6285\n",
            "Epoch 68/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 472.7417 - MAE: 19.5826 - val_loss: 387.9353 - val_MAE: 18.5811\n",
            "Epoch 69/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 470.8885 - MAE: 19.5351 - val_loss: 386.1853 - val_MAE: 18.5339\n",
            "Epoch 70/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 469.0508 - MAE: 19.4888 - val_loss: 384.4491 - val_MAE: 18.4870\n",
            "Epoch 71/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 467.2274 - MAE: 19.4415 - val_loss: 382.7386 - val_MAE: 18.4407\n",
            "Epoch 72/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 465.4305 - MAE: 19.3947 - val_loss: 381.0353 - val_MAE: 18.3944\n",
            "Epoch 73/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 463.6405 - MAE: 19.3484 - val_loss: 379.3261 - val_MAE: 18.3479\n",
            "Epoch 74/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 461.8445 - MAE: 19.3020 - val_loss: 377.6324 - val_MAE: 18.3017\n",
            "Epoch 75/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 460.0646 - MAE: 19.2559 - val_loss: 375.9505 - val_MAE: 18.2557\n",
            "Epoch 76/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 458.2962 - MAE: 19.2100 - val_loss: 374.2626 - val_MAE: 18.2094\n",
            "Epoch 77/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 456.5221 - MAE: 19.1635 - val_loss: 372.5960 - val_MAE: 18.1636\n",
            "Epoch 78/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 454.7695 - MAE: 19.1177 - val_loss: 370.9256 - val_MAE: 18.1176\n",
            "Epoch 79/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 453.0135 - MAE: 19.0724 - val_loss: 369.2812 - val_MAE: 18.0721\n",
            "Epoch 80/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 451.2835 - MAE: 19.0269 - val_loss: 367.6302 - val_MAE: 18.0264\n",
            "Epoch 81/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 449.5469 - MAE: 18.9813 - val_loss: 365.9781 - val_MAE: 17.9805\n",
            "Epoch 82/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 447.8096 - MAE: 18.9354 - val_loss: 364.3449 - val_MAE: 17.9350\n",
            "Epoch 83/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 446.0911 - MAE: 18.8897 - val_loss: 362.7239 - val_MAE: 17.8898\n",
            "Epoch 84/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 444.3853 - MAE: 18.8447 - val_loss: 361.1063 - val_MAE: 17.8445\n",
            "Epoch 85/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 442.6829 - MAE: 18.7995 - val_loss: 359.4881 - val_MAE: 17.7991\n",
            "Epoch 86/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 440.9810 - MAE: 18.7538 - val_loss: 357.9023 - val_MAE: 17.7545\n",
            "Epoch 87/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 439.3109 - MAE: 18.7097 - val_loss: 356.3104 - val_MAE: 17.7096\n",
            "Epoch 88/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 437.6347 - MAE: 18.6646 - val_loss: 354.7255 - val_MAE: 17.6648\n",
            "Epoch 89/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 435.9662 - MAE: 18.6199 - val_loss: 353.1560 - val_MAE: 17.6203\n",
            "Epoch 90/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 434.3140 - MAE: 18.5759 - val_loss: 351.6009 - val_MAE: 17.5762\n",
            "Epoch 91/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 432.6753 - MAE: 18.5314 - val_loss: 350.0363 - val_MAE: 17.5316\n",
            "Epoch 92/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 431.0270 - MAE: 18.4867 - val_loss: 348.4706 - val_MAE: 17.4869\n",
            "Epoch 93/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 429.3778 - MAE: 18.4417 - val_loss: 346.9225 - val_MAE: 17.4426\n",
            "Epoch 94/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 427.7479 - MAE: 18.3979 - val_loss: 345.4084 - val_MAE: 17.3991\n",
            "Epoch 95/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 426.1513 - MAE: 18.3542 - val_loss: 343.8787 - val_MAE: 17.3551\n",
            "Epoch 96/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 424.5396 - MAE: 18.3107 - val_loss: 342.3577 - val_MAE: 17.3112\n",
            "Epoch 97/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 422.9362 - MAE: 18.2664 - val_loss: 340.8340 - val_MAE: 17.2671\n",
            "Epoch 98/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 421.3294 - MAE: 18.2227 - val_loss: 339.3051 - val_MAE: 17.2228\n",
            "Epoch 99/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 419.7183 - MAE: 18.1789 - val_loss: 337.8120 - val_MAE: 17.1794\n",
            "Epoch 100/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 418.1432 - MAE: 18.1353 - val_loss: 336.3156 - val_MAE: 17.1358\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 432.7368 - MAE: 18.6947\n",
            "Epoch 1/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 544738.3750 - MAE: 259.5896 - val_loss: 123.9481 - val_MAE: 9.4205\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 134.7522 - MAE: 8.3755 - val_loss: 49.2889 - val_MAE: 5.5455\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 95.1499 - MAE: 6.7419 - val_loss: 42.8881 - val_MAE: 5.3277\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 90.4687 - MAE: 6.6766 - val_loss: 42.9420 - val_MAE: 5.3057\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 89.6869 - MAE: 6.7725 - val_loss: 43.5132 - val_MAE: 5.3316\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 89.4448 - MAE: 6.8519 - val_loss: 43.1265 - val_MAE: 5.3138\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 89.5031 - MAE: 6.7746 - val_loss: 43.5110 - val_MAE: 5.3316\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 89.4867 - MAE: 6.8067 - val_loss: 44.0157 - val_MAE: 5.3617\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 89.6460 - MAE: 6.8218 - val_loss: 43.4849 - val_MAE: 5.3305\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 89.4599 - MAE: 6.8116 - val_loss: 44.0221 - val_MAE: 5.3621\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 89.5693 - MAE: 6.8707 - val_loss: 43.5998 - val_MAE: 5.3350\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 89.6552 - MAE: 6.8452 - val_loss: 43.4206 - val_MAE: 5.3278\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 89.5942 - MAE: 6.7958 - val_loss: 43.4238 - val_MAE: 5.3280\n",
            "Epoch 13: early stopping\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 83.7286 - MAE: 6.5323\n",
            "Epoch 1/100\n",
            "12/12 [==============================] - 1s 16ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "4/4 [==============================] - 0s 3ms/step - loss: nan - MAE: nan\n"
          ]
        }
      ],
      "source": [
        "hl = []\n",
        "for x in [0,1,2,3]:\n",
        "    model = build_model(x,25,\"sgd\",10e-5)\n",
        "    tensorboard_cb = tf.keras.callbacks.TensorBoard(get_run_logdir(\"hl\",x))\n",
        "    model.fit(X_train, y_train, epochs=100,  callbacks=[tensorboard_cb,es], validation_split=0.1) \n",
        "    ev = model.evaluate(X_test, y_test)\n",
        "    hl.append((x,ev[0],ev[1]))\n",
        "import pickle\n",
        "pickle.dump(hl, open(\"hl.pkl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6257b098-49db-4561-826f-65202a5836a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6257b098-49db-4561-826f-65202a5836a1",
        "outputId": "a3f372b8-cb9e-4c48-8c28-0de18078ddc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "12/12 [==============================] - 1s 16ms/step - loss: 14374.3311 - MAE: 54.7305 - val_loss: 500.3337 - val_MAE: 21.3929\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 588.5240 - MAE: 22.3446 - val_loss: 498.0657 - val_MAE: 21.3398\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 586.1573 - MAE: 22.2912 - val_loss: 495.8081 - val_MAE: 21.2868\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 583.8007 - MAE: 22.2382 - val_loss: 493.5380 - val_MAE: 21.2334\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 581.4316 - MAE: 22.1849 - val_loss: 491.2727 - val_MAE: 21.1800\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 579.0676 - MAE: 22.1309 - val_loss: 489.0379 - val_MAE: 21.1272\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 576.7341 - MAE: 22.0789 - val_loss: 486.7993 - val_MAE: 21.0741\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 574.3969 - MAE: 22.0261 - val_loss: 484.5627 - val_MAE: 21.0210\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 572.0629 - MAE: 21.9731 - val_loss: 482.3718 - val_MAE: 20.9688\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 569.7733 - MAE: 21.9207 - val_loss: 480.1514 - val_MAE: 20.9158\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 567.4553 - MAE: 21.8673 - val_loss: 477.9583 - val_MAE: 20.8633\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 565.1649 - MAE: 21.8149 - val_loss: 475.7802 - val_MAE: 20.8111\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 562.8898 - MAE: 21.7631 - val_loss: 473.6216 - val_MAE: 20.7591\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 560.6337 - MAE: 21.7109 - val_loss: 471.4570 - val_MAE: 20.7069\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 558.3721 - MAE: 21.6588 - val_loss: 469.3071 - val_MAE: 20.6550\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 556.1265 - MAE: 21.6071 - val_loss: 467.1807 - val_MAE: 20.6034\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 553.9023 - MAE: 21.5560 - val_loss: 465.0165 - val_MAE: 20.5508\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 551.6427 - MAE: 21.5032 - val_loss: 462.9189 - val_MAE: 20.4997\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 549.4485 - MAE: 21.4519 - val_loss: 460.7974 - val_MAE: 20.4479\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 547.2312 - MAE: 21.4001 - val_loss: 458.7101 - val_MAE: 20.3968\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 545.0478 - MAE: 21.3490 - val_loss: 456.5892 - val_MAE: 20.3448\n",
            "Epoch 22/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 542.8309 - MAE: 21.2977 - val_loss: 454.5084 - val_MAE: 20.2936\n",
            "Epoch 23/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 540.6554 - MAE: 21.2463 - val_loss: 452.4515 - val_MAE: 20.2428\n",
            "Epoch 24/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 538.5036 - MAE: 21.1950 - val_loss: 450.3898 - val_MAE: 20.1918\n",
            "Epoch 25/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 536.3472 - MAE: 21.1443 - val_loss: 448.3378 - val_MAE: 20.1410\n",
            "Epoch 26/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 534.2000 - MAE: 21.0934 - val_loss: 446.3012 - val_MAE: 20.0903\n",
            "Epoch 27/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 532.0699 - MAE: 21.0431 - val_loss: 444.2903 - val_MAE: 20.0402\n",
            "Epoch 28/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 529.9648 - MAE: 20.9929 - val_loss: 442.2591 - val_MAE: 19.9895\n",
            "Epoch 29/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 527.8400 - MAE: 20.9420 - val_loss: 440.2562 - val_MAE: 19.9393\n",
            "Epoch 30/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 525.7440 - MAE: 20.8925 - val_loss: 438.2697 - val_MAE: 19.8894\n",
            "Epoch 31/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 523.6636 - MAE: 20.8429 - val_loss: 436.2701 - val_MAE: 19.8391\n",
            "Epoch 32/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 521.5702 - MAE: 20.7924 - val_loss: 434.2941 - val_MAE: 19.7892\n",
            "Epoch 33/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 519.5016 - MAE: 20.7426 - val_loss: 432.3255 - val_MAE: 19.7394\n",
            "Epoch 34/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 517.4393 - MAE: 20.6926 - val_loss: 430.3450 - val_MAE: 19.6892\n",
            "Epoch 35/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 515.3658 - MAE: 20.6424 - val_loss: 428.3878 - val_MAE: 19.6395\n",
            "Epoch 36/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 513.3168 - MAE: 20.5925 - val_loss: 426.4596 - val_MAE: 19.5903\n",
            "Epoch 37/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 511.2960 - MAE: 20.5436 - val_loss: 424.5218 - val_MAE: 19.5408\n",
            "Epoch 38/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 509.2671 - MAE: 20.4939 - val_loss: 422.6151 - val_MAE: 19.4919\n",
            "Epoch 39/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 507.2682 - MAE: 20.4454 - val_loss: 420.6997 - val_MAE: 19.4427\n",
            "Epoch 40/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 505.2621 - MAE: 20.3962 - val_loss: 418.8197 - val_MAE: 19.3943\n",
            "Epoch 41/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 503.2908 - MAE: 20.3474 - val_loss: 416.9085 - val_MAE: 19.3450\n",
            "Epoch 42/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 501.2881 - MAE: 20.2978 - val_loss: 415.0276 - val_MAE: 19.2963\n",
            "Epoch 43/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 499.3161 - MAE: 20.2499 - val_loss: 413.1524 - val_MAE: 19.2477\n",
            "Epoch 44/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 497.3506 - MAE: 20.2018 - val_loss: 411.2939 - val_MAE: 19.1993\n",
            "Epoch 45/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 495.4018 - MAE: 20.1525 - val_loss: 409.4412 - val_MAE: 19.1510\n",
            "Epoch 46/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 493.4595 - MAE: 20.1043 - val_loss: 407.6120 - val_MAE: 19.1032\n",
            "Epoch 47/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 491.5405 - MAE: 20.0564 - val_loss: 405.7662 - val_MAE: 19.0548\n",
            "Epoch 48/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 489.6051 - MAE: 20.0088 - val_loss: 403.9636 - val_MAE: 19.0075\n",
            "Epoch 49/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 487.7137 - MAE: 19.9609 - val_loss: 402.1537 - val_MAE: 18.9598\n",
            "Epoch 50/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 485.8150 - MAE: 19.9140 - val_loss: 400.3446 - val_MAE: 18.9120\n",
            "Epoch 51/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 483.9162 - MAE: 19.8660 - val_loss: 398.5376 - val_MAE: 18.8642\n",
            "Epoch 52/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 482.0201 - MAE: 19.8183 - val_loss: 396.7469 - val_MAE: 18.8167\n",
            "Epoch 53/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 480.1409 - MAE: 19.7703 - val_loss: 394.9624 - val_MAE: 18.7692\n",
            "Epoch 54/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 478.2677 - MAE: 19.7232 - val_loss: 393.1751 - val_MAE: 18.7215\n",
            "Epoch 55/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 476.3908 - MAE: 19.6752 - val_loss: 391.4065 - val_MAE: 18.6742\n",
            "Epoch 56/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 474.5347 - MAE: 19.6279 - val_loss: 389.6534 - val_MAE: 18.6272\n",
            "Epoch 57/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 472.6930 - MAE: 19.5812 - val_loss: 387.8937 - val_MAE: 18.5799\n",
            "Epoch 58/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 470.8455 - MAE: 19.5339 - val_loss: 386.1599 - val_MAE: 18.5332\n",
            "Epoch 59/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 469.0242 - MAE: 19.4876 - val_loss: 384.4221 - val_MAE: 18.4863\n",
            "Epoch 60/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 467.1985 - MAE: 19.4404 - val_loss: 382.6943 - val_MAE: 18.4395\n",
            "Epoch 61/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 465.3838 - MAE: 19.3938 - val_loss: 380.9767 - val_MAE: 18.3929\n",
            "Epoch 62/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 463.5789 - MAE: 19.3471 - val_loss: 379.2806 - val_MAE: 18.3467\n",
            "Epoch 63/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 461.7965 - MAE: 19.3012 - val_loss: 377.5738 - val_MAE: 18.3001\n",
            "Epoch 64/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 460.0031 - MAE: 19.2542 - val_loss: 375.8922 - val_MAE: 18.2541\n",
            "Epoch 65/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 458.2358 - MAE: 19.2082 - val_loss: 374.2233 - val_MAE: 18.2083\n",
            "Epoch 66/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 456.4802 - MAE: 19.1628 - val_loss: 372.5370 - val_MAE: 18.1620\n",
            "Epoch 67/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 454.7078 - MAE: 19.1170 - val_loss: 370.8691 - val_MAE: 18.1160\n",
            "Epoch 68/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 452.9535 - MAE: 19.0706 - val_loss: 369.1978 - val_MAE: 18.0698\n",
            "Epoch 69/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 451.1956 - MAE: 19.0244 - val_loss: 367.5402 - val_MAE: 18.0239\n",
            "Epoch 70/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 449.4524 - MAE: 18.9793 - val_loss: 365.8958 - val_MAE: 17.9782\n",
            "Epoch 71/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 447.7230 - MAE: 18.9333 - val_loss: 364.2762 - val_MAE: 17.9331\n",
            "Epoch 72/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 446.0192 - MAE: 18.8876 - val_loss: 362.6634 - val_MAE: 17.8881\n",
            "Epoch 73/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 444.3219 - MAE: 18.8426 - val_loss: 361.0446 - val_MAE: 17.8428\n",
            "Epoch 74/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 442.6185 - MAE: 18.7973 - val_loss: 359.4407 - val_MAE: 17.7978\n",
            "Epoch 75/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 440.9305 - MAE: 18.7525 - val_loss: 357.8481 - val_MAE: 17.7530\n",
            "Epoch 76/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 439.2534 - MAE: 18.7078 - val_loss: 356.2493 - val_MAE: 17.7079\n",
            "Epoch 77/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 437.5708 - MAE: 18.6625 - val_loss: 354.6712 - val_MAE: 17.6633\n",
            "Epoch 78/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 435.9088 - MAE: 18.6179 - val_loss: 353.0891 - val_MAE: 17.6184\n",
            "Epoch 79/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 434.2433 - MAE: 18.5738 - val_loss: 351.5321 - val_MAE: 17.5742\n",
            "Epoch 80/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 432.6029 - MAE: 18.5295 - val_loss: 349.9686 - val_MAE: 17.5297\n",
            "Epoch 81/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 430.9557 - MAE: 18.4851 - val_loss: 348.4037 - val_MAE: 17.4850\n",
            "Epoch 82/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 429.3079 - MAE: 18.4404 - val_loss: 346.8571 - val_MAE: 17.4407\n",
            "Epoch 83/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 427.6780 - MAE: 18.3959 - val_loss: 345.3220 - val_MAE: 17.3966\n",
            "Epoch 84/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 426.0603 - MAE: 18.3521 - val_loss: 343.7900 - val_MAE: 17.3525\n",
            "Epoch 85/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 424.4457 - MAE: 18.3080 - val_loss: 342.2573 - val_MAE: 17.3083\n",
            "Epoch 86/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 422.8315 - MAE: 18.2635 - val_loss: 340.7560 - val_MAE: 17.2649\n",
            "Epoch 87/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 421.2478 - MAE: 18.2205 - val_loss: 339.2484 - val_MAE: 17.2212\n",
            "Epoch 88/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 419.6581 - MAE: 18.1766 - val_loss: 337.7475 - val_MAE: 17.1775\n",
            "Epoch 89/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 418.0756 - MAE: 18.1331 - val_loss: 336.2614 - val_MAE: 17.1342\n",
            "Epoch 90/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 416.5089 - MAE: 18.0903 - val_loss: 334.7890 - val_MAE: 17.0912\n",
            "Epoch 91/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 414.9551 - MAE: 18.0470 - val_loss: 333.3073 - val_MAE: 17.0478\n",
            "Epoch 92/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 413.3918 - MAE: 18.0035 - val_loss: 331.8242 - val_MAE: 17.0043\n",
            "Epoch 93/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 411.8274 - MAE: 17.9596 - val_loss: 330.3582 - val_MAE: 16.9611\n",
            "Epoch 94/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 410.2815 - MAE: 17.9169 - val_loss: 328.9250 - val_MAE: 16.9188\n",
            "Epoch 95/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 408.7679 - MAE: 17.8744 - val_loss: 327.4765 - val_MAE: 16.8759\n",
            "Epoch 96/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 407.2394 - MAE: 17.8320 - val_loss: 326.0362 - val_MAE: 16.8332\n",
            "Epoch 97/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 405.7188 - MAE: 17.7889 - val_loss: 324.5931 - val_MAE: 16.7903\n",
            "Epoch 98/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 404.1948 - MAE: 17.7463 - val_loss: 323.1448 - val_MAE: 16.7471\n",
            "Epoch 99/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 402.6662 - MAE: 17.7036 - val_loss: 321.7311 - val_MAE: 16.7048\n",
            "Epoch 100/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 401.1726 - MAE: 17.6612 - val_loss: 320.3141 - val_MAE: 16.6624\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 415.2590 - MAE: 18.2213\n",
            "Epoch 1/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 56667500.0000 - MAE: 2238.0610 - val_loss: 742.2399 - val_MAE: 26.4492\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 832.8025 - MAE: 27.1812 - val_loss: 262.0474 - val_MAE: 14.5539\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 814.8817 - MAE: 25.9713 - val_loss: 735.8595 - val_MAE: 26.3283\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 833.1601 - MAE: 27.2745 - val_loss: 732.4161 - val_MAE: 26.2628\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 829.5967 - MAE: 27.2091 - val_loss: 728.9817 - val_MAE: 26.1974\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 826.0424 - MAE: 27.1431 - val_loss: 725.5880 - val_MAE: 26.1325\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 822.5291 - MAE: 27.0790 - val_loss: 722.1928 - val_MAE: 26.0675\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 819.0141 - MAE: 27.0142 - val_loss: 718.8029 - val_MAE: 26.0024\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 815.5061 - MAE: 26.9493 - val_loss: 715.4724 - val_MAE: 25.9382\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 812.0558 - MAE: 26.8849 - val_loss: 712.1086 - val_MAE: 25.8733\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 808.5737 - MAE: 26.8197 - val_loss: 708.7814 - val_MAE: 25.8089\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 805.1285 - MAE: 26.7554 - val_loss: 705.4758 - val_MAE: 25.7448\n",
            "Epoch 12: early stopping\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 828.7385 - MAE: 27.3037\n",
            "Epoch 1/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 37791416.0000 - MAE: 1933.2405 - val_loss: 211.2204 - val_MAE: 12.4934\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 420.0926 - MAE: 17.9638 - val_loss: 354.9820 - val_MAE: 17.6726\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 436.4010 - MAE: 18.6317 - val_loss: 353.4224 - val_MAE: 17.6284\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 434.7579 - MAE: 18.5875 - val_loss: 351.8510 - val_MAE: 17.5837\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 433.1027 - MAE: 18.5430 - val_loss: 350.2821 - val_MAE: 17.5391\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 431.4501 - MAE: 18.4977 - val_loss: 348.7369 - val_MAE: 17.4950\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 429.8217 - MAE: 18.4544 - val_loss: 347.1872 - val_MAE: 17.4506\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 428.1884 - MAE: 18.4102 - val_loss: 345.6377 - val_MAE: 17.4062\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 426.5564 - MAE: 18.3659 - val_loss: 344.1245 - val_MAE: 17.3626\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 424.9599 - MAE: 18.3221 - val_loss: 342.5856 - val_MAE: 17.3183\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 423.3382 - MAE: 18.2774 - val_loss: 341.0677 - val_MAE: 17.2744\n",
            "Epoch 11: early stopping\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 438.0844 - MAE: 18.8372\n"
          ]
        }
      ],
      "source": [
        "nn = []\n",
        "for x in [5,25,125]:\n",
        "    model = build_model(1,x,\"sgd\",10e-5)\n",
        "    tensorboard_cb = tf.keras.callbacks.TensorBoard(get_run_logdir(\"nn\",x))\n",
        "    model.fit(X_train, y_train, epochs=100,  callbacks=[tensorboard_cb,es], validation_split=0.1)\n",
        "    ev = model.evaluate(X_test, y_test)\n",
        "    nn.append((x,ev[0],ev[1]))\n",
        "import pickle\n",
        "pickle.dump(nn, open(\"nn.pkl\", \"wb\")) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "563a03f8-067e-418c-aef6-8c1aaf03c9a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "563a03f8-067e-418c-aef6-8c1aaf03c9a6",
        "outputId": "c9b43792-f34b-4254-ad77-6209a6aef66e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 427640032.0000 - MAE: 6068.5122 - val_loss: 50.2310 - val_MAE: 5.8455\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 68.1092 - MAE: 5.6119 - val_loss: 44.7271 - val_MAE: 5.4800\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 84.9172 - MAE: 6.5169 - val_loss: 63.2436 - val_MAE: 6.5680\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 84.1590 - MAE: 6.3033 - val_loss: 24.1667 - val_MAE: 3.9273\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 72.4589 - MAE: 5.7591 - val_loss: 20.9668 - val_MAE: 3.7882\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 84.2368 - MAE: 6.3764 - val_loss: 23.6464 - val_MAE: 3.8395\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 74.2032 - MAE: 5.9708 - val_loss: 57.0141 - val_MAE: 6.1931\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 67.9367 - MAE: 5.5529 - val_loss: 31.0139 - val_MAE: 4.5552\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 75.2416 - MAE: 6.0198 - val_loss: 35.7568 - val_MAE: 4.8845\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 83.9463 - MAE: 6.5886 - val_loss: 65.1463 - val_MAE: 6.9203\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 111.5791 - MAE: 7.6795 - val_loss: 59.2979 - val_MAE: 6.3403\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 77.4788 - MAE: 5.8305 - val_loss: 36.5915 - val_MAE: 4.9438\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 74.9417 - MAE: 6.1343 - val_loss: 74.6826 - val_MAE: 7.1379\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 80.0499 - MAE: 6.0394 - val_loss: 28.7015 - val_MAE: 4.4724\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 71.2392 - MAE: 5.7090 - val_loss: 21.0458 - val_MAE: 3.7924\n",
            "Epoch 15: early stopping\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 62.7852 - MAE: 5.9250\n",
            "Epoch 1/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 18719736.0000 - MAE: 1289.9166 - val_loss: 389.7881 - val_MAE: 18.6308\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 472.8355 - MAE: 19.5854 - val_loss: 388.0593 - val_MAE: 18.5844\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 471.0198 - MAE: 19.5386 - val_loss: 386.3384 - val_MAE: 18.5380\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 469.2119 - MAE: 19.4922 - val_loss: 384.6055 - val_MAE: 18.4912\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 467.3916 - MAE: 19.4456 - val_loss: 382.8756 - val_MAE: 18.4444\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 465.5745 - MAE: 19.3982 - val_loss: 381.1711 - val_MAE: 18.3981\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 463.7831 - MAE: 19.3527 - val_loss: 379.4622 - val_MAE: 18.3516\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 461.9870 - MAE: 19.3064 - val_loss: 377.7538 - val_MAE: 18.3050\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 460.1928 - MAE: 19.2599 - val_loss: 376.0840 - val_MAE: 18.2594\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 458.4361 - MAE: 19.2140 - val_loss: 374.3875 - val_MAE: 18.2128\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 456.6534 - MAE: 19.1672 - val_loss: 372.7135 - val_MAE: 18.1668\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 454.8937 - MAE: 19.1212 - val_loss: 371.0515 - val_MAE: 18.1210\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 453.1463 - MAE: 19.0758 - val_loss: 369.4053 - val_MAE: 18.0756\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 451.4140 - MAE: 19.0301 - val_loss: 367.7529 - val_MAE: 18.0298\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 449.6761 - MAE: 18.9844 - val_loss: 366.1121 - val_MAE: 17.9842\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 447.9510 - MAE: 18.9392 - val_loss: 364.4906 - val_MAE: 17.9391\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 446.2433 - MAE: 18.8944 - val_loss: 362.8352 - val_MAE: 17.8929\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 444.5036 - MAE: 18.8480 - val_loss: 361.2367 - val_MAE: 17.8482\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 442.8199 - MAE: 18.8031 - val_loss: 359.6162 - val_MAE: 17.8027\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 441.1148 - MAE: 18.7576 - val_loss: 358.0244 - val_MAE: 17.7580\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 439.4385 - MAE: 18.7129 - val_loss: 356.4023 - val_MAE: 17.7122\n",
            "Epoch 22/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 437.7316 - MAE: 18.6679 - val_loss: 354.8141 - val_MAE: 17.6673\n",
            "Epoch 23/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 436.0599 - MAE: 18.6228 - val_loss: 353.2456 - val_MAE: 17.6229\n",
            "Epoch 24/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 434.4078 - MAE: 18.5778 - val_loss: 351.6718 - val_MAE: 17.5782\n",
            "Epoch 25/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 432.7504 - MAE: 18.5334 - val_loss: 350.1055 - val_MAE: 17.5336\n",
            "Epoch 26/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 431.1000 - MAE: 18.4887 - val_loss: 348.5515 - val_MAE: 17.4892\n",
            "Epoch 27/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 429.4637 - MAE: 18.4446 - val_loss: 347.0189 - val_MAE: 17.4453\n",
            "Epoch 28/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 427.8481 - MAE: 18.4007 - val_loss: 345.4674 - val_MAE: 17.4008\n",
            "Epoch 29/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 426.2141 - MAE: 18.3560 - val_loss: 343.9397 - val_MAE: 17.3568\n",
            "Epoch 30/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 424.6043 - MAE: 18.3127 - val_loss: 342.4252 - val_MAE: 17.3132\n",
            "Epoch 31/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 423.0070 - MAE: 18.2693 - val_loss: 340.8981 - val_MAE: 17.2690\n",
            "Epoch 32/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 421.3972 - MAE: 18.2250 - val_loss: 339.3907 - val_MAE: 17.2253\n",
            "Epoch 33/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 419.8082 - MAE: 18.1813 - val_loss: 337.8886 - val_MAE: 17.1816\n",
            "Epoch 34/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 418.2235 - MAE: 18.1374 - val_loss: 336.3752 - val_MAE: 17.1376\n",
            "Epoch 35/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 416.6281 - MAE: 18.0933 - val_loss: 334.8811 - val_MAE: 17.0939\n",
            "Epoch 36/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 415.0531 - MAE: 18.0496 - val_loss: 333.4112 - val_MAE: 17.0509\n",
            "Epoch 37/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 413.5013 - MAE: 18.0068 - val_loss: 331.9320 - val_MAE: 17.0074\n",
            "Epoch 38/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 411.9418 - MAE: 17.9632 - val_loss: 330.4787 - val_MAE: 16.9646\n",
            "Epoch 39/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 410.4075 - MAE: 17.9207 - val_loss: 329.0171 - val_MAE: 16.9215\n",
            "Epoch 40/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 408.8659 - MAE: 17.8776 - val_loss: 327.5852 - val_MAE: 16.8791\n",
            "Epoch 41/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 407.3536 - MAE: 17.8348 - val_loss: 326.1252 - val_MAE: 16.8358\n",
            "Epoch 42/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 405.8127 - MAE: 17.7912 - val_loss: 324.6905 - val_MAE: 16.7932\n",
            "Epoch 43/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 404.2979 - MAE: 17.7493 - val_loss: 323.2599 - val_MAE: 16.7505\n",
            "Epoch 44/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 402.7877 - MAE: 17.7072 - val_loss: 321.8428 - val_MAE: 16.7082\n",
            "Epoch 45/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 401.2910 - MAE: 17.6639 - val_loss: 320.4297 - val_MAE: 16.6658\n",
            "Epoch 46/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 399.7990 - MAE: 17.6217 - val_loss: 319.0364 - val_MAE: 16.6240\n",
            "Epoch 47/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 398.3266 - MAE: 17.5798 - val_loss: 317.6275 - val_MAE: 16.5816\n",
            "Epoch 48/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 396.8386 - MAE: 17.5381 - val_loss: 316.2552 - val_MAE: 16.5401\n",
            "Epoch 49/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 395.3884 - MAE: 17.4962 - val_loss: 314.8757 - val_MAE: 16.4984\n",
            "Epoch 50/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 393.9307 - MAE: 17.4555 - val_loss: 313.4960 - val_MAE: 16.4565\n",
            "Epoch 51/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 392.4717 - MAE: 17.4139 - val_loss: 312.1171 - val_MAE: 16.4146\n",
            "Epoch 52/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 391.0143 - MAE: 17.3726 - val_loss: 310.7515 - val_MAE: 16.3729\n",
            "Epoch 53/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 389.5709 - MAE: 17.3309 - val_loss: 309.3903 - val_MAE: 16.3313\n",
            "Epoch 54/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 388.1315 - MAE: 17.2900 - val_loss: 308.0258 - val_MAE: 16.2895\n",
            "Epoch 55/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 386.6879 - MAE: 17.2484 - val_loss: 306.6767 - val_MAE: 16.2480\n",
            "Epoch 56/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 385.2616 - MAE: 17.2074 - val_loss: 305.3402 - val_MAE: 16.2068\n",
            "Epoch 57/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 383.8470 - MAE: 17.1670 - val_loss: 303.9970 - val_MAE: 16.1653\n",
            "Epoch 58/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 382.4264 - MAE: 17.1258 - val_loss: 302.6755 - val_MAE: 16.1244\n",
            "Epoch 59/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 381.0279 - MAE: 17.0858 - val_loss: 301.3496 - val_MAE: 16.0832\n",
            "Epoch 60/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 379.6244 - MAE: 17.0450 - val_loss: 300.0315 - val_MAE: 16.0422\n",
            "Epoch 61/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 378.2298 - MAE: 17.0044 - val_loss: 298.7214 - val_MAE: 16.0013\n",
            "Epoch 62/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 376.8427 - MAE: 16.9638 - val_loss: 297.4291 - val_MAE: 15.9609\n",
            "Epoch 63/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 375.4745 - MAE: 16.9242 - val_loss: 296.1266 - val_MAE: 15.9200\n",
            "Epoch 64/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 374.0957 - MAE: 16.8832 - val_loss: 294.8452 - val_MAE: 15.8797\n",
            "Epoch 65/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 372.7387 - MAE: 16.8435 - val_loss: 293.5739 - val_MAE: 15.8396\n",
            "Epoch 66/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 371.3911 - MAE: 16.8044 - val_loss: 292.2868 - val_MAE: 15.7990\n",
            "Epoch 67/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 370.0279 - MAE: 16.7646 - val_loss: 291.0147 - val_MAE: 15.7586\n",
            "Epoch 68/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 368.6797 - MAE: 16.7243 - val_loss: 289.7388 - val_MAE: 15.7181\n",
            "Epoch 69/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 367.3274 - MAE: 16.6840 - val_loss: 288.4738 - val_MAE: 15.6778\n",
            "Epoch 70/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 365.9871 - MAE: 16.6451 - val_loss: 287.2197 - val_MAE: 15.6378\n",
            "Epoch 71/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 364.6579 - MAE: 16.6051 - val_loss: 285.9862 - val_MAE: 15.5983\n",
            "Epoch 72/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 363.3503 - MAE: 16.5657 - val_loss: 284.7578 - val_MAE: 15.5589\n",
            "Epoch 73/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 362.0476 - MAE: 16.5265 - val_loss: 283.5233 - val_MAE: 15.5191\n",
            "Epoch 74/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 360.7385 - MAE: 16.4872 - val_loss: 282.3009 - val_MAE: 15.4797\n",
            "Epoch 75/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 359.4422 - MAE: 16.4485 - val_loss: 281.0876 - val_MAE: 15.4405\n",
            "Epoch 76/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 358.1545 - MAE: 16.4097 - val_loss: 279.8681 - val_MAE: 15.4009\n",
            "Epoch 77/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 356.8609 - MAE: 16.3704 - val_loss: 278.6657 - val_MAE: 15.3618\n",
            "Epoch 78/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 355.5847 - MAE: 16.3319 - val_loss: 277.4591 - val_MAE: 15.3225\n",
            "Epoch 79/100\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 354.3046 - MAE: 16.2936 - val_loss: 276.2735 - val_MAE: 15.2838\n",
            "Epoch 80/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 353.0455 - MAE: 16.2553 - val_loss: 275.0813 - val_MAE: 15.2447\n",
            "Epoch 81/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 351.7798 - MAE: 16.2167 - val_loss: 273.8872 - val_MAE: 15.2055\n",
            "Epoch 82/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 350.5125 - MAE: 16.1782 - val_loss: 272.7082 - val_MAE: 15.1667\n",
            "Epoch 83/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 349.2601 - MAE: 16.1397 - val_loss: 271.5385 - val_MAE: 15.1281\n",
            "Epoch 84/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 348.0176 - MAE: 16.1020 - val_loss: 270.3706 - val_MAE: 15.0894\n",
            "Epoch 85/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 346.7770 - MAE: 16.0640 - val_loss: 269.2014 - val_MAE: 15.0506\n",
            "Epoch 86/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 345.5359 - MAE: 16.0254 - val_loss: 268.0586 - val_MAE: 15.0126\n",
            "Epoch 87/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 344.3207 - MAE: 15.9886 - val_loss: 266.9097 - val_MAE: 14.9743\n",
            "Epoch 88/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 343.0994 - MAE: 15.9507 - val_loss: 265.7658 - val_MAE: 14.9361\n",
            "Epoch 89/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 341.8836 - MAE: 15.9132 - val_loss: 264.6339 - val_MAE: 14.8981\n",
            "Epoch 90/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 340.6808 - MAE: 15.8765 - val_loss: 263.5132 - val_MAE: 14.8605\n",
            "Epoch 91/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 339.4885 - MAE: 15.8391 - val_loss: 262.3837 - val_MAE: 14.8224\n",
            "Epoch 92/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 338.2871 - MAE: 15.8015 - val_loss: 261.2522 - val_MAE: 14.7842\n",
            "Epoch 93/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 337.0838 - MAE: 15.7636 - val_loss: 260.1348 - val_MAE: 14.7464\n",
            "Epoch 94/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 335.8961 - MAE: 15.7270 - val_loss: 259.0451 - val_MAE: 14.7094\n",
            "Epoch 95/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 334.7358 - MAE: 15.6902 - val_loss: 257.9414 - val_MAE: 14.6718\n",
            "Epoch 96/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 333.5616 - MAE: 15.6537 - val_loss: 256.8441 - val_MAE: 14.6344\n",
            "Epoch 97/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 332.3936 - MAE: 15.6168 - val_loss: 255.7436 - val_MAE: 14.5967\n",
            "Epoch 98/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 331.2218 - MAE: 15.5798 - val_loss: 254.6379 - val_MAE: 14.5588\n",
            "Epoch 99/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 330.0452 - MAE: 15.5430 - val_loss: 253.5614 - val_MAE: 14.5218\n",
            "Epoch 100/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 328.8984 - MAE: 15.5064 - val_loss: 252.4814 - val_MAE: 14.4862\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 340.6363 - MAE: 16.0723\n",
            "Epoch 1/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 474595.9688 - MAE: 223.5663 - val_loss: 523.2644 - val_MAE: 21.9222\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 612.4368 - MAE: 22.8734 - val_loss: 520.8846 - val_MAE: 21.8679\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 609.9561 - MAE: 22.8187 - val_loss: 518.5159 - val_MAE: 21.8137\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 607.4863 - MAE: 22.7645 - val_loss: 516.1346 - val_MAE: 21.7590\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 605.0036 - MAE: 22.7100 - val_loss: 513.7584 - val_MAE: 21.7043\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 602.5264 - MAE: 22.6547 - val_loss: 511.4138 - val_MAE: 21.6503\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 600.0811 - MAE: 22.6014 - val_loss: 509.0656 - val_MAE: 21.5960\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 597.6318 - MAE: 22.5473 - val_loss: 506.7196 - val_MAE: 21.5416\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 595.1864 - MAE: 22.4931 - val_loss: 504.4207 - val_MAE: 21.4882\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 592.7866 - MAE: 22.4394 - val_loss: 502.0919 - val_MAE: 21.4339\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 590.3580 - MAE: 22.3849 - val_loss: 499.7913 - val_MAE: 21.3802\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 587.9579 - MAE: 22.3312 - val_loss: 497.5064 - val_MAE: 21.3267\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 585.5737 - MAE: 22.2782 - val_loss: 495.2415 - val_MAE: 21.2735\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 583.2092 - MAE: 22.2247 - val_loss: 492.9710 - val_MAE: 21.2201\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 580.8396 - MAE: 22.1714 - val_loss: 490.7157 - val_MAE: 21.1669\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 578.4863 - MAE: 22.1185 - val_loss: 488.4846 - val_MAE: 21.1141\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 576.1553 - MAE: 22.0661 - val_loss: 486.2152 - val_MAE: 21.0603\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 573.7883 - MAE: 22.0121 - val_loss: 484.0142 - val_MAE: 21.0080\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 571.4885 - MAE: 21.9596 - val_loss: 481.7890 - val_MAE: 20.9549\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 569.1652 - MAE: 21.9066 - val_loss: 479.5990 - val_MAE: 20.9026\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 566.8771 - MAE: 21.8543 - val_loss: 477.3749 - val_MAE: 20.8493\n",
            "Epoch 22/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 564.5549 - MAE: 21.8018 - val_loss: 475.1921 - val_MAE: 20.7969\n",
            "Epoch 23/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 562.2752 - MAE: 21.7491 - val_loss: 473.0341 - val_MAE: 20.7450\n",
            "Epoch 24/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 560.0201 - MAE: 21.6967 - val_loss: 470.8713 - val_MAE: 20.6928\n",
            "Epoch 25/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 557.7606 - MAE: 21.6448 - val_loss: 468.7187 - val_MAE: 20.6407\n",
            "Epoch 26/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 555.5106 - MAE: 21.5926 - val_loss: 466.5823 - val_MAE: 20.5889\n",
            "Epoch 27/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 553.2786 - MAE: 21.5411 - val_loss: 464.4723 - val_MAE: 20.5376\n",
            "Epoch 28/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 551.0723 - MAE: 21.4898 - val_loss: 462.3416 - val_MAE: 20.4857\n",
            "Epoch 29/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 548.8460 - MAE: 21.4377 - val_loss: 460.2406 - val_MAE: 20.4343\n",
            "Epoch 30/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 546.6496 - MAE: 21.3870 - val_loss: 458.1562 - val_MAE: 20.3832\n",
            "Epoch 31/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 544.4693 - MAE: 21.3362 - val_loss: 456.0587 - val_MAE: 20.3317\n",
            "Epoch 32/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 542.2760 - MAE: 21.2845 - val_loss: 453.9857 - val_MAE: 20.2807\n",
            "Epoch 33/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 540.1083 - MAE: 21.2335 - val_loss: 451.9205 - val_MAE: 20.2297\n",
            "Epoch 34/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 537.9472 - MAE: 21.1823 - val_loss: 449.8434 - val_MAE: 20.1783\n",
            "Epoch 35/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 535.7750 - MAE: 21.1309 - val_loss: 447.7903 - val_MAE: 20.1274\n",
            "Epoch 36/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 533.6281 - MAE: 21.0799 - val_loss: 445.7671 - val_MAE: 20.0770\n",
            "Epoch 37/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 531.5100 - MAE: 21.0298 - val_loss: 443.7344 - val_MAE: 20.0264\n",
            "Epoch 38/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 529.3842 - MAE: 20.9790 - val_loss: 441.7337 - val_MAE: 19.9763\n",
            "Epoch 39/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 527.2893 - MAE: 20.9293 - val_loss: 439.7244 - val_MAE: 19.9260\n",
            "Epoch 40/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 525.1873 - MAE: 20.8789 - val_loss: 437.7516 - val_MAE: 19.8764\n",
            "Epoch 41/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 523.1210 - MAE: 20.8289 - val_loss: 435.7469 - val_MAE: 19.8259\n",
            "Epoch 42/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 521.0228 - MAE: 20.7782 - val_loss: 433.7736 - val_MAE: 19.7761\n",
            "Epoch 43/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 518.9565 - MAE: 20.7292 - val_loss: 431.8064 - val_MAE: 19.7263\n",
            "Epoch 44/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 516.8967 - MAE: 20.6799 - val_loss: 429.8565 - val_MAE: 19.6768\n",
            "Epoch 45/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 514.8545 - MAE: 20.6295 - val_loss: 427.9126 - val_MAE: 19.6274\n",
            "Epoch 46/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 512.8192 - MAE: 20.5801 - val_loss: 425.9932 - val_MAE: 19.5784\n",
            "Epoch 47/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 510.8079 - MAE: 20.5311 - val_loss: 424.0570 - val_MAE: 19.5289\n",
            "Epoch 48/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 508.7799 - MAE: 20.4823 - val_loss: 422.1652 - val_MAE: 19.4804\n",
            "Epoch 49/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 506.7974 - MAE: 20.4334 - val_loss: 420.2661 - val_MAE: 19.4316\n",
            "Epoch 50/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 504.8075 - MAE: 20.3853 - val_loss: 418.3681 - val_MAE: 19.3827\n",
            "Epoch 51/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 502.8177 - MAE: 20.3361 - val_loss: 416.4725 - val_MAE: 19.3337\n",
            "Epoch 52/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 500.8310 - MAE: 20.2874 - val_loss: 414.5937 - val_MAE: 19.2851\n",
            "Epoch 53/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 498.8617 - MAE: 20.2383 - val_loss: 412.7215 - val_MAE: 19.2365\n",
            "Epoch 54/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 496.8988 - MAE: 20.1900 - val_loss: 410.8466 - val_MAE: 19.1877\n",
            "Epoch 55/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 494.9324 - MAE: 20.1408 - val_loss: 408.9911 - val_MAE: 19.1393\n",
            "Epoch 56/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 492.9873 - MAE: 20.0925 - val_loss: 407.1518 - val_MAE: 19.0912\n",
            "Epoch 57/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 491.0573 - MAE: 20.0447 - val_loss: 405.3058 - val_MAE: 19.0427\n",
            "Epoch 58/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 489.1216 - MAE: 19.9962 - val_loss: 403.4867 - val_MAE: 18.9949\n",
            "Epoch 59/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 487.2130 - MAE: 19.9488 - val_loss: 401.6636 - val_MAE: 18.9469\n",
            "Epoch 60/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 485.3000 - MAE: 19.9006 - val_loss: 399.8510 - val_MAE: 18.8990\n",
            "Epoch 61/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 483.3986 - MAE: 19.8528 - val_loss: 398.0489 - val_MAE: 18.8512\n",
            "Epoch 62/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 481.5073 - MAE: 19.8051 - val_loss: 396.2692 - val_MAE: 18.8040\n",
            "Epoch 63/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 479.6393 - MAE: 19.7580 - val_loss: 394.4786 - val_MAE: 18.7563\n",
            "Epoch 64/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 477.7602 - MAE: 19.7099 - val_loss: 392.7142 - val_MAE: 18.7092\n",
            "Epoch 65/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 475.9081 - MAE: 19.6628 - val_loss: 390.9628 - val_MAE: 18.6623\n",
            "Epoch 66/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 474.0680 - MAE: 19.6163 - val_loss: 389.1940 - val_MAE: 18.6149\n",
            "Epoch 67/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 472.2110 - MAE: 19.5695 - val_loss: 387.4441 - val_MAE: 18.5678\n",
            "Epoch 68/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 470.3728 - MAE: 19.5220 - val_loss: 385.6909 - val_MAE: 18.5206\n",
            "Epoch 69/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 468.5311 - MAE: 19.4747 - val_loss: 383.9519 - val_MAE: 18.4736\n",
            "Epoch 70/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 466.7047 - MAE: 19.4285 - val_loss: 382.2267 - val_MAE: 18.4268\n",
            "Epoch 71/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 464.8925 - MAE: 19.3814 - val_loss: 380.5272 - val_MAE: 18.3806\n",
            "Epoch 72/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 463.1068 - MAE: 19.3347 - val_loss: 378.8347 - val_MAE: 18.3345\n",
            "Epoch 73/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 461.3280 - MAE: 19.2886 - val_loss: 377.1364 - val_MAE: 18.2882\n",
            "Epoch 74/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 459.5430 - MAE: 19.2422 - val_loss: 375.4533 - val_MAE: 18.2421\n",
            "Epoch 75/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 457.7741 - MAE: 19.1963 - val_loss: 373.7822 - val_MAE: 18.1962\n",
            "Epoch 76/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 456.0166 - MAE: 19.1506 - val_loss: 372.1049 - val_MAE: 18.1501\n",
            "Epoch 77/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 454.2535 - MAE: 19.1042 - val_loss: 370.4489 - val_MAE: 18.1044\n",
            "Epoch 78/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 452.5117 - MAE: 19.0586 - val_loss: 368.7891 - val_MAE: 18.0585\n",
            "Epoch 79/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 450.7667 - MAE: 19.0134 - val_loss: 367.1552 - val_MAE: 18.0132\n",
            "Epoch 80/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 449.0473 - MAE: 18.9681 - val_loss: 365.5147 - val_MAE: 17.9676\n",
            "Epoch 81/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 447.3214 - MAE: 18.9226 - val_loss: 363.8730 - val_MAE: 17.9219\n",
            "Epoch 82/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 445.5949 - MAE: 18.8768 - val_loss: 362.2502 - val_MAE: 17.8765\n",
            "Epoch 83/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 443.8870 - MAE: 18.8313 - val_loss: 360.6395 - val_MAE: 17.8314\n",
            "Epoch 84/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 442.1917 - MAE: 18.7864 - val_loss: 359.0321 - val_MAE: 17.7863\n",
            "Epoch 85/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 440.4998 - MAE: 18.7413 - val_loss: 357.4241 - val_MAE: 17.7410\n",
            "Epoch 86/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 438.8084 - MAE: 18.6958 - val_loss: 355.8484 - val_MAE: 17.6966\n",
            "Epoch 87/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 437.1486 - MAE: 18.6518 - val_loss: 354.2667 - val_MAE: 17.6518\n",
            "Epoch 88/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 435.4828 - MAE: 18.6068 - val_loss: 352.6919 - val_MAE: 17.6072\n",
            "Epoch 89/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 433.8246 - MAE: 18.5623 - val_loss: 351.1323 - val_MAE: 17.5628\n",
            "Epoch 90/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 432.1826 - MAE: 18.5184 - val_loss: 349.5871 - val_MAE: 17.5188\n",
            "Epoch 91/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 430.5541 - MAE: 18.4741 - val_loss: 348.0325 - val_MAE: 17.4743\n",
            "Epoch 92/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 428.9161 - MAE: 18.4296 - val_loss: 346.4766 - val_MAE: 17.4298\n",
            "Epoch 93/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 427.2769 - MAE: 18.3847 - val_loss: 344.9384 - val_MAE: 17.3856\n",
            "Epoch 94/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 425.6571 - MAE: 18.3410 - val_loss: 343.4340 - val_MAE: 17.3423\n",
            "Epoch 95/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 424.0704 - MAE: 18.2974 - val_loss: 341.9140 - val_MAE: 17.2984\n",
            "Epoch 96/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 422.4687 - MAE: 18.2541 - val_loss: 340.4026 - val_MAE: 17.2547\n",
            "Epoch 97/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 420.8751 - MAE: 18.2099 - val_loss: 338.8886 - val_MAE: 17.2107\n",
            "Epoch 98/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 419.2782 - MAE: 18.1663 - val_loss: 337.3694 - val_MAE: 17.1665\n",
            "Epoch 99/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 417.6770 - MAE: 18.1226 - val_loss: 335.8857 - val_MAE: 17.1233\n",
            "Epoch 100/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 416.1117 - MAE: 18.0792 - val_loss: 334.3989 - val_MAE: 17.0798\n",
            "4/4 [==============================] - 0s 4ms/step - loss: 430.6453 - MAE: 18.6387\n",
            "Epoch 1/100\n",
            "12/12 [==============================] - 1s 16ms/step - loss: 7793.9702 - MAE: 80.3827 - val_loss: 7021.1401 - val_MAE: 76.9776\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7026.9209 - MAE: 75.8215 - val_loss: 6336.8911 - val_MAE: 72.8007\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6335.4224 - MAE: 71.4668 - val_loss: 5716.0259 - val_MAE: 68.7553\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5704.3315 - MAE: 67.1667 - val_loss: 5162.2598 - val_MAE: 65.0439\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5151.4082 - MAE: 63.2182 - val_loss: 4655.5879 - val_MAE: 61.3824\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4647.3389 - MAE: 59.4561 - val_loss: 4206.3584 - val_MAE: 57.9887\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4202.0073 - MAE: 56.1839 - val_loss: 3815.3660 - val_MAE: 55.1394\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3813.7156 - MAE: 53.4690 - val_loss: 3469.8940 - val_MAE: 52.7231\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3470.4558 - MAE: 51.1336 - val_loss: 3164.5964 - val_MAE: 50.5118\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3174.2180 - MAE: 49.1165 - val_loss: 2894.6316 - val_MAE: 48.5196\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2911.0662 - MAE: 47.2376 - val_loss: 2652.7344 - val_MAE: 46.5792\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2683.5388 - MAE: 45.5746 - val_loss: 2440.1167 - val_MAE: 44.7805\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2475.7434 - MAE: 43.7962 - val_loss: 2260.9961 - val_MAE: 43.1749\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2300.9944 - MAE: 42.2437 - val_loss: 2100.3967 - val_MAE: 41.6256\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2144.5518 - MAE: 40.7657 - val_loss: 1953.3588 - val_MAE: 40.1807\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2001.7534 - MAE: 39.3229 - val_loss: 1824.1315 - val_MAE: 38.8564\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1878.2533 - MAE: 38.0849 - val_loss: 1707.4792 - val_MAE: 37.6791\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1766.5721 - MAE: 36.9084 - val_loss: 1602.5476 - val_MAE: 36.6093\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1668.3188 - MAE: 35.8346 - val_loss: 1508.7599 - val_MAE: 35.6055\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1577.5918 - MAE: 34.8455 - val_loss: 1423.4034 - val_MAE: 34.6334\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1496.6238 - MAE: 33.9175 - val_loss: 1345.6583 - val_MAE: 33.6943\n",
            "Epoch 22/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1421.0187 - MAE: 33.0566 - val_loss: 1274.3146 - val_MAE: 32.7870\n",
            "Epoch 23/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1353.8339 - MAE: 32.2331 - val_loss: 1209.4034 - val_MAE: 31.9213\n",
            "Epoch 24/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1293.3372 - MAE: 31.4846 - val_loss: 1148.5615 - val_MAE: 31.0717\n",
            "Epoch 25/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1233.8313 - MAE: 30.7594 - val_loss: 1092.8162 - val_MAE: 30.2649\n",
            "Epoch 26/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1179.7134 - MAE: 30.0741 - val_loss: 1040.4404 - val_MAE: 29.4771\n",
            "Epoch 27/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1128.3612 - MAE: 29.4244 - val_loss: 990.5715 - val_MAE: 28.6990\n",
            "Epoch 28/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1080.0441 - MAE: 28.7923 - val_loss: 943.2675 - val_MAE: 27.9387\n",
            "Epoch 29/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1034.5476 - MAE: 28.1927 - val_loss: 897.9095 - val_MAE: 27.2096\n",
            "Epoch 30/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 990.4180 - MAE: 27.5933 - val_loss: 856.2800 - val_MAE: 26.5383\n",
            "Epoch 31/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 950.8003 - MAE: 27.0376 - val_loss: 815.7427 - val_MAE: 25.8819\n",
            "Epoch 32/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 910.8495 - MAE: 26.4704 - val_loss: 777.3751 - val_MAE: 25.2210\n",
            "Epoch 33/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 872.7270 - MAE: 25.9098 - val_loss: 741.8725 - val_MAE: 24.6052\n",
            "Epoch 34/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 838.0963 - MAE: 25.3757 - val_loss: 706.7032 - val_MAE: 23.9727\n",
            "Epoch 35/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 803.3309 - MAE: 24.8439 - val_loss: 673.9482 - val_MAE: 23.3705\n",
            "Epoch 36/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 771.3574 - MAE: 24.3173 - val_loss: 642.2474 - val_MAE: 22.7746\n",
            "Epoch 37/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 739.6415 - MAE: 23.8022 - val_loss: 612.7393 - val_MAE: 22.2040\n",
            "Epoch 38/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 710.6755 - MAE: 23.2897 - val_loss: 583.9825 - val_MAE: 21.6187\n",
            "Epoch 39/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 681.7645 - MAE: 22.7918 - val_loss: 557.2991 - val_MAE: 21.0877\n",
            "Epoch 40/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 655.2712 - MAE: 22.3083 - val_loss: 531.1754 - val_MAE: 20.5633\n",
            "Epoch 41/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 628.8886 - MAE: 21.8230 - val_loss: 506.5040 - val_MAE: 20.0484\n",
            "Epoch 42/100\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 604.3976 - MAE: 21.3565 - val_loss: 482.7787 - val_MAE: 19.5313\n",
            "Epoch 43/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 580.4760 - MAE: 20.8864 - val_loss: 460.1607 - val_MAE: 19.0382\n",
            "Epoch 44/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 557.5646 - MAE: 20.4328 - val_loss: 438.9131 - val_MAE: 18.5535\n",
            "Epoch 45/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 535.9130 - MAE: 19.9823 - val_loss: 418.6068 - val_MAE: 18.0866\n",
            "Epoch 46/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 515.0260 - MAE: 19.5434 - val_loss: 398.6575 - val_MAE: 17.6319\n",
            "Epoch 47/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 495.1033 - MAE: 19.0949 - val_loss: 379.6048 - val_MAE: 17.1854\n",
            "Epoch 48/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 476.0299 - MAE: 18.6737 - val_loss: 361.6631 - val_MAE: 16.7420\n",
            "Epoch 49/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 457.7574 - MAE: 18.2488 - val_loss: 344.8990 - val_MAE: 16.3197\n",
            "Epoch 50/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 440.8987 - MAE: 17.8503 - val_loss: 329.2533 - val_MAE: 15.9111\n",
            "Epoch 51/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 425.0519 - MAE: 17.4520 - val_loss: 314.3745 - val_MAE: 15.5050\n",
            "Epoch 52/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 409.4653 - MAE: 17.0744 - val_loss: 300.6524 - val_MAE: 15.1098\n",
            "Epoch 53/100\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 395.2957 - MAE: 16.7093 - val_loss: 287.0985 - val_MAE: 14.7085\n",
            "Epoch 54/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 381.0865 - MAE: 16.3468 - val_loss: 274.9301 - val_MAE: 14.3374\n",
            "Epoch 55/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 368.1441 - MAE: 16.0044 - val_loss: 262.9276 - val_MAE: 13.9552\n",
            "Epoch 56/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 355.7165 - MAE: 15.6704 - val_loss: 251.9109 - val_MAE: 13.6005\n",
            "Epoch 57/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 343.5642 - MAE: 15.3339 - val_loss: 241.4028 - val_MAE: 13.2581\n",
            "Epoch 58/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 332.6851 - MAE: 15.0259 - val_loss: 231.5267 - val_MAE: 12.9162\n",
            "Epoch 59/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 321.9015 - MAE: 14.7199 - val_loss: 222.1641 - val_MAE: 12.5943\n",
            "Epoch 60/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 311.9385 - MAE: 14.4199 - val_loss: 213.3421 - val_MAE: 12.2822\n",
            "Epoch 61/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 302.5381 - MAE: 14.1407 - val_loss: 205.1320 - val_MAE: 11.9814\n",
            "Epoch 62/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 293.5591 - MAE: 13.8721 - val_loss: 197.7036 - val_MAE: 11.7011\n",
            "Epoch 63/100\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 285.3822 - MAE: 13.6049 - val_loss: 190.4468 - val_MAE: 11.4291\n",
            "Epoch 64/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 277.6581 - MAE: 13.3546 - val_loss: 183.6685 - val_MAE: 11.1694\n",
            "Epoch 65/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 269.9245 - MAE: 13.1109 - val_loss: 177.4445 - val_MAE: 10.9211\n",
            "Epoch 66/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 262.8886 - MAE: 12.8728 - val_loss: 171.0562 - val_MAE: 10.6546\n",
            "Epoch 67/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 255.8659 - MAE: 12.6404 - val_loss: 165.4527 - val_MAE: 10.4122\n",
            "Epoch 68/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 249.5693 - MAE: 12.4329 - val_loss: 160.0944 - val_MAE: 10.1823\n",
            "Epoch 69/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 243.4672 - MAE: 12.2464 - val_loss: 155.2348 - val_MAE: 9.9915\n",
            "Epoch 70/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 237.9176 - MAE: 12.0517 - val_loss: 150.3325 - val_MAE: 9.7902\n",
            "Epoch 71/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 232.2009 - MAE: 11.8587 - val_loss: 146.1746 - val_MAE: 9.6137\n",
            "Epoch 72/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 227.2771 - MAE: 11.6963 - val_loss: 142.1255 - val_MAE: 9.4345\n",
            "Epoch 73/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 222.4419 - MAE: 11.5172 - val_loss: 138.0720 - val_MAE: 9.2483\n",
            "Epoch 74/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 218.0051 - MAE: 11.3563 - val_loss: 134.4905 - val_MAE: 9.0780\n",
            "Epoch 75/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 213.7963 - MAE: 11.1995 - val_loss: 131.0635 - val_MAE: 8.9079\n",
            "Epoch 76/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 209.8887 - MAE: 11.0565 - val_loss: 128.0229 - val_MAE: 8.7633\n",
            "Epoch 77/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 206.1862 - MAE: 10.9277 - val_loss: 125.3798 - val_MAE: 8.6310\n",
            "Epoch 78/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 202.6845 - MAE: 10.8092 - val_loss: 122.7363 - val_MAE: 8.5066\n",
            "Epoch 79/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 199.4203 - MAE: 10.6962 - val_loss: 120.0185 - val_MAE: 8.3972\n",
            "Epoch 80/100\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 196.2456 - MAE: 10.5749 - val_loss: 117.5619 - val_MAE: 8.2936\n",
            "Epoch 81/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 193.1738 - MAE: 10.4695 - val_loss: 115.4373 - val_MAE: 8.1961\n",
            "Epoch 82/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 190.5030 - MAE: 10.3838 - val_loss: 113.4379 - val_MAE: 8.1062\n",
            "Epoch 83/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 187.7676 - MAE: 10.2821 - val_loss: 111.4850 - val_MAE: 8.0206\n",
            "Epoch 84/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 185.1636 - MAE: 10.1916 - val_loss: 109.5142 - val_MAE: 7.9296\n",
            "Epoch 85/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 182.5681 - MAE: 10.1140 - val_loss: 107.9111 - val_MAE: 7.8588\n",
            "Epoch 86/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 180.4486 - MAE: 10.0604 - val_loss: 106.5831 - val_MAE: 7.8096\n",
            "Epoch 87/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 178.0493 - MAE: 9.9811 - val_loss: 104.7424 - val_MAE: 7.7448\n",
            "Epoch 88/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 175.8977 - MAE: 9.9076 - val_loss: 103.1278 - val_MAE: 7.6871\n",
            "Epoch 89/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 173.6904 - MAE: 9.8402 - val_loss: 101.7028 - val_MAE: 7.6339\n",
            "Epoch 90/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 171.7297 - MAE: 9.7801 - val_loss: 100.4044 - val_MAE: 7.5838\n",
            "Epoch 91/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 170.0922 - MAE: 9.7299 - val_loss: 99.0640 - val_MAE: 7.5362\n",
            "Epoch 92/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 168.3436 - MAE: 9.6868 - val_loss: 98.2850 - val_MAE: 7.5077\n",
            "Epoch 93/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 166.6364 - MAE: 9.6420 - val_loss: 97.3316 - val_MAE: 7.4787\n",
            "Epoch 94/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 165.0614 - MAE: 9.5900 - val_loss: 96.1532 - val_MAE: 7.4405\n",
            "Epoch 95/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 163.7136 - MAE: 9.5452 - val_loss: 94.8980 - val_MAE: 7.3991\n",
            "Epoch 96/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 162.0891 - MAE: 9.4902 - val_loss: 93.9439 - val_MAE: 7.3657\n",
            "Epoch 97/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 160.8691 - MAE: 9.4498 - val_loss: 93.0909 - val_MAE: 7.3359\n",
            "Epoch 98/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 159.4219 - MAE: 9.4104 - val_loss: 92.2424 - val_MAE: 7.3049\n",
            "Epoch 99/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 158.2405 - MAE: 9.3689 - val_loss: 91.2809 - val_MAE: 7.2681\n",
            "Epoch 100/100\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 156.8911 - MAE: 9.3368 - val_loss: 90.7445 - val_MAE: 7.2476\n",
            "4/4 [==============================] - 0s 5ms/step - loss: 144.2864 - MAE: 9.1756\n"
          ]
        }
      ],
      "source": [
        "opt = []\n",
        "for x in [\"sgd\",\"nesterov\",\"momentum\", \"adam\"]:\n",
        "    model = build_model(1,25,x,10e-5)\n",
        "    tensorboard_cb = tf.keras.callbacks.TensorBoard(get_run_logdir(\"opt\",x))\n",
        "    model.fit(X_train, y_train, epochs=100,  callbacks=[tensorboard_cb,es], validation_split=0.1)\n",
        "    ev = model.evaluate(X_test, y_test)\n",
        "    opt.append((x,ev[0],ev[1]))\n",
        "import pickle\n",
        "pickle.dump(opt, open(\"opt.pkl\", \"wb\")) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b9800ad-1539-4d7f-a039-8a9e27a7b4f9",
      "metadata": {
        "id": "3b9800ad-1539-4d7f-a039-8a9e27a7b4f9"
      },
      "outputs": [],
      "source": [
        "mom = []\n",
        "for x in [0.1,0.5,0.9]:\n",
        "    model = build_model(1,25,\"sgd\",10e-5,x)\n",
        "    tensorboard_cb = tf.keras.callbacks.TensorBoard(get_run_logdir(\"mom\",x))\n",
        "    model.fit(X_train, y_train, epochs=100,  callbacks=[tensorboard_cb,es], validation_split=0.1)\n",
        "    ev = model.evaluate(X_test, y_test)\n",
        "    opt.append((x,ev[0],ev[1]))\n",
        "import pickle\n",
        "pickle.dump(mom, open(\"mom.pkl\", \"wb\")) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_distribs = {\n",
        "  \"model__n_hidden\": [0,1,2,3],\n",
        "  \"model__n_neurons\": [5,25,125],\n",
        "  \"model__learning_rate\": [10e-6,10e-5,10e-4],\n",
        "  \"model__optimizer\": [\"sgd\",\"nesterov\",\"momentum\", \"adam\"],\n",
        "  \"model__momentum\": [0.1,0.5,0.9]\n",
        "}\n"
      ],
      "metadata": {
        "id": "0F6mlJW4tkgF"
      },
      "id": "0F6mlJW4tkgF",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scikeras\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "es = tf.keras.callbacks.EarlyStopping(patience=10, min_delta=1.0, verbose=1)\n",
        "keras_reg = KerasRegressor(build_model, callbacks=[es])"
      ],
      "metadata": {
        "id": "uLU5EeMIty4A"
      },
      "id": "uLU5EeMIty4A",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "rnd_search_cv = RandomizedSearchCV(keras_reg,\n",
        "param_distribs,\n",
        "n_iter=30,\n",
        "cv=3,\n",
        "verbose=2)\n",
        "rnd_search_cv.fit(X_train, y_train, epochs=100, validation_split=0.1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqyVScajt2GB",
        "outputId": "34ee37c0-dc49-45e1-93cb-a41bbd451dbe"
      },
      "id": "uqyVScajt2GB",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 32ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=sgd; total time=   1.9s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 23ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=sgd; total time=   1.8s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 24ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=sgd; total time=   2.4s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 25ms/step - loss: 1773.2534 - MAE: 40.6117 - val_loss: 1676.9166 - val_MAE: 40.2374\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1770.5439 - MAE: 40.5797 - val_loss: 1674.1443 - val_MAE: 40.2043\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1767.8197 - MAE: 40.5476 - val_loss: 1671.3817 - val_MAE: 40.1713\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1765.0963 - MAE: 40.5157 - val_loss: 1668.6343 - val_MAE: 40.1385\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1762.4177 - MAE: 40.4840 - val_loss: 1665.8810 - val_MAE: 40.1055\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1759.6946 - MAE: 40.4520 - val_loss: 1663.1443 - val_MAE: 40.0727\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1757.0039 - MAE: 40.4203 - val_loss: 1660.3990 - val_MAE: 40.0398\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1754.2893 - MAE: 40.3885 - val_loss: 1657.6643 - val_MAE: 40.0070\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1751.6044 - MAE: 40.3568 - val_loss: 1654.9197 - val_MAE: 39.9740\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1748.9125 - MAE: 40.3249 - val_loss: 1652.1935 - val_MAE: 39.9413\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1746.2534 - MAE: 40.2934 - val_loss: 1649.4633 - val_MAE: 39.9084\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1743.5674 - MAE: 40.2616 - val_loss: 1646.7678 - val_MAE: 39.8760\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1740.8899 - MAE: 40.2301 - val_loss: 1644.0846 - val_MAE: 39.8436\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1738.2917 - MAE: 40.1988 - val_loss: 1641.3552 - val_MAE: 39.8107\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1735.5828 - MAE: 40.1671 - val_loss: 1638.6689 - val_MAE: 39.7783\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1732.9296 - MAE: 40.1355 - val_loss: 1635.9780 - val_MAE: 39.7458\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1730.3159 - MAE: 40.1042 - val_loss: 1633.2697 - val_MAE: 39.7130\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1727.6713 - MAE: 40.0727 - val_loss: 1630.5740 - val_MAE: 39.6804\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1724.9879 - MAE: 40.0412 - val_loss: 1627.9417 - val_MAE: 39.6485\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1722.3776 - MAE: 40.0100 - val_loss: 1625.2798 - val_MAE: 39.6163\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1719.7705 - MAE: 39.9787 - val_loss: 1622.5913 - val_MAE: 39.5837\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1717.1387 - MAE: 39.9474 - val_loss: 1619.9272 - val_MAE: 39.5513\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1714.5493 - MAE: 39.9164 - val_loss: 1617.2555 - val_MAE: 39.5188\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1711.9182 - MAE: 39.8850 - val_loss: 1614.6163 - val_MAE: 39.4867\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1709.3418 - MAE: 39.8539 - val_loss: 1611.9653 - val_MAE: 39.4545\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1706.7178 - MAE: 39.8227 - val_loss: 1609.3451 - val_MAE: 39.4225\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1704.1339 - MAE: 39.7918 - val_loss: 1606.7128 - val_MAE: 39.3904\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1701.5651 - MAE: 39.7609 - val_loss: 1604.0984 - val_MAE: 39.3585\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1699.0016 - MAE: 39.7301 - val_loss: 1601.4728 - val_MAE: 39.3265\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1696.4027 - MAE: 39.6988 - val_loss: 1598.8600 - val_MAE: 39.2945\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1693.8438 - MAE: 39.6680 - val_loss: 1596.2563 - val_MAE: 39.2627\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1691.2726 - MAE: 39.6371 - val_loss: 1593.6650 - val_MAE: 39.2310\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1688.7629 - MAE: 39.6067 - val_loss: 1591.0522 - val_MAE: 39.1989\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1686.1942 - MAE: 39.5760 - val_loss: 1588.4998 - val_MAE: 39.1676\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 1683.6906 - MAE: 39.5457 - val_loss: 1585.9497 - val_MAE: 39.1363\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 1681.1455 - MAE: 39.5152 - val_loss: 1583.4198 - val_MAE: 39.1052\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 1678.6852 - MAE: 39.4852 - val_loss: 1580.8669 - val_MAE: 39.0739\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1676.1556 - MAE: 39.4546 - val_loss: 1578.3475 - val_MAE: 39.0429\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 1673.6360 - MAE: 39.4241 - val_loss: 1575.8477 - val_MAE: 39.0122\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1671.1313 - MAE: 39.3939 - val_loss: 1573.3577 - val_MAE: 38.9815\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1668.6398 - MAE: 39.3635 - val_loss: 1570.8669 - val_MAE: 38.9508\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 1666.1357 - MAE: 39.3332 - val_loss: 1568.3977 - val_MAE: 38.9203\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 1663.6434 - MAE: 39.3029 - val_loss: 1565.9313 - val_MAE: 38.8899\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 1661.2026 - MAE: 39.2730 - val_loss: 1563.4280 - val_MAE: 38.8590\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 1658.6733 - MAE: 39.2425 - val_loss: 1560.9810 - val_MAE: 38.8287\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 1656.2329 - MAE: 39.2127 - val_loss: 1558.5200 - val_MAE: 38.7982\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 1653.7521 - MAE: 39.1826 - val_loss: 1556.0737 - val_MAE: 38.7679\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1651.2976 - MAE: 39.1526 - val_loss: 1553.6248 - val_MAE: 38.7376\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 1648.8374 - MAE: 39.1228 - val_loss: 1551.1813 - val_MAE: 38.7073\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 1646.3800 - MAE: 39.0926 - val_loss: 1548.7185 - val_MAE: 38.6767\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 1643.9132 - MAE: 39.0625 - val_loss: 1546.2810 - val_MAE: 38.6464\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 1641.4661 - MAE: 39.0326 - val_loss: 1543.8330 - val_MAE: 38.6160\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 1639.0129 - MAE: 39.0026 - val_loss: 1541.4005 - val_MAE: 38.5857\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 1636.5801 - MAE: 38.9725 - val_loss: 1538.9552 - val_MAE: 38.5552\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1634.1390 - MAE: 38.9426 - val_loss: 1536.5165 - val_MAE: 38.5248\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1631.6722 - MAE: 38.9126 - val_loss: 1534.1044 - val_MAE: 38.4947\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1629.2483 - MAE: 38.8829 - val_loss: 1531.6884 - val_MAE: 38.4646\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1626.8376 - MAE: 38.8530 - val_loss: 1529.2540 - val_MAE: 38.4342\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1624.3842 - MAE: 38.8230 - val_loss: 1526.8553 - val_MAE: 38.4042\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1622.0046 - MAE: 38.7936 - val_loss: 1524.4459 - val_MAE: 38.3740\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1619.5603 - MAE: 38.7637 - val_loss: 1522.0616 - val_MAE: 38.3441\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1617.1793 - MAE: 38.7341 - val_loss: 1519.6686 - val_MAE: 38.3141\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1614.7733 - MAE: 38.7045 - val_loss: 1517.2844 - val_MAE: 38.2842\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 1612.3778 - MAE: 38.6749 - val_loss: 1514.8936 - val_MAE: 38.2542\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1609.9626 - MAE: 38.6454 - val_loss: 1512.5193 - val_MAE: 38.2244\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1607.6034 - MAE: 38.6158 - val_loss: 1510.1138 - val_MAE: 38.1941\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1605.2025 - MAE: 38.5861 - val_loss: 1507.7268 - val_MAE: 38.1641\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1602.8363 - MAE: 38.5567 - val_loss: 1505.3405 - val_MAE: 38.1340\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1600.4635 - MAE: 38.5273 - val_loss: 1502.9805 - val_MAE: 38.1043\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1598.0912 - MAE: 38.4980 - val_loss: 1500.6393 - val_MAE: 38.0747\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1595.7542 - MAE: 38.4689 - val_loss: 1498.3055 - val_MAE: 38.0453\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1593.4170 - MAE: 38.4401 - val_loss: 1495.9575 - val_MAE: 38.0156\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1591.0819 - MAE: 38.4110 - val_loss: 1493.6326 - val_MAE: 37.9862\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1588.7546 - MAE: 38.3822 - val_loss: 1491.3038 - val_MAE: 37.9567\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1586.4463 - MAE: 38.3534 - val_loss: 1488.9556 - val_MAE: 37.9270\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1584.1018 - MAE: 38.3245 - val_loss: 1486.6398 - val_MAE: 37.8976\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1581.7924 - MAE: 38.2955 - val_loss: 1484.3225 - val_MAE: 37.8682\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1579.4803 - MAE: 38.2668 - val_loss: 1482.0107 - val_MAE: 37.8389\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1577.1691 - MAE: 38.2380 - val_loss: 1479.6991 - val_MAE: 37.8095\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 1574.8490 - MAE: 38.2092 - val_loss: 1477.4052 - val_MAE: 37.7803\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1572.5796 - MAE: 38.1805 - val_loss: 1475.0839 - val_MAE: 37.7508\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1570.2415 - MAE: 38.1517 - val_loss: 1472.7803 - val_MAE: 37.7215\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1567.9601 - MAE: 38.1230 - val_loss: 1470.4719 - val_MAE: 37.6921\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 1565.6692 - MAE: 38.0945 - val_loss: 1468.1868 - val_MAE: 37.6629\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1563.3885 - MAE: 38.0661 - val_loss: 1465.9198 - val_MAE: 37.6340\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1561.1248 - MAE: 38.0378 - val_loss: 1463.6599 - val_MAE: 37.6051\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1558.8630 - MAE: 38.0093 - val_loss: 1461.4005 - val_MAE: 37.5762\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1556.6433 - MAE: 37.9812 - val_loss: 1459.1147 - val_MAE: 37.5470\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1554.3710 - MAE: 37.9528 - val_loss: 1456.8408 - val_MAE: 37.5178\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1552.1144 - MAE: 37.9243 - val_loss: 1454.5883 - val_MAE: 37.4890\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1549.8801 - MAE: 37.8962 - val_loss: 1452.3383 - val_MAE: 37.4601\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1547.6310 - MAE: 37.8680 - val_loss: 1450.0999 - val_MAE: 37.4314\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1545.3821 - MAE: 37.8399 - val_loss: 1447.8640 - val_MAE: 37.4027\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 1543.1660 - MAE: 37.8119 - val_loss: 1445.6194 - val_MAE: 37.3738\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1540.9331 - MAE: 37.7838 - val_loss: 1443.3805 - val_MAE: 37.3450\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1538.7037 - MAE: 37.7557 - val_loss: 1441.1451 - val_MAE: 37.3162\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 1536.4968 - MAE: 37.7278 - val_loss: 1438.9041 - val_MAE: 37.2873\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 1534.2562 - MAE: 37.6996 - val_loss: 1436.6792 - val_MAE: 37.2586\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1532.0420 - MAE: 37.6716 - val_loss: 1434.4445 - val_MAE: 37.2298\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1529.8373 - MAE: 37.6439 - val_loss: 1432.2318 - val_MAE: 37.2012\n",
            "5/5 [==============================] - 0s 5ms/step\n",
            "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=2, model__n_neurons=5, model__optimizer=adam; total time=   9.3s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 43ms/step - loss: 567.5124 - MAE: 20.1820 - val_loss: 540.7092 - val_MAE: 20.4899\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 566.3146 - MAE: 20.1685 - val_loss: 539.0040 - val_MAE: 20.4629\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 565.1281 - MAE: 20.1559 - val_loss: 537.2504 - val_MAE: 20.4353\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 563.9185 - MAE: 20.1433 - val_loss: 535.5980 - val_MAE: 20.4088\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 562.7956 - MAE: 20.1314 - val_loss: 533.9931 - val_MAE: 20.3829\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 561.6702 - MAE: 20.1181 - val_loss: 532.4760 - val_MAE: 20.3583\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 560.6451 - MAE: 20.1069 - val_loss: 530.9218 - val_MAE: 20.3332\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: 559.6684 - MAE: 20.0968 - val_loss: 529.2608 - val_MAE: 20.3066\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 558.4720 - MAE: 20.0835 - val_loss: 527.8243 - val_MAE: 20.2830\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 557.4341 - MAE: 20.0710 - val_loss: 526.3763 - val_MAE: 20.2591\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 556.4285 - MAE: 20.0587 - val_loss: 524.8571 - val_MAE: 20.2341\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 555.3600 - MAE: 20.0463 - val_loss: 523.4062 - val_MAE: 20.2100\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 554.3430 - MAE: 20.0349 - val_loss: 521.9412 - val_MAE: 20.1856\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 553.4099 - MAE: 20.0253 - val_loss: 520.3730 - val_MAE: 20.1597\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 552.3245 - MAE: 20.0124 - val_loss: 518.8943 - val_MAE: 20.1350\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 551.2933 - MAE: 19.9999 - val_loss: 517.5532 - val_MAE: 20.1121\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 550.3418 - MAE: 19.9886 - val_loss: 516.1169 - val_MAE: 20.0878\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 549.2797 - MAE: 19.9753 - val_loss: 514.7593 - val_MAE: 20.0647\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 548.3439 - MAE: 19.9646 - val_loss: 513.2784 - val_MAE: 20.0396\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 547.3223 - MAE: 19.9513 - val_loss: 511.8546 - val_MAE: 20.0152\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 546.3196 - MAE: 19.9385 - val_loss: 510.4488 - val_MAE: 19.9911\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 545.2936 - MAE: 19.9263 - val_loss: 509.0758 - val_MAE: 19.9675\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 544.3117 - MAE: 19.9138 - val_loss: 507.6948 - val_MAE: 19.9436\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 543.3760 - MAE: 19.9027 - val_loss: 506.2701 - val_MAE: 19.9190\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 542.3568 - MAE: 19.8893 - val_loss: 505.0022 - val_MAE: 19.8967\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 541.4535 - MAE: 19.8778 - val_loss: 503.6122 - val_MAE: 19.8724\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 540.4438 - MAE: 19.8648 - val_loss: 502.3167 - val_MAE: 19.8496\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 539.6051 - MAE: 19.8551 - val_loss: 500.8217 - val_MAE: 19.8237\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 538.5171 - MAE: 19.8414 - val_loss: 499.5359 - val_MAE: 19.8008\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 537.5861 - MAE: 19.8295 - val_loss: 498.2060 - val_MAE: 19.7772\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 536.6692 - MAE: 19.8176 - val_loss: 496.9187 - val_MAE: 19.7541\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 535.7359 - MAE: 19.8047 - val_loss: 495.6550 - val_MAE: 19.7314\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 534.8142 - MAE: 19.7917 - val_loss: 494.4420 - val_MAE: 19.7094\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 533.9047 - MAE: 19.7791 - val_loss: 493.1780 - val_MAE: 19.6865\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 533.0894 - MAE: 19.7691 - val_loss: 491.7254 - val_MAE: 19.6605\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 532.0304 - MAE: 19.7538 - val_loss: 490.5963 - val_MAE: 19.6397\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 531.1681 - MAE: 19.7419 - val_loss: 489.3614 - val_MAE: 19.6172\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 530.2656 - MAE: 19.7292 - val_loss: 488.1012 - val_MAE: 19.5941\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 529.3492 - MAE: 19.7171 - val_loss: 486.8425 - val_MAE: 19.5711\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 528.4796 - MAE: 19.7054 - val_loss: 485.5102 - val_MAE: 19.5468\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 527.5563 - MAE: 19.6921 - val_loss: 484.2297 - val_MAE: 19.5231\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 526.6384 - MAE: 19.6792 - val_loss: 483.0016 - val_MAE: 19.5004\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 525.7747 - MAE: 19.6671 - val_loss: 481.7754 - val_MAE: 19.4777\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 524.9497 - MAE: 19.6559 - val_loss: 480.5121 - val_MAE: 19.4543\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 524.0137 - MAE: 19.6415 - val_loss: 479.4477 - val_MAE: 19.4341\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 523.1628 - MAE: 19.6292 - val_loss: 478.2926 - val_MAE: 19.4124\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 522.2903 - MAE: 19.6158 - val_loss: 477.1833 - val_MAE: 19.3915\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 521.4769 - MAE: 19.6038 - val_loss: 475.9197 - val_MAE: 19.3679\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 520.6115 - MAE: 19.5920 - val_loss: 474.7123 - val_MAE: 19.3451\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 519.7464 - MAE: 19.5791 - val_loss: 473.5296 - val_MAE: 19.3227\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 518.8668 - MAE: 19.5658 - val_loss: 472.3326 - val_MAE: 19.3001\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 517.9893 - MAE: 19.5538 - val_loss: 471.1205 - val_MAE: 19.2770\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 517.1659 - MAE: 19.5422 - val_loss: 469.8731 - val_MAE: 19.2533\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 516.2569 - MAE: 19.5285 - val_loss: 468.7568 - val_MAE: 19.2318\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 515.4160 - MAE: 19.5165 - val_loss: 467.6014 - val_MAE: 19.2096\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 514.6025 - MAE: 19.5043 - val_loss: 466.4434 - val_MAE: 19.1873\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 513.7847 - MAE: 19.4925 - val_loss: 465.3264 - val_MAE: 19.1656\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 512.9611 - MAE: 19.4797 - val_loss: 464.2119 - val_MAE: 19.1440\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 512.1367 - MAE: 19.4667 - val_loss: 463.1075 - val_MAE: 19.1225\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 511.3271 - MAE: 19.4540 - val_loss: 462.0870 - val_MAE: 19.1024\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 510.5384 - MAE: 19.4406 - val_loss: 461.0733 - val_MAE: 19.0824\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 509.7455 - MAE: 19.4277 - val_loss: 460.0106 - val_MAE: 19.0616\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 508.9387 - MAE: 19.4149 - val_loss: 458.9254 - val_MAE: 19.0402\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 508.1070 - MAE: 19.4018 - val_loss: 457.9086 - val_MAE: 19.0201\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 507.3235 - MAE: 19.3894 - val_loss: 456.7856 - val_MAE: 18.9980\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 506.5685 - MAE: 19.3785 - val_loss: 455.5739 - val_MAE: 18.9742\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 505.6904 - MAE: 19.3646 - val_loss: 454.5782 - val_MAE: 18.9543\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 504.9184 - MAE: 19.3525 - val_loss: 453.4784 - val_MAE: 18.9325\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 504.1187 - MAE: 19.3395 - val_loss: 452.4750 - val_MAE: 18.9123\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 503.3400 - MAE: 19.3268 - val_loss: 451.4469 - val_MAE: 18.8918\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 502.5703 - MAE: 19.3144 - val_loss: 450.4163 - val_MAE: 18.8711\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 501.8174 - MAE: 19.3019 - val_loss: 449.3164 - val_MAE: 18.8491\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 500.9780 - MAE: 19.2885 - val_loss: 448.3896 - val_MAE: 18.8303\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 500.2328 - MAE: 19.2757 - val_loss: 447.3865 - val_MAE: 18.8100\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 499.4402 - MAE: 19.2624 - val_loss: 446.4193 - val_MAE: 18.7904\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 498.7063 - MAE: 19.2506 - val_loss: 445.3555 - val_MAE: 18.7689\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 497.9471 - MAE: 19.2384 - val_loss: 444.2846 - val_MAE: 18.7472\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 497.1192 - MAE: 19.2237 - val_loss: 443.4287 - val_MAE: 18.7296\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 496.3986 - MAE: 19.2116 - val_loss: 442.3585 - val_MAE: 18.7079\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 495.6118 - MAE: 19.1984 - val_loss: 441.4178 - val_MAE: 18.6886\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 494.8811 - MAE: 19.1860 - val_loss: 440.4398 - val_MAE: 18.6686\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 494.1354 - MAE: 19.1737 - val_loss: 439.4281 - val_MAE: 18.6479\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 493.3498 - MAE: 19.1603 - val_loss: 438.4434 - val_MAE: 18.6277\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 492.5972 - MAE: 19.1473 - val_loss: 437.5150 - val_MAE: 18.6085\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 491.8312 - MAE: 19.1341 - val_loss: 436.5484 - val_MAE: 18.5886\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 491.0916 - MAE: 19.1220 - val_loss: 435.5581 - val_MAE: 18.5682\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 490.3938 - MAE: 19.1097 - val_loss: 434.5321 - val_MAE: 18.5470\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 489.6146 - MAE: 19.0966 - val_loss: 433.6281 - val_MAE: 18.5282\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 488.9384 - MAE: 19.0843 - val_loss: 432.6639 - val_MAE: 18.5082\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 488.1339 - MAE: 19.0699 - val_loss: 431.8453 - val_MAE: 18.4911\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 487.4242 - MAE: 19.0572 - val_loss: 430.9065 - val_MAE: 18.4716\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 486.6883 - MAE: 19.0441 - val_loss: 430.0164 - val_MAE: 18.4529\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 486.0179 - MAE: 19.0324 - val_loss: 429.0383 - val_MAE: 18.4325\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 485.2145 - MAE: 19.0182 - val_loss: 428.2408 - val_MAE: 18.4157\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 484.5061 - MAE: 19.0045 - val_loss: 427.3972 - val_MAE: 18.3979\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 483.8240 - MAE: 18.9922 - val_loss: 426.3679 - val_MAE: 18.3763\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 483.0561 - MAE: 18.9791 - val_loss: 425.4623 - val_MAE: 18.3573\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 482.3051 - MAE: 18.9653 - val_loss: 424.6276 - val_MAE: 18.3396\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 481.6117 - MAE: 18.9529 - val_loss: 423.6744 - val_MAE: 18.3195\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 480.8985 - MAE: 18.9408 - val_loss: 422.7422 - val_MAE: 18.2998\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=2, model__n_neurons=5, model__optimizer=adam; total time=  11.4s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 20ms/step - loss: 3253.7637 - MAE: 49.4708 - val_loss: 2671.5388 - val_MAE: 46.3805\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3246.4641 - MAE: 49.4169 - val_loss: 2665.9229 - val_MAE: 46.3333\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 3239.4265 - MAE: 49.3646 - val_loss: 2660.2583 - val_MAE: 46.2857\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3232.4197 - MAE: 49.3117 - val_loss: 2654.6279 - val_MAE: 46.2384\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3225.3887 - MAE: 49.2587 - val_loss: 2648.9797 - val_MAE: 46.1907\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3218.1279 - MAE: 49.2049 - val_loss: 2643.4216 - val_MAE: 46.1436\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3210.8760 - MAE: 49.1516 - val_loss: 2637.8984 - val_MAE: 46.0967\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3204.1069 - MAE: 49.0997 - val_loss: 2632.2354 - val_MAE: 46.0486\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3196.7319 - MAE: 49.0463 - val_loss: 2626.7678 - val_MAE: 46.0020\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3189.9180 - MAE: 48.9939 - val_loss: 2621.1270 - val_MAE: 45.9540\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3182.6245 - MAE: 48.9404 - val_loss: 2615.6704 - val_MAE: 45.9073\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3175.7568 - MAE: 48.8886 - val_loss: 2610.1577 - val_MAE: 45.8602\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3168.8494 - MAE: 48.8358 - val_loss: 2604.5608 - val_MAE: 45.8124\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3161.7336 - MAE: 48.7826 - val_loss: 2599.0471 - val_MAE: 45.7652\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3154.7366 - MAE: 48.7299 - val_loss: 2593.5491 - val_MAE: 45.7180\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3147.7017 - MAE: 48.6780 - val_loss: 2588.1267 - val_MAE: 45.6713\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3140.9307 - MAE: 48.6252 - val_loss: 2582.6323 - val_MAE: 45.6242\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3133.8560 - MAE: 48.5731 - val_loss: 2577.1799 - val_MAE: 45.5773\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3126.8237 - MAE: 48.5206 - val_loss: 2571.7222 - val_MAE: 45.5304\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3120.1292 - MAE: 48.4681 - val_loss: 2566.1301 - val_MAE: 45.4827\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3112.7078 - MAE: 48.4140 - val_loss: 2560.8276 - val_MAE: 45.4370\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3106.2141 - MAE: 48.3631 - val_loss: 2555.3818 - val_MAE: 45.3904\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3099.1602 - MAE: 48.3108 - val_loss: 2550.0691 - val_MAE: 45.3446\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3092.3508 - MAE: 48.2594 - val_loss: 2544.7737 - val_MAE: 45.2989\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3085.6511 - MAE: 48.2080 - val_loss: 2539.4702 - val_MAE: 45.2531\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3078.7351 - MAE: 48.1561 - val_loss: 2534.2969 - val_MAE: 45.2084\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3072.2856 - MAE: 48.1058 - val_loss: 2528.8904 - val_MAE: 45.1618\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3065.3291 - MAE: 48.0535 - val_loss: 2523.6074 - val_MAE: 45.1162\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3058.5190 - MAE: 48.0026 - val_loss: 2518.4954 - val_MAE: 45.0719\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3052.0439 - MAE: 47.9524 - val_loss: 2513.3091 - val_MAE: 45.0271\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3045.2441 - MAE: 47.9013 - val_loss: 2508.1282 - val_MAE: 44.9822\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3038.6841 - MAE: 47.8508 - val_loss: 2502.9138 - val_MAE: 44.9371\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3031.9966 - MAE: 47.7999 - val_loss: 2497.6890 - val_MAE: 44.8917\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3025.4041 - MAE: 47.7492 - val_loss: 2492.4644 - val_MAE: 44.8464\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3018.7231 - MAE: 47.6984 - val_loss: 2487.3203 - val_MAE: 44.8017\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3011.9055 - MAE: 47.6468 - val_loss: 2482.3682 - val_MAE: 44.7591\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3005.4827 - MAE: 47.5971 - val_loss: 2477.2871 - val_MAE: 44.7157\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2998.8496 - MAE: 47.5466 - val_loss: 2472.1860 - val_MAE: 44.6720\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2992.3296 - MAE: 47.4968 - val_loss: 2467.0369 - val_MAE: 44.6280\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2985.7021 - MAE: 47.4459 - val_loss: 2461.9658 - val_MAE: 44.5847\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2979.0750 - MAE: 47.3953 - val_loss: 2456.9763 - val_MAE: 44.5420\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2972.5083 - MAE: 47.3456 - val_loss: 2452.0276 - val_MAE: 44.4996\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2966.2556 - MAE: 47.2967 - val_loss: 2446.9309 - val_MAE: 44.4560\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2959.6873 - MAE: 47.2465 - val_loss: 2441.9189 - val_MAE: 44.4129\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2953.2327 - MAE: 47.1968 - val_loss: 2436.9333 - val_MAE: 44.3700\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2946.6094 - MAE: 47.1468 - val_loss: 2432.0378 - val_MAE: 44.3278\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2940.4104 - MAE: 47.0974 - val_loss: 2426.9651 - val_MAE: 44.2841\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2933.7729 - MAE: 47.0474 - val_loss: 2421.9998 - val_MAE: 44.2413\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2927.3298 - MAE: 46.9983 - val_loss: 2417.0627 - val_MAE: 44.1986\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2920.7585 - MAE: 46.9482 - val_loss: 2412.2085 - val_MAE: 44.1566\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2914.5127 - MAE: 46.9002 - val_loss: 2407.2427 - val_MAE: 44.1136\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2908.0728 - MAE: 46.8514 - val_loss: 2402.3303 - val_MAE: 44.0711\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2901.7268 - MAE: 46.8032 - val_loss: 2397.4158 - val_MAE: 44.0284\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2895.4265 - MAE: 46.7543 - val_loss: 2392.4834 - val_MAE: 43.9858\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2889.0847 - MAE: 46.7065 - val_loss: 2387.6118 - val_MAE: 43.9435\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2883.0293 - MAE: 46.6591 - val_loss: 2382.6572 - val_MAE: 43.9005\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2876.5605 - MAE: 46.6109 - val_loss: 2377.8525 - val_MAE: 43.8586\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2870.3652 - MAE: 46.5638 - val_loss: 2373.1477 - val_MAE: 43.8182\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2864.2698 - MAE: 46.5172 - val_loss: 2368.4658 - val_MAE: 43.7781\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2858.1013 - MAE: 46.4705 - val_loss: 2363.7852 - val_MAE: 43.7379\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2852.2266 - MAE: 46.4244 - val_loss: 2358.9666 - val_MAE: 43.6966\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2845.7500 - MAE: 46.3766 - val_loss: 2354.3967 - val_MAE: 43.6571\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2839.7019 - MAE: 46.3301 - val_loss: 2349.7585 - val_MAE: 43.6172\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2833.7866 - MAE: 46.2846 - val_loss: 2344.9663 - val_MAE: 43.5760\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2827.4932 - MAE: 46.2367 - val_loss: 2340.3064 - val_MAE: 43.5359\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2821.3909 - MAE: 46.1905 - val_loss: 2335.6013 - val_MAE: 43.4953\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2815.2581 - MAE: 46.1441 - val_loss: 2330.9700 - val_MAE: 43.4553\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2809.3171 - MAE: 46.0975 - val_loss: 2326.2644 - val_MAE: 43.4147\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2803.1697 - MAE: 46.0521 - val_loss: 2321.6267 - val_MAE: 43.3745\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2797.3286 - MAE: 46.0066 - val_loss: 2317.0105 - val_MAE: 43.3345\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2791.3311 - MAE: 45.9610 - val_loss: 2312.3936 - val_MAE: 43.2944\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2785.3633 - MAE: 45.9156 - val_loss: 2307.8171 - val_MAE: 43.2546\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2779.3032 - MAE: 45.8698 - val_loss: 2303.2913 - val_MAE: 43.2151\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2773.3643 - MAE: 45.8242 - val_loss: 2298.6702 - val_MAE: 43.1749\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2767.5352 - MAE: 45.7788 - val_loss: 2293.9873 - val_MAE: 43.1342\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2761.2913 - MAE: 45.7317 - val_loss: 2289.4841 - val_MAE: 43.0947\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2755.4597 - MAE: 45.6864 - val_loss: 2284.9246 - val_MAE: 43.0549\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2749.5923 - MAE: 45.6415 - val_loss: 2280.3538 - val_MAE: 43.0148\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2743.6160 - MAE: 45.5951 - val_loss: 2275.8245 - val_MAE: 42.9752\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2737.7952 - MAE: 45.5504 - val_loss: 2271.1943 - val_MAE: 42.9347\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2731.9631 - MAE: 45.5049 - val_loss: 2266.5652 - val_MAE: 42.8942\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2725.8330 - MAE: 45.4585 - val_loss: 2262.1284 - val_MAE: 42.8552\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2720.0754 - MAE: 45.4137 - val_loss: 2257.7029 - val_MAE: 42.8166\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2714.3931 - MAE: 45.3693 - val_loss: 2253.3357 - val_MAE: 42.7788\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2708.4294 - MAE: 45.3238 - val_loss: 2249.0764 - val_MAE: 42.7418\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2702.7930 - MAE: 45.2794 - val_loss: 2244.7419 - val_MAE: 42.7041\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2696.9861 - MAE: 45.2349 - val_loss: 2240.4084 - val_MAE: 42.6664\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2691.2390 - MAE: 45.1901 - val_loss: 2236.0466 - val_MAE: 42.6284\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2685.5251 - MAE: 45.1454 - val_loss: 2231.7053 - val_MAE: 42.5907\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2679.5664 - MAE: 45.0998 - val_loss: 2227.3750 - val_MAE: 42.5530\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2673.9404 - MAE: 45.0560 - val_loss: 2222.7295 - val_MAE: 42.5124\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2668.1011 - MAE: 45.0106 - val_loss: 2218.1926 - val_MAE: 42.4726\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2662.3323 - MAE: 44.9659 - val_loss: 2213.7349 - val_MAE: 42.4334\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2656.4573 - MAE: 44.9207 - val_loss: 2209.4243 - val_MAE: 42.3954\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2651.0012 - MAE: 44.8780 - val_loss: 2204.8877 - val_MAE: 42.3554\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2645.0435 - MAE: 44.8323 - val_loss: 2200.5164 - val_MAE: 42.3168\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2639.7886 - MAE: 44.7898 - val_loss: 2195.8931 - val_MAE: 42.2761\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2633.6711 - MAE: 44.7433 - val_loss: 2191.5676 - val_MAE: 42.2378\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2628.1125 - MAE: 44.6998 - val_loss: 2187.2097 - val_MAE: 42.1992\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2622.4480 - MAE: 44.6563 - val_loss: 2182.9482 - val_MAE: 42.1613\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=2, model__n_neurons=5, model__optimizer=adam; total time=   5.6s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 21ms/step - loss: 1232.4990 - MAE: 29.3908 - val_loss: 447.4223 - val_MAE: 20.2314\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 595.9348 - MAE: 22.6421 - val_loss: 445.9611 - val_MAE: 20.1953\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 594.3009 - MAE: 22.6057 - val_loss: 444.5111 - val_MAE: 20.1593\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 592.6772 - MAE: 22.5699 - val_loss: 443.0428 - val_MAE: 20.1229\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 591.0353 - MAE: 22.5335 - val_loss: 441.5956 - val_MAE: 20.0869\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 589.4168 - MAE: 22.4971 - val_loss: 440.1600 - val_MAE: 20.0511\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 587.8090 - MAE: 22.4617 - val_loss: 438.7254 - val_MAE: 20.0153\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 586.2025 - MAE: 22.4263 - val_loss: 437.2885 - val_MAE: 19.9794\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 584.5923 - MAE: 22.3901 - val_loss: 435.8470 - val_MAE: 19.9433\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 582.9791 - MAE: 22.3539 - val_loss: 434.4280 - val_MAE: 19.9077\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 581.3888 - MAE: 22.3186 - val_loss: 433.0050 - val_MAE: 19.8719\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 579.7946 - MAE: 22.2828 - val_loss: 431.5886 - val_MAE: 19.8362\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 578.2066 - MAE: 22.2472 - val_loss: 430.1682 - val_MAE: 19.8004\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 576.6161 - MAE: 22.2112 - val_loss: 428.7642 - val_MAE: 19.7649\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 575.0410 - MAE: 22.1756 - val_loss: 427.3625 - val_MAE: 19.7294\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 573.4708 - MAE: 22.1400 - val_loss: 425.9773 - val_MAE: 19.6943\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 571.9158 - MAE: 22.1054 - val_loss: 424.5849 - val_MAE: 19.6589\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 570.3537 - MAE: 22.0700 - val_loss: 423.1962 - val_MAE: 19.6235\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 568.7958 - MAE: 22.0352 - val_loss: 421.8115 - val_MAE: 19.5882\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 567.2412 - MAE: 21.9995 - val_loss: 420.4279 - val_MAE: 19.5529\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 565.6880 - MAE: 21.9639 - val_loss: 419.0486 - val_MAE: 19.5176\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 564.1409 - MAE: 21.9287 - val_loss: 417.6875 - val_MAE: 19.4827\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 562.6115 - MAE: 21.8942 - val_loss: 416.3194 - val_MAE: 19.4475\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 561.0746 - MAE: 21.8588 - val_loss: 414.9549 - val_MAE: 19.4124\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 559.5420 - MAE: 21.8234 - val_loss: 413.5959 - val_MAE: 19.3774\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 558.0148 - MAE: 21.7886 - val_loss: 412.2416 - val_MAE: 19.3424\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 556.4935 - MAE: 21.7539 - val_loss: 410.8951 - val_MAE: 19.3076\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 554.9803 - MAE: 21.7189 - val_loss: 409.5540 - val_MAE: 19.2728\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 553.4714 - MAE: 21.6843 - val_loss: 408.2102 - val_MAE: 19.2379\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 551.9619 - MAE: 21.6490 - val_loss: 406.8850 - val_MAE: 19.2034\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 550.4716 - MAE: 21.6146 - val_loss: 405.5679 - val_MAE: 19.1691\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 548.9882 - MAE: 21.5805 - val_loss: 404.2341 - val_MAE: 19.1343\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 547.4877 - MAE: 21.5456 - val_loss: 402.9095 - val_MAE: 19.0996\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 545.9967 - MAE: 21.5112 - val_loss: 401.5880 - val_MAE: 19.0650\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 544.5099 - MAE: 21.4766 - val_loss: 400.2744 - val_MAE: 19.0305\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 543.0317 - MAE: 21.4415 - val_loss: 398.9751 - val_MAE: 18.9964\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 541.5682 - MAE: 21.4078 - val_loss: 397.6790 - val_MAE: 18.9622\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 540.1095 - MAE: 21.3739 - val_loss: 396.3955 - val_MAE: 18.9284\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 538.6617 - MAE: 21.3400 - val_loss: 395.0970 - val_MAE: 18.8940\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 537.1993 - MAE: 21.3060 - val_loss: 393.8042 - val_MAE: 18.8598\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 535.7421 - MAE: 21.2711 - val_loss: 392.5215 - val_MAE: 18.8257\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 534.2963 - MAE: 21.2373 - val_loss: 391.2452 - val_MAE: 18.7918\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 532.8580 - MAE: 21.2032 - val_loss: 389.9778 - val_MAE: 18.7581\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 531.4278 - MAE: 21.1697 - val_loss: 388.7062 - val_MAE: 18.7241\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 529.9940 - MAE: 21.1355 - val_loss: 387.4393 - val_MAE: 18.6903\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 528.5646 - MAE: 21.1017 - val_loss: 386.1838 - val_MAE: 18.6567\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 527.1490 - MAE: 21.0683 - val_loss: 384.9335 - val_MAE: 18.6231\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 525.7358 - MAE: 21.0347 - val_loss: 383.6728 - val_MAE: 18.5892\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 524.3131 - MAE: 21.0010 - val_loss: 382.4229 - val_MAE: 18.5556\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 522.9022 - MAE: 20.9675 - val_loss: 381.1809 - val_MAE: 18.5221\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 521.4990 - MAE: 20.9341 - val_loss: 379.9361 - val_MAE: 18.4885\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 520.0936 - MAE: 20.9002 - val_loss: 378.6989 - val_MAE: 18.4550\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 518.6954 - MAE: 20.8669 - val_loss: 377.4620 - val_MAE: 18.4214\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 517.2995 - MAE: 20.8328 - val_loss: 376.2479 - val_MAE: 18.3884\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 515.9255 - MAE: 20.8004 - val_loss: 375.0243 - val_MAE: 18.3551\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 514.5419 - MAE: 20.7670 - val_loss: 373.8028 - val_MAE: 18.3218\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 513.1615 - MAE: 20.7337 - val_loss: 372.5953 - val_MAE: 18.2889\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 511.7950 - MAE: 20.7010 - val_loss: 371.3785 - val_MAE: 18.2556\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 510.4193 - MAE: 20.6673 - val_loss: 370.1759 - val_MAE: 18.2226\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 509.0584 - MAE: 20.6344 - val_loss: 368.9746 - val_MAE: 18.1896\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 507.6997 - MAE: 20.6017 - val_loss: 367.7827 - val_MAE: 18.1568\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 506.3495 - MAE: 20.5684 - val_loss: 366.5915 - val_MAE: 18.1240\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 505.0010 - MAE: 20.5357 - val_loss: 365.4052 - val_MAE: 18.0912\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 503.6568 - MAE: 20.5032 - val_loss: 364.2125 - val_MAE: 18.0582\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 502.3078 - MAE: 20.4705 - val_loss: 363.0435 - val_MAE: 18.0258\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 500.9823 - MAE: 20.4380 - val_loss: 361.8691 - val_MAE: 17.9932\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 499.6512 - MAE: 20.4054 - val_loss: 360.6955 - val_MAE: 17.9606\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 498.3204 - MAE: 20.3727 - val_loss: 359.5175 - val_MAE: 17.9278\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 496.9862 - MAE: 20.3393 - val_loss: 358.3499 - val_MAE: 17.8952\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 495.6615 - MAE: 20.3073 - val_loss: 357.1916 - val_MAE: 17.8628\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 494.3492 - MAE: 20.2745 - val_loss: 356.0472 - val_MAE: 17.8307\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 493.0494 - MAE: 20.2431 - val_loss: 354.8889 - val_MAE: 17.7982\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 491.7362 - MAE: 20.2100 - val_loss: 353.7490 - val_MAE: 17.7662\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 490.4409 - MAE: 20.1784 - val_loss: 352.5948 - val_MAE: 17.7336\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 489.1317 - MAE: 20.1458 - val_loss: 351.4558 - val_MAE: 17.7015\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 487.8400 - MAE: 20.1142 - val_loss: 350.3295 - val_MAE: 17.6697\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 486.5588 - MAE: 20.0816 - val_loss: 349.1903 - val_MAE: 17.6374\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 485.2650 - MAE: 20.0493 - val_loss: 348.0596 - val_MAE: 17.6053\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 483.9803 - MAE: 20.0176 - val_loss: 346.9344 - val_MAE: 17.5733\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 482.7018 - MAE: 19.9854 - val_loss: 345.8046 - val_MAE: 17.5411\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 481.4197 - MAE: 19.9531 - val_loss: 344.7012 - val_MAE: 17.5097\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 480.1628 - MAE: 19.9222 - val_loss: 343.5850 - val_MAE: 17.4778\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 478.8937 - MAE: 19.8900 - val_loss: 342.4721 - val_MAE: 17.4459\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 477.6278 - MAE: 19.8583 - val_loss: 341.3642 - val_MAE: 17.4141\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 476.3683 - MAE: 19.8268 - val_loss: 340.2649 - val_MAE: 17.3825\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 475.1160 - MAE: 19.7952 - val_loss: 339.1584 - val_MAE: 17.3507\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 473.8575 - MAE: 19.7629 - val_loss: 338.0674 - val_MAE: 17.3192\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 472.6146 - MAE: 19.7319 - val_loss: 336.9652 - val_MAE: 17.2873\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 471.3612 - MAE: 19.7001 - val_loss: 335.8841 - val_MAE: 17.2560\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 470.1291 - MAE: 19.6682 - val_loss: 334.8012 - val_MAE: 17.2246\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 468.8952 - MAE: 19.6373 - val_loss: 333.7189 - val_MAE: 17.1932\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 467.6633 - MAE: 19.6057 - val_loss: 332.6477 - val_MAE: 17.1620\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 466.4420 - MAE: 19.5749 - val_loss: 331.5776 - val_MAE: 17.1308\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 465.2214 - MAE: 19.5437 - val_loss: 330.5064 - val_MAE: 17.0995\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 463.9998 - MAE: 19.5120 - val_loss: 329.4383 - val_MAE: 17.0683\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 462.7824 - MAE: 19.4811 - val_loss: 328.3817 - val_MAE: 17.0373\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 461.5757 - MAE: 19.4503 - val_loss: 327.3110 - val_MAE: 17.0058\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 460.3551 - MAE: 19.4180 - val_loss: 326.2575 - val_MAE: 16.9748\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 459.1519 - MAE: 19.3875 - val_loss: 325.1977 - val_MAE: 16.9436\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 457.9429 - MAE: 19.3569 - val_loss: 324.1450 - val_MAE: 16.9125\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=3, model__n_neurons=5, model__optimizer=sgd; total time=   5.7s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 554.2702 - MAE: 20.4548 - val_loss: 387.7246 - val_MAE: 17.7764\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 693.3531 - MAE: 22.5374 - val_loss: 445.9569 - val_MAE: 20.1952\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 631.9127 - MAE: 23.0053 - val_loss: 442.5045 - val_MAE: 20.1095\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 627.9811 - MAE: 22.9193 - val_loss: 439.0697 - val_MAE: 20.0239\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 624.0556 - MAE: 22.8340 - val_loss: 435.6454 - val_MAE: 19.9382\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 620.1365 - MAE: 22.7465 - val_loss: 432.2191 - val_MAE: 19.8521\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 616.2032 - MAE: 22.6607 - val_loss: 428.7587 - val_MAE: 19.7648\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 612.2311 - MAE: 22.5748 - val_loss: 425.2364 - val_MAE: 19.6755\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 608.1711 - MAE: 22.4833 - val_loss: 421.6365 - val_MAE: 19.5838\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 604.0169 - MAE: 22.3905 - val_loss: 417.9755 - val_MAE: 19.4901\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 599.7868 - MAE: 22.2959 - val_loss: 414.2747 - val_MAE: 19.3949\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=3, model__n_neurons=5, model__optimizer=sgd; total time=   1.2s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 5141.8911 - MAE: 43.5460 - val_loss: 548.8612 - val_MAE: 21.8421\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 559.0699 - MAE: 21.9696 - val_loss: 547.2996 - val_MAE: 21.8061\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 556.4087 - MAE: 21.8937 - val_loss: 545.7698 - val_MAE: 21.7710\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 554.1428 - MAE: 21.8308 - val_loss: 544.2525 - val_MAE: 21.7361\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 552.0217 - MAE: 21.7722 - val_loss: 542.7446 - val_MAE: 21.7014\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 549.8965 - MAE: 21.7121 - val_loss: 541.2430 - val_MAE: 21.6668\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 547.6413 - MAE: 21.6469 - val_loss: 539.7361 - val_MAE: 21.6320\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 545.1031 - MAE: 21.5664 - val_loss: 538.0922 - val_MAE: 21.5927\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 542.3471 - MAE: 21.4744 - val_loss: 536.1311 - val_MAE: 21.5431\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 538.7974 - MAE: 21.3413 - val_loss: 533.8397 - val_MAE: 21.4817\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 534.5217 - MAE: 21.1490 - val_loss: 528.7687 - val_MAE: 21.3273\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 516.8120 - MAE: 20.3684 - val_loss: 477.3831 - val_MAE: 19.3129\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 358.8597 - MAE: 16.0391 - val_loss: 211.0791 - val_MAE: 11.5672\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 216.9202 - MAE: 12.0516 - val_loss: 187.8270 - val_MAE: 11.0834\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 151.6475 - MAE: 9.7923 - val_loss: 149.6510 - val_MAE: 9.7194\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 133.9381 - MAE: 8.7888 - val_loss: 146.4321 - val_MAE: 9.4265\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 132.8817 - MAE: 8.8131 - val_loss: 121.8577 - val_MAE: 8.0224\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 108.1447 - MAE: 7.8028 - val_loss: 105.3854 - val_MAE: 7.4444\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 103.8788 - MAE: 7.4971 - val_loss: 94.4424 - val_MAE: 6.8785\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 84.7735 - MAE: 6.4233 - val_loss: 117.1484 - val_MAE: 8.1594\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 95.6301 - MAE: 7.3816 - val_loss: 77.4053 - val_MAE: 6.2063\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 76.7813 - MAE: 6.3504 - val_loss: 74.0504 - val_MAE: 5.9879\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 75.1961 - MAE: 6.4603 - val_loss: 70.2014 - val_MAE: 6.0386\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 81.2790 - MAE: 6.4792 - val_loss: 138.6304 - val_MAE: 10.0904\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 110.9833 - MAE: 8.0106 - val_loss: 65.9730 - val_MAE: 5.6714\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 76.2867 - MAE: 6.4667 - val_loss: 70.3737 - val_MAE: 6.3499\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 81.4563 - MAE: 6.6382 - val_loss: 70.7897 - val_MAE: 6.5171\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 74.3926 - MAE: 6.4763 - val_loss: 65.5379 - val_MAE: 5.7807\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 81.2957 - MAE: 6.6910 - val_loss: 75.3150 - val_MAE: 6.0997\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 90.0386 - MAE: 7.1195 - val_loss: 75.6234 - val_MAE: 6.9434\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 76.8440 - MAE: 6.6417 - val_loss: 66.2485 - val_MAE: 5.8259\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 72.3302 - MAE: 6.2362 - val_loss: 90.9954 - val_MAE: 7.9012\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 90.3742 - MAE: 7.2877 - val_loss: 112.6079 - val_MAE: 8.3040\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 94.0229 - MAE: 7.3310 - val_loss: 63.7629 - val_MAE: 5.8413\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 72.4696 - MAE: 6.3236 - val_loss: 62.6652 - val_MAE: 5.6070\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 106.0577 - MAE: 8.0250 - val_loss: 137.4505 - val_MAE: 9.6060\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 98.8287 - MAE: 7.2358 - val_loss: 85.8884 - val_MAE: 6.9275\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 105.2784 - MAE: 7.9840 - val_loss: 64.5653 - val_MAE: 5.6034\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 106.2548 - MAE: 7.7479 - val_loss: 61.3984 - val_MAE: 5.9659\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 73.2516 - MAE: 6.4682 - val_loss: 62.4943 - val_MAE: 5.7409\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 96.3196 - MAE: 7.4743 - val_loss: 94.7098 - val_MAE: 7.4124\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 82.4948 - MAE: 6.5463 - val_loss: 350.6797 - val_MAE: 17.1745\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 231.5876 - MAE: 12.0716 - val_loss: 97.8990 - val_MAE: 7.6438\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 87.3994 - MAE: 7.0550 - val_loss: 61.3488 - val_MAE: 5.4136\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 81.2635 - MAE: 6.6777 - val_loss: 93.6521 - val_MAE: 8.0502\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 86.0489 - MAE: 7.0759 - val_loss: 63.1302 - val_MAE: 5.9881\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 79.2717 - MAE: 6.6233 - val_loss: 63.1743 - val_MAE: 5.4792\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 79.1495 - MAE: 6.6890 - val_loss: 82.3102 - val_MAE: 6.7651\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 75.9891 - MAE: 6.3502 - val_loss: 61.8041 - val_MAE: 5.4162\n",
            "Epoch 49: early stopping\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=3, model__n_neurons=5, model__optimizer=sgd; total time=   5.6s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 16642.3691 - MAE: 84.3804 - val_loss: 35.7471 - val_MAE: 4.9718\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 88.4289 - MAE: 6.5131 - val_loss: 25.8747 - val_MAE: 4.2677\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 83.4299 - MAE: 6.6181 - val_loss: 24.4778 - val_MAE: 4.1408\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 82.6585 - MAE: 6.5440 - val_loss: 25.1866 - val_MAE: 4.2446\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 81.6923 - MAE: 6.5406 - val_loss: 23.0292 - val_MAE: 4.0582\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 80.8291 - MAE: 6.5451 - val_loss: 21.0696 - val_MAE: 3.7507\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 80.6008 - MAE: 6.3640 - val_loss: 20.5136 - val_MAE: 3.7771\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 80.3704 - MAE: 6.3366 - val_loss: 23.7151 - val_MAE: 4.1417\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 78.8884 - MAE: 6.3471 - val_loss: 24.7726 - val_MAE: 4.2432\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 78.6344 - MAE: 6.4475 - val_loss: 20.6406 - val_MAE: 3.8455\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 77.6186 - MAE: 6.3097 - val_loss: 20.2039 - val_MAE: 3.7983\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 78.3113 - MAE: 6.3438 - val_loss: 18.6023 - val_MAE: 3.5828\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 77.0239 - MAE: 6.1689 - val_loss: 24.3792 - val_MAE: 4.1525\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 78.0844 - MAE: 6.4881 - val_loss: 17.0782 - val_MAE: 3.3087\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 75.4732 - MAE: 6.1318 - val_loss: 16.6667 - val_MAE: 3.2384\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 75.3672 - MAE: 6.2268 - val_loss: 16.4512 - val_MAE: 3.2838\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 75.0218 - MAE: 5.9563 - val_loss: 17.9059 - val_MAE: 3.4281\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 74.8047 - MAE: 6.1006 - val_loss: 16.5994 - val_MAE: 3.2515\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 73.1927 - MAE: 5.9273 - val_loss: 19.6704 - val_MAE: 3.6246\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 74.0878 - MAE: 6.0519 - val_loss: 21.6039 - val_MAE: 3.8178\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 73.0816 - MAE: 6.1381 - val_loss: 18.2968 - val_MAE: 3.4803\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 73.5550 - MAE: 6.0253 - val_loss: 16.1118 - val_MAE: 3.2102\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 72.4276 - MAE: 5.8927 - val_loss: 20.6577 - val_MAE: 3.7474\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 73.1029 - MAE: 6.0067 - val_loss: 18.7188 - val_MAE: 3.5406\n",
            "Epoch 24: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=2, model__n_neurons=25, model__optimizer=momentum; total time=   3.0s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 10503.6455 - MAE: 65.0895 - val_loss: 84.9007 - val_MAE: 8.0836\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 135.1537 - MAE: 8.9554 - val_loss: 60.3159 - val_MAE: 6.7859\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 116.3570 - MAE: 8.0137 - val_loss: 53.0996 - val_MAE: 5.8654\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 99.5774 - MAE: 7.3153 - val_loss: 38.6264 - val_MAE: 4.9475\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 91.1380 - MAE: 7.0621 - val_loss: 26.7516 - val_MAE: 4.2160\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 88.0463 - MAE: 6.9451 - val_loss: 18.3705 - val_MAE: 3.5506\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 85.7673 - MAE: 6.7906 - val_loss: 20.5109 - val_MAE: 3.7266\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 87.3224 - MAE: 6.7207 - val_loss: 29.3934 - val_MAE: 4.5048\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 81.0122 - MAE: 6.5910 - val_loss: 33.2125 - val_MAE: 4.7945\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 80.9181 - MAE: 6.7117 - val_loss: 28.1923 - val_MAE: 4.4243\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 81.5203 - MAE: 6.6762 - val_loss: 17.2120 - val_MAE: 3.4513\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 80.2319 - MAE: 6.4739 - val_loss: 25.5989 - val_MAE: 4.1947\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 80.1981 - MAE: 6.5883 - val_loss: 24.0098 - val_MAE: 4.0717\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 81.0839 - MAE: 6.6526 - val_loss: 16.7561 - val_MAE: 3.3959\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 78.1730 - MAE: 6.3999 - val_loss: 18.6390 - val_MAE: 3.5982\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 78.6016 - MAE: 6.4659 - val_loss: 17.0160 - val_MAE: 3.4178\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 80.2459 - MAE: 6.4239 - val_loss: 28.2801 - val_MAE: 4.4207\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 81.3484 - MAE: 6.6162 - val_loss: 18.3687 - val_MAE: 3.5695\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 78.0010 - MAE: 6.3503 - val_loss: 22.4305 - val_MAE: 3.9226\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 78.8564 - MAE: 6.4316 - val_loss: 26.4933 - val_MAE: 4.3111\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 76.5123 - MAE: 6.5298 - val_loss: 15.9748 - val_MAE: 3.2878\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 78.7712 - MAE: 6.3681 - val_loss: 15.9452 - val_MAE: 3.2778\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 77.2137 - MAE: 6.2919 - val_loss: 22.3616 - val_MAE: 3.9372\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 78.6488 - MAE: 6.3797 - val_loss: 22.1586 - val_MAE: 3.9234\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 77.8286 - MAE: 6.4409 - val_loss: 16.8590 - val_MAE: 3.3915\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 79.6733 - MAE: 6.4239 - val_loss: 24.4712 - val_MAE: 4.1734\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 78.5404 - MAE: 6.4058 - val_loss: 13.2885 - val_MAE: 2.9271\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 80.8813 - MAE: 6.4223 - val_loss: 13.2930 - val_MAE: 2.8877\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 77.3874 - MAE: 6.2692 - val_loss: 14.8495 - val_MAE: 3.1419\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 77.2116 - MAE: 6.3290 - val_loss: 16.4992 - val_MAE: 3.3390\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 79.2330 - MAE: 6.4548 - val_loss: 14.3358 - val_MAE: 2.9819\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 80.5834 - MAE: 6.0777 - val_loss: 33.4315 - val_MAE: 4.9864\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 77.7493 - MAE: 6.4734 - val_loss: 21.5013 - val_MAE: 3.9148\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 75.7601 - MAE: 6.2648 - val_loss: 30.1962 - val_MAE: 4.6673\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 76.6509 - MAE: 6.4123 - val_loss: 20.7634 - val_MAE: 3.8339\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 76.8580 - MAE: 6.3408 - val_loss: 17.6740 - val_MAE: 3.4907\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 77.9240 - MAE: 6.3939 - val_loss: 13.6395 - val_MAE: 2.9494\n",
            "Epoch 37: early stopping\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=2, model__n_neurons=25, model__optimizer=momentum; total time=   3.0s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 26776.9844 - MAE: 94.9251 - val_loss: 548.4175 - val_MAE: 21.8317\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 556.8996 - MAE: 21.8871 - val_loss: 548.0932 - val_MAE: 21.8243\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 556.5771 - MAE: 21.8797 - val_loss: 547.7716 - val_MAE: 21.8169\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 556.2546 - MAE: 21.8724 - val_loss: 547.4510 - val_MAE: 21.8096\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 555.9335 - MAE: 21.8650 - val_loss: 547.1313 - val_MAE: 21.8023\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 555.6132 - MAE: 21.8577 - val_loss: 546.8118 - val_MAE: 21.7949\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 555.2928 - MAE: 21.8504 - val_loss: 546.4906 - val_MAE: 21.7876\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 554.9703 - MAE: 21.8429 - val_loss: 546.1702 - val_MAE: 21.7802\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 554.6495 - MAE: 21.8356 - val_loss: 545.8513 - val_MAE: 21.7729\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 554.3297 - MAE: 21.8283 - val_loss: 545.5314 - val_MAE: 21.7655\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 554.0088 - MAE: 21.8210 - val_loss: 545.2117 - val_MAE: 21.7582\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 553.6887 - MAE: 21.8136 - val_loss: 544.8940 - val_MAE: 21.7509\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 553.3704 - MAE: 21.8063 - val_loss: 544.5753 - val_MAE: 21.7436\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 553.0505 - MAE: 21.7990 - val_loss: 544.2559 - val_MAE: 21.7362\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 552.7304 - MAE: 21.7916 - val_loss: 543.9374 - val_MAE: 21.7289\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 552.4109 - MAE: 21.7843 - val_loss: 543.6170 - val_MAE: 21.7215\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 552.0894 - MAE: 21.7768 - val_loss: 543.2980 - val_MAE: 21.7142\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 551.7701 - MAE: 21.7695 - val_loss: 542.9798 - val_MAE: 21.7068\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 551.4511 - MAE: 21.7623 - val_loss: 542.6624 - val_MAE: 21.6995\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 551.1329 - MAE: 21.7549 - val_loss: 542.3440 - val_MAE: 21.6922\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 550.8135 - MAE: 21.7476 - val_loss: 542.0261 - val_MAE: 21.6849\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 550.4949 - MAE: 21.7403 - val_loss: 541.7079 - val_MAE: 21.6775\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 550.1756 - MAE: 21.7330 - val_loss: 541.3886 - val_MAE: 21.6702\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 549.8550 - MAE: 21.7256 - val_loss: 541.0679 - val_MAE: 21.6628\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 549.5334 - MAE: 21.7181 - val_loss: 540.7477 - val_MAE: 21.6554\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 549.2123 - MAE: 21.7108 - val_loss: 540.4280 - val_MAE: 21.6480\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 548.8919 - MAE: 21.7034 - val_loss: 540.1088 - val_MAE: 21.6406\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 548.5721 - MAE: 21.6960 - val_loss: 539.7886 - val_MAE: 21.6332\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 548.2509 - MAE: 21.6886 - val_loss: 539.4697 - val_MAE: 21.6258\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 547.9312 - MAE: 21.6812 - val_loss: 539.1498 - val_MAE: 21.6184\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 547.6102 - MAE: 21.6738 - val_loss: 538.8298 - val_MAE: 21.6110\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 547.2891 - MAE: 21.6665 - val_loss: 538.5093 - val_MAE: 21.6036\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 546.9680 - MAE: 21.6590 - val_loss: 538.1912 - val_MAE: 21.5963\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 546.6496 - MAE: 21.6516 - val_loss: 537.8726 - val_MAE: 21.5889\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 546.3295 - MAE: 21.6443 - val_loss: 537.5524 - val_MAE: 21.5815\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 546.0086 - MAE: 21.6368 - val_loss: 537.2350 - val_MAE: 21.5741\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 545.6913 - MAE: 21.6293 - val_loss: 536.9180 - val_MAE: 21.5668\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 545.3733 - MAE: 21.6221 - val_loss: 536.6007 - val_MAE: 21.5594\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 545.0549 - MAE: 21.6147 - val_loss: 536.2817 - val_MAE: 21.5520\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 544.7344 - MAE: 21.6074 - val_loss: 535.9621 - val_MAE: 21.5446\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 544.4139 - MAE: 21.6000 - val_loss: 535.6430 - val_MAE: 21.5372\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 544.0936 - MAE: 21.5926 - val_loss: 535.3196 - val_MAE: 21.5297\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 543.7684 - MAE: 21.5850 - val_loss: 534.9983 - val_MAE: 21.5222\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 543.4473 - MAE: 21.5776 - val_loss: 534.6788 - val_MAE: 21.5148\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 543.1266 - MAE: 21.5702 - val_loss: 534.3570 - val_MAE: 21.5073\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 542.8037 - MAE: 21.5627 - val_loss: 534.0366 - val_MAE: 21.4999\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 542.4830 - MAE: 21.5551 - val_loss: 533.7172 - val_MAE: 21.4924\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 542.1627 - MAE: 21.5478 - val_loss: 533.3970 - val_MAE: 21.4850\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 541.8412 - MAE: 21.5403 - val_loss: 533.0757 - val_MAE: 21.4775\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 541.5189 - MAE: 21.5328 - val_loss: 532.7543 - val_MAE: 21.4700\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 541.1967 - MAE: 21.5253 - val_loss: 532.4336 - val_MAE: 21.4625\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 540.8751 - MAE: 21.5178 - val_loss: 532.1119 - val_MAE: 21.4551\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 540.5519 - MAE: 21.5104 - val_loss: 531.7887 - val_MAE: 21.4475\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 540.2275 - MAE: 21.5028 - val_loss: 531.4651 - val_MAE: 21.4400\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 539.9031 - MAE: 21.4953 - val_loss: 531.1424 - val_MAE: 21.4324\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 539.5798 - MAE: 21.4878 - val_loss: 530.8202 - val_MAE: 21.4249\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 539.2567 - MAE: 21.4803 - val_loss: 530.4963 - val_MAE: 21.4174\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 538.9312 - MAE: 21.4727 - val_loss: 530.1718 - val_MAE: 21.4098\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 538.6057 - MAE: 21.4652 - val_loss: 529.8459 - val_MAE: 21.4022\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 538.2789 - MAE: 21.4575 - val_loss: 529.5225 - val_MAE: 21.3946\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 537.9548 - MAE: 21.4500 - val_loss: 529.1968 - val_MAE: 21.3870\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 537.6275 - MAE: 21.4422 - val_loss: 528.8714 - val_MAE: 21.3794\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 537.3019 - MAE: 21.4346 - val_loss: 528.5476 - val_MAE: 21.3718\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 536.9772 - MAE: 21.4272 - val_loss: 528.2228 - val_MAE: 21.3642\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 536.6511 - MAE: 21.4194 - val_loss: 527.8981 - val_MAE: 21.3566\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 536.3254 - MAE: 21.4119 - val_loss: 527.5706 - val_MAE: 21.3490\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 535.9962 - MAE: 21.4042 - val_loss: 527.2431 - val_MAE: 21.3413\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 535.6684 - MAE: 21.3965 - val_loss: 526.9177 - val_MAE: 21.3337\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 535.3420 - MAE: 21.3890 - val_loss: 526.5886 - val_MAE: 21.3259\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 535.0110 - MAE: 21.3812 - val_loss: 526.2615 - val_MAE: 21.3183\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 534.6837 - MAE: 21.3736 - val_loss: 525.9336 - val_MAE: 21.3106\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 534.3544 - MAE: 21.3659 - val_loss: 525.6055 - val_MAE: 21.3029\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 534.0256 - MAE: 21.3582 - val_loss: 525.2779 - val_MAE: 21.2952\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 533.6968 - MAE: 21.3505 - val_loss: 524.9490 - val_MAE: 21.2875\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 533.3667 - MAE: 21.3428 - val_loss: 524.6196 - val_MAE: 21.2797\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 533.0361 - MAE: 21.3350 - val_loss: 524.2891 - val_MAE: 21.2720\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 532.7045 - MAE: 21.3272 - val_loss: 523.9587 - val_MAE: 21.2642\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 532.3729 - MAE: 21.3194 - val_loss: 523.6262 - val_MAE: 21.2564\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 532.0396 - MAE: 21.3114 - val_loss: 523.2972 - val_MAE: 21.2486\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 531.7104 - MAE: 21.3038 - val_loss: 522.9677 - val_MAE: 21.2409\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 531.3787 - MAE: 21.2961 - val_loss: 522.6328 - val_MAE: 21.2330\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 531.0418 - MAE: 21.2881 - val_loss: 522.2977 - val_MAE: 21.2251\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 530.7060 - MAE: 21.2803 - val_loss: 521.9615 - val_MAE: 21.2172\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 530.3685 - MAE: 21.2724 - val_loss: 521.6266 - val_MAE: 21.2093\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 530.0333 - MAE: 21.2644 - val_loss: 521.2935 - val_MAE: 21.2014\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 529.6992 - MAE: 21.2565 - val_loss: 520.9595 - val_MAE: 21.1936\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 529.3642 - MAE: 21.2487 - val_loss: 520.6256 - val_MAE: 21.1857\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 529.0291 - MAE: 21.2408 - val_loss: 520.2906 - val_MAE: 21.1778\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 528.6928 - MAE: 21.2329 - val_loss: 519.9549 - val_MAE: 21.1698\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 528.3561 - MAE: 21.2250 - val_loss: 519.6193 - val_MAE: 21.1619\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 528.0192 - MAE: 21.2171 - val_loss: 519.2809 - val_MAE: 21.1539\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 527.6787 - MAE: 21.2091 - val_loss: 518.9393 - val_MAE: 21.1458\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 527.3358 - MAE: 21.2009 - val_loss: 518.5996 - val_MAE: 21.1378\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 526.9960 - MAE: 21.1928 - val_loss: 518.2618 - val_MAE: 21.1298\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 526.6572 - MAE: 21.1849 - val_loss: 517.9212 - val_MAE: 21.1218\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 526.3149 - MAE: 21.1768 - val_loss: 517.5823 - val_MAE: 21.1137\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 525.9755 - MAE: 21.1688 - val_loss: 517.2418 - val_MAE: 21.1057\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 525.6331 - MAE: 21.1606 - val_loss: 516.8996 - val_MAE: 21.0976\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 525.2897 - MAE: 21.1525 - val_loss: 516.5568 - val_MAE: 21.0894\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 524.9460 - MAE: 21.1443 - val_loss: 516.2151 - val_MAE: 21.0813\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=2, model__n_neurons=25, model__optimizer=momentum; total time=   6.1s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 0s 19ms/step - loss: inf - MAE: 195599107562932273152.0000 - val_loss: inf - val_MAE: 1703567912076558932639744.0000\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "5/5 [==============================] - 0s 5ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.9, model__n_hidden=0, model__n_neurons=125, model__optimizer=sgd; total time=   1.0s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 19ms/step - loss: inf - MAE: 841663449504203931648.0000 - val_loss: inf - val_MAE: 6935293242184064154206208.0000\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.9, model__n_hidden=0, model__n_neurons=125, model__optimizer=sgd; total time=   1.0s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 19ms/step - loss: inf - MAE: 1048891942020541579264.0000 - val_loss: inf - val_MAE: 7870942654022484693614592.0000\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.9, model__n_hidden=0, model__n_neurons=125, model__optimizer=sgd; total time=   1.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 23ms/step - loss: 216.7984 - MAE: 11.7439 - val_loss: 123.3488 - val_MAE: 9.1971\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 127.3043 - MAE: 8.5462 - val_loss: 67.2777 - val_MAE: 5.7628\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 100.1381 - MAE: 7.8023 - val_loss: 56.0557 - val_MAE: 5.4501\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 91.5590 - MAE: 7.5473 - val_loss: 46.5418 - val_MAE: 5.0269\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 83.8493 - MAE: 7.1298 - val_loss: 41.7745 - val_MAE: 4.8606\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 76.9976 - MAE: 6.7240 - val_loss: 36.6784 - val_MAE: 4.4231\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 74.0707 - MAE: 6.2434 - val_loss: 34.9568 - val_MAE: 4.4097\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 71.0083 - MAE: 6.2143 - val_loss: 34.6271 - val_MAE: 4.6188\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 69.6640 - MAE: 6.2689 - val_loss: 30.5223 - val_MAE: 4.3027\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 67.0720 - MAE: 6.0734 - val_loss: 28.8549 - val_MAE: 4.2251\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 65.8914 - MAE: 5.9067 - val_loss: 29.2833 - val_MAE: 4.3426\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 64.4477 - MAE: 5.9020 - val_loss: 27.1892 - val_MAE: 4.1478\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 63.6853 - MAE: 5.6714 - val_loss: 25.9006 - val_MAE: 4.0547\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 64.0957 - MAE: 6.0365 - val_loss: 25.7740 - val_MAE: 4.0855\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 62.7218 - MAE: 5.5553 - val_loss: 23.5730 - val_MAE: 3.8826\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 61.2953 - MAE: 5.7315 - val_loss: 25.5223 - val_MAE: 4.1632\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 60.3916 - MAE: 5.5039 - val_loss: 21.2774 - val_MAE: 3.6159\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 59.4927 - MAE: 5.4838 - val_loss: 25.2522 - val_MAE: 4.2066\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 61.2869 - MAE: 5.4848 - val_loss: 22.1894 - val_MAE: 3.8296\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 59.2353 - MAE: 5.6074 - val_loss: 22.3821 - val_MAE: 3.8777\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 59.3810 - MAE: 5.6436 - val_loss: 19.6986 - val_MAE: 3.5157\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 57.9669 - MAE: 5.2319 - val_loss: 20.1869 - val_MAE: 3.6287\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 57.1971 - MAE: 5.3763 - val_loss: 22.2909 - val_MAE: 3.9487\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 57.1379 - MAE: 5.4714 - val_loss: 19.3404 - val_MAE: 3.5485\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 57.1118 - MAE: 5.4097 - val_loss: 18.8565 - val_MAE: 3.4737\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 56.1354 - MAE: 5.1555 - val_loss: 20.2172 - val_MAE: 3.7172\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 55.6488 - MAE: 5.2926 - val_loss: 20.9050 - val_MAE: 3.8396\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 55.3036 - MAE: 5.2872 - val_loss: 18.3489 - val_MAE: 3.4361\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 54.8100 - MAE: 5.1733 - val_loss: 20.6138 - val_MAE: 3.8150\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 54.8494 - MAE: 5.3573 - val_loss: 17.9314 - val_MAE: 3.3709\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 55.0656 - MAE: 5.0416 - val_loss: 18.9831 - val_MAE: 3.5726\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 54.1140 - MAE: 5.1724 - val_loss: 17.4139 - val_MAE: 3.3043\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 54.2144 - MAE: 5.2560 - val_loss: 18.2499 - val_MAE: 3.4759\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 53.3320 - MAE: 5.0320 - val_loss: 17.3911 - val_MAE: 3.3517\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 53.5540 - MAE: 5.1563 - val_loss: 18.5734 - val_MAE: 3.5628\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 52.8593 - MAE: 5.2001 - val_loss: 17.5869 - val_MAE: 3.3852\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 52.3455 - MAE: 5.0125 - val_loss: 18.0248 - val_MAE: 3.4975\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 52.5110 - MAE: 5.0296 - val_loss: 17.9103 - val_MAE: 3.5043\n",
            "Epoch 38: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=2, model__n_neurons=125, model__optimizer=adam; total time=   3.0s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 23ms/step - loss: 357.0345 - MAE: 15.1510 - val_loss: 199.4108 - val_MAE: 11.8742\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 199.7547 - MAE: 11.0419 - val_loss: 95.1412 - val_MAE: 7.8193\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 136.6946 - MAE: 9.0544 - val_loss: 55.0092 - val_MAE: 5.8675\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 128.8162 - MAE: 8.7728 - val_loss: 51.9499 - val_MAE: 5.9430\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 122.5229 - MAE: 8.5560 - val_loss: 49.3828 - val_MAE: 5.8753\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 116.0987 - MAE: 8.3002 - val_loss: 45.2263 - val_MAE: 5.4336\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 109.8915 - MAE: 7.6208 - val_loss: 44.4449 - val_MAE: 5.0945\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 105.1340 - MAE: 7.5077 - val_loss: 52.9273 - val_MAE: 5.9904\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 103.6475 - MAE: 7.6764 - val_loss: 43.7312 - val_MAE: 5.2394\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 98.8651 - MAE: 7.2646 - val_loss: 39.8504 - val_MAE: 5.0508\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 96.3844 - MAE: 7.2412 - val_loss: 40.7976 - val_MAE: 5.2513\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 94.9550 - MAE: 7.0395 - val_loss: 34.1740 - val_MAE: 4.6419\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 90.9425 - MAE: 7.0209 - val_loss: 39.7111 - val_MAE: 5.1928\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 89.9756 - MAE: 7.0932 - val_loss: 31.0381 - val_MAE: 4.4617\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 88.5076 - MAE: 6.6417 - val_loss: 33.5696 - val_MAE: 4.7281\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 87.5424 - MAE: 7.0810 - val_loss: 30.7195 - val_MAE: 4.5260\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 86.2518 - MAE: 6.4791 - val_loss: 27.6591 - val_MAE: 4.2302\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 83.0165 - MAE: 6.7599 - val_loss: 32.6475 - val_MAE: 4.7665\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 85.5113 - MAE: 6.5073 - val_loss: 26.1140 - val_MAE: 4.2148\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 80.9314 - MAE: 6.6529 - val_loss: 29.8142 - val_MAE: 4.5498\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 80.7350 - MAE: 6.7182 - val_loss: 22.4387 - val_MAE: 3.7750\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 80.4894 - MAE: 6.1358 - val_loss: 24.4512 - val_MAE: 4.0501\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 77.9965 - MAE: 6.5031 - val_loss: 27.0931 - val_MAE: 4.3343\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 77.8087 - MAE: 6.3973 - val_loss: 21.8241 - val_MAE: 3.8580\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 78.1660 - MAE: 6.4421 - val_loss: 21.4524 - val_MAE: 3.7948\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 76.5740 - MAE: 6.1408 - val_loss: 23.8548 - val_MAE: 4.0164\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 76.1349 - MAE: 6.4068 - val_loss: 23.8475 - val_MAE: 3.9869\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 76.8621 - MAE: 6.0969 - val_loss: 21.9731 - val_MAE: 3.8655\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 74.3655 - MAE: 6.2771 - val_loss: 23.2701 - val_MAE: 4.0275\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 74.6099 - MAE: 6.0862 - val_loss: 21.2893 - val_MAE: 3.7688\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 73.8966 - MAE: 6.2126 - val_loss: 21.4385 - val_MAE: 3.8391\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 74.9293 - MAE: 6.0975 - val_loss: 18.7584 - val_MAE: 3.4770\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 74.1988 - MAE: 6.4330 - val_loss: 23.4585 - val_MAE: 4.0642\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 74.7265 - MAE: 6.0605 - val_loss: 17.1594 - val_MAE: 3.3656\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 72.5792 - MAE: 6.2397 - val_loss: 23.5657 - val_MAE: 4.1224\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 71.3406 - MAE: 6.1828 - val_loss: 16.6544 - val_MAE: 3.2750\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 71.7136 - MAE: 5.9116 - val_loss: 22.2151 - val_MAE: 3.9361\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 70.6767 - MAE: 6.0953 - val_loss: 17.6504 - val_MAE: 3.4269\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 70.9494 - MAE: 6.0261 - val_loss: 17.9397 - val_MAE: 3.5068\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 71.6949 - MAE: 5.7687 - val_loss: 18.3200 - val_MAE: 3.5703\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 72.2796 - MAE: 6.5649 - val_loss: 20.0598 - val_MAE: 3.7712\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 70.0921 - MAE: 5.6857 - val_loss: 15.9333 - val_MAE: 3.2220\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 68.6024 - MAE: 5.9521 - val_loss: 21.7949 - val_MAE: 3.9501\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 68.2079 - MAE: 5.9469 - val_loss: 15.9510 - val_MAE: 3.2801\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 71.6654 - MAE: 6.1884 - val_loss: 17.0349 - val_MAE: 3.4617\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 71.1262 - MAE: 5.6234 - val_loss: 16.7313 - val_MAE: 3.4201\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 68.2866 - MAE: 6.0439 - val_loss: 17.3053 - val_MAE: 3.4809\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 67.2741 - MAE: 5.8336 - val_loss: 14.7583 - val_MAE: 3.1592\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 66.9575 - MAE: 5.5976 - val_loss: 17.8186 - val_MAE: 3.5904\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 67.7571 - MAE: 6.3245 - val_loss: 19.5338 - val_MAE: 3.7385\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 66.6416 - MAE: 5.5755 - val_loss: 13.5921 - val_MAE: 3.0302\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 66.9313 - MAE: 5.9704 - val_loss: 19.5486 - val_MAE: 3.7702\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 65.6488 - MAE: 5.6774 - val_loss: 13.6043 - val_MAE: 3.0335\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 67.7862 - MAE: 5.9882 - val_loss: 19.3640 - val_MAE: 3.7413\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 65.5193 - MAE: 5.5317 - val_loss: 13.4744 - val_MAE: 3.0152\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 65.5219 - MAE: 5.9812 - val_loss: 18.0835 - val_MAE: 3.6233\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 66.7451 - MAE: 5.6115 - val_loss: 14.9068 - val_MAE: 3.2420\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 64.8977 - MAE: 5.9651 - val_loss: 15.2429 - val_MAE: 3.2996\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 64.5190 - MAE: 5.4920 - val_loss: 16.8012 - val_MAE: 3.4738\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 63.3414 - MAE: 5.8531 - val_loss: 15.3306 - val_MAE: 3.3185\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 62.8427 - MAE: 5.6216 - val_loss: 12.9618 - val_MAE: 2.9641\n",
            "Epoch 61: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=2, model__n_neurons=125, model__optimizer=adam; total time=   3.9s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 21ms/step - loss: 2436.1829 - MAE: 41.8196 - val_loss: 1287.0916 - val_MAE: 28.2712\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 527.8181 - MAE: 16.9564 - val_loss: 285.6989 - val_MAE: 12.8645\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 242.5045 - MAE: 12.7794 - val_loss: 260.9995 - val_MAE: 13.7300\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 276.2150 - MAE: 14.3909 - val_loss: 192.4987 - val_MAE: 11.5924\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 167.8007 - MAE: 10.6093 - val_loss: 121.9639 - val_MAE: 8.2574\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 112.3433 - MAE: 7.7639 - val_loss: 126.0523 - val_MAE: 8.1395\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 102.1131 - MAE: 7.2368 - val_loss: 99.9487 - val_MAE: 7.1215\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 82.8042 - MAE: 6.5758 - val_loss: 71.6163 - val_MAE: 6.0403\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 74.8234 - MAE: 6.0778 - val_loss: 58.3166 - val_MAE: 5.5632\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 68.9488 - MAE: 5.7516 - val_loss: 54.2274 - val_MAE: 5.2782\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 63.9937 - MAE: 5.6309 - val_loss: 53.2259 - val_MAE: 5.2779\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 61.4650 - MAE: 5.5404 - val_loss: 49.8972 - val_MAE: 5.1369\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 59.7321 - MAE: 5.3402 - val_loss: 47.6725 - val_MAE: 5.0984\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 58.2037 - MAE: 5.2007 - val_loss: 46.8330 - val_MAE: 5.0716\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 57.4196 - MAE: 5.2404 - val_loss: 46.7747 - val_MAE: 5.1174\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 56.5689 - MAE: 5.2326 - val_loss: 45.3951 - val_MAE: 5.0346\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 56.1561 - MAE: 5.1706 - val_loss: 44.9164 - val_MAE: 5.0108\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 55.5829 - MAE: 5.1048 - val_loss: 43.7232 - val_MAE: 4.9578\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 55.7804 - MAE: 5.0466 - val_loss: 43.0663 - val_MAE: 4.9346\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 54.8462 - MAE: 5.0711 - val_loss: 43.1299 - val_MAE: 4.9345\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 54.4415 - MAE: 5.0769 - val_loss: 42.5731 - val_MAE: 4.8998\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 54.2830 - MAE: 5.0209 - val_loss: 41.6854 - val_MAE: 4.8483\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 54.1087 - MAE: 4.9796 - val_loss: 41.3554 - val_MAE: 4.8426\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 53.2744 - MAE: 5.0581 - val_loss: 41.6753 - val_MAE: 4.8738\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 53.2371 - MAE: 5.1002 - val_loss: 41.3127 - val_MAE: 4.8447\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 53.3181 - MAE: 4.9838 - val_loss: 40.4798 - val_MAE: 4.7666\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 52.8479 - MAE: 5.0250 - val_loss: 41.2787 - val_MAE: 4.8821\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 52.4455 - MAE: 5.0490 - val_loss: 40.3792 - val_MAE: 4.7756\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 52.0962 - MAE: 4.9050 - val_loss: 40.0078 - val_MAE: 4.7338\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 52.0762 - MAE: 4.9689 - val_loss: 39.7321 - val_MAE: 4.7670\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 51.6815 - MAE: 4.9763 - val_loss: 39.6212 - val_MAE: 4.7416\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 51.5890 - MAE: 4.8605 - val_loss: 39.2058 - val_MAE: 4.6817\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 51.3243 - MAE: 4.9253 - val_loss: 39.6004 - val_MAE: 4.7966\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 51.0812 - MAE: 4.9730 - val_loss: 38.6200 - val_MAE: 4.6520\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 51.1033 - MAE: 4.8204 - val_loss: 38.3670 - val_MAE: 4.6353\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 50.5578 - MAE: 4.9009 - val_loss: 38.6405 - val_MAE: 4.7307\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 50.8338 - MAE: 5.0255 - val_loss: 38.1022 - val_MAE: 4.6721\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 50.1053 - MAE: 4.8013 - val_loss: 38.1168 - val_MAE: 4.5944\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 50.6148 - MAE: 4.7693 - val_loss: 37.9868 - val_MAE: 4.6261\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 49.8730 - MAE: 4.8606 - val_loss: 37.4677 - val_MAE: 4.6354\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 50.0467 - MAE: 4.8164 - val_loss: 37.2151 - val_MAE: 4.6056\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 49.6871 - MAE: 4.8054 - val_loss: 37.1881 - val_MAE: 4.5903\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 49.5818 - MAE: 4.9238 - val_loss: 38.6200 - val_MAE: 4.7655\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 49.2545 - MAE: 4.8824 - val_loss: 36.9626 - val_MAE: 4.5435\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 49.1361 - MAE: 4.6985 - val_loss: 36.7921 - val_MAE: 4.5550\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 48.5425 - MAE: 4.7806 - val_loss: 36.7017 - val_MAE: 4.6239\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 49.1776 - MAE: 4.9966 - val_loss: 37.1273 - val_MAE: 4.6353\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 49.0942 - MAE: 4.6987 - val_loss: 36.4607 - val_MAE: 4.4917\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 49.2419 - MAE: 4.7799 - val_loss: 36.4157 - val_MAE: 4.5905\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 47.9576 - MAE: 4.7517 - val_loss: 35.8606 - val_MAE: 4.4660\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 47.9248 - MAE: 4.7132 - val_loss: 36.2068 - val_MAE: 4.5403\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 47.6250 - MAE: 4.6919 - val_loss: 35.5481 - val_MAE: 4.4759\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 47.2870 - MAE: 4.6529 - val_loss: 35.3291 - val_MAE: 4.4985\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 47.5102 - MAE: 4.8664 - val_loss: 34.9895 - val_MAE: 4.5115\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 47.4697 - MAE: 4.6901 - val_loss: 34.5078 - val_MAE: 4.4316\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 47.0848 - MAE: 4.7752 - val_loss: 35.4712 - val_MAE: 4.5730\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 46.8123 - MAE: 4.6882 - val_loss: 34.5137 - val_MAE: 4.4095\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 46.4829 - MAE: 4.6246 - val_loss: 34.1167 - val_MAE: 4.4268\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 46.2203 - MAE: 4.6191 - val_loss: 33.9387 - val_MAE: 4.4038\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 45.8988 - MAE: 4.7175 - val_loss: 33.8177 - val_MAE: 4.4418\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 45.7870 - MAE: 4.5961 - val_loss: 33.7119 - val_MAE: 4.3715\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 46.1690 - MAE: 4.7095 - val_loss: 33.9586 - val_MAE: 4.4245\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 45.2092 - MAE: 4.5816 - val_loss: 33.2835 - val_MAE: 4.3394\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 45.4057 - MAE: 4.4950 - val_loss: 33.1748 - val_MAE: 4.3307\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 45.4143 - MAE: 4.6865 - val_loss: 33.2425 - val_MAE: 4.4121\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 44.8069 - MAE: 4.5855 - val_loss: 32.8310 - val_MAE: 4.3200\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 44.6183 - MAE: 4.4966 - val_loss: 32.9407 - val_MAE: 4.3513\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 44.9276 - MAE: 4.7305 - val_loss: 32.9760 - val_MAE: 4.3448\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 45.8067 - MAE: 4.4823 - val_loss: 32.5051 - val_MAE: 4.3139\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 44.0052 - MAE: 4.5955 - val_loss: 32.5055 - val_MAE: 4.3659\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 43.9672 - MAE: 4.5669 - val_loss: 31.7851 - val_MAE: 4.2720\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 43.6883 - MAE: 4.5263 - val_loss: 32.2148 - val_MAE: 4.2995\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 43.9857 - MAE: 4.4578 - val_loss: 31.7972 - val_MAE: 4.2827\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 43.1384 - MAE: 4.4679 - val_loss: 31.7322 - val_MAE: 4.2921\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 43.1250 - MAE: 4.5396 - val_loss: 31.3894 - val_MAE: 4.2474\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 43.0711 - MAE: 4.4206 - val_loss: 31.4814 - val_MAE: 4.2596\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 42.9625 - MAE: 4.5187 - val_loss: 31.3618 - val_MAE: 4.2597\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 42.5992 - MAE: 4.4679 - val_loss: 30.6216 - val_MAE: 4.2106\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 43.4183 - MAE: 4.6116 - val_loss: 30.6451 - val_MAE: 4.2263\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 42.3744 - MAE: 4.3658 - val_loss: 30.8154 - val_MAE: 4.2792\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 42.3555 - MAE: 4.4297 - val_loss: 30.4742 - val_MAE: 4.2193\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 42.0700 - MAE: 4.5213 - val_loss: 30.1595 - val_MAE: 4.1890\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 41.9616 - MAE: 4.2740 - val_loss: 29.9597 - val_MAE: 4.1943\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 41.7504 - MAE: 4.4969 - val_loss: 30.1874 - val_MAE: 4.1953\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 41.1849 - MAE: 4.4817 - val_loss: 30.1124 - val_MAE: 4.2483\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 42.3737 - MAE: 4.2359 - val_loss: 29.9303 - val_MAE: 4.2023\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 41.2259 - MAE: 4.3563 - val_loss: 29.5787 - val_MAE: 4.1628\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 40.7203 - MAE: 4.3661 - val_loss: 29.3618 - val_MAE: 4.1592\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 40.8075 - MAE: 4.2383 - val_loss: 29.1283 - val_MAE: 4.1416\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 40.6249 - MAE: 4.3735 - val_loss: 29.0539 - val_MAE: 4.1302\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 40.5160 - MAE: 4.3343 - val_loss: 29.1288 - val_MAE: 4.1186\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 40.3679 - MAE: 4.3493 - val_loss: 28.9745 - val_MAE: 4.1139\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 40.3494 - MAE: 4.3784 - val_loss: 28.6341 - val_MAE: 4.1002\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 39.9127 - MAE: 4.3080 - val_loss: 28.5903 - val_MAE: 4.1418\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 40.2988 - MAE: 4.2775 - val_loss: 28.4855 - val_MAE: 4.1326\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 40.1338 - MAE: 4.2897 - val_loss: 28.1025 - val_MAE: 4.0713\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 39.7722 - MAE: 4.3071 - val_loss: 28.3393 - val_MAE: 4.1504\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 39.6482 - MAE: 4.1826 - val_loss: 27.9332 - val_MAE: 4.0794\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 39.3297 - MAE: 4.2342 - val_loss: 27.6831 - val_MAE: 4.0687\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 39.5764 - MAE: 4.2988 - val_loss: 27.6788 - val_MAE: 4.0547\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=2, model__n_neurons=125, model__optimizer=adam; total time=   5.8s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 0s 20ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.9, model__n_hidden=2, model__n_neurons=25, model__optimizer=momentum; total time=   1.8s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 22ms/step - loss: 243109133367115776.0000 - MAE: 183554400.0000 - val_loss: 157092572823552.0000 - val_MAE: 12533657.0000\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 263501813645312.0000 - MAE: 16097116.0000 - val_loss: 381531877015552.0000 - val_MAE: 19532842.0000\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 414984437760000.0000 - MAE: 20366486.0000 - val_loss: 432024619843584.0000 - val_MAE: 20785202.0000\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 414996886454272.0000 - MAE: 20368614.0000 - val_loss: 378336119357440.0000 - val_MAE: 19450864.0000\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 343892562018304.0000 - MAE: 18533638.0000 - val_loss: 293207652958208.0000 - val_MAE: 17123308.0000\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 258837965701120.0000 - MAE: 16073248.0000 - val_loss: 212517850710016.0000 - val_MAE: 14577991.0000\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 184463443099648.0000 - MAE: 13565395.0000 - val_loss: 148068880089088.0000 - val_MAE: 12168356.0000\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 127208827912192.0000 - MAE: 11263154.0000 - val_loss: 100687455191040.0000 - val_MAE: 10034313.0000\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 85947303591936.0000 - MAE: 9256931.0000 - val_loss: 67424695091200.0000 - val_MAE: 8211254.5000\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 57317773017088.0000 - MAE: 7558960.5000 - val_loss: 44707681927168.0000 - val_MAE: 6686380.0000\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 37905070292992.0000 - MAE: 6146727.0000 - val_loss: 29455667953664.0000 - val_MAE: 5427308.0000\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 24930508865536.0000 - MAE: 4984783.0000 - val_loss: 19326029529088.0000 - val_MAE: 4396138.0000\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16338483412992.0000 - MAE: 4035309.7500 - val_loss: 12645248794624.0000 - val_MAE: 3556015.7500\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 10682484391936.0000 - MAE: 3262877.0000 - val_loss: 8259048046592.0000 - val_MAE: 2873855.7500\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6973671079936.0000 - MAE: 2636277.2500 - val_loss: 5387871846400.0000 - val_MAE: 2321179.0000\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 4547865477120.0000 - MAE: 2128930.0000 - val_loss: 3512080531456.0000 - val_MAE: 1874054.5000\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 2963887357952.0000 - MAE: 1718646.3750 - val_loss: 2288163749888.0000 - val_MAE: 1512667.8750\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1930734993408.0000 - MAE: 1387125.6250 - val_loss: 1490257969152.0000 - val_MAE: 1220761.1250\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1257349840896.0000 - MAE: 1119390.6250 - val_loss: 970371170304.0000 - val_MAE: 985074.2500\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 818663260160.0000 - MAE: 903245.4375 - val_loss: 631756554240.0000 - val_MAE: 794831.1875\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 532965064704.0000 - MAE: 728789.5625 - val_loss: 411262222336.0000 - val_MAE: 641297.2500\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 346940768256.0000 - MAE: 588003.8750 - val_loss: 267706925056.0000 - val_MAE: 517404.0312\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 225833041920.0000 - MAE: 474401.5625 - val_loss: 174253686784.0000 - val_MAE: 417437.0312\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 146995331072.0000 - MAE: 382740.0938 - val_loss: 113420640256.0000 - val_MAE: 336779.8125\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 95677276160.0000 - MAE: 308785.3125 - val_loss: 73823469568.0000 - val_MAE: 271704.7500\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 62274060288.0000 - MAE: 249118.3906 - val_loss: 48049815552.0000 - val_MAE: 219202.6875\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 40532262912.0000 - MAE: 200979.9062 - val_loss: 31274194944.0000 - val_MAE: 176845.1094\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 26381021184.0000 - MAE: 162142.8594 - val_loss: 20355348480.0000 - val_MAE: 142672.1719\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 17170380800.0000 - MAE: 130810.3203 - val_loss: 13248605184.0000 - val_MAE: 115102.5859\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 11175500800.0000 - MAE: 105532.3359 - val_loss: 8623065088.0000 - val_MAE: 92860.4531\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7273660416.0000 - MAE: 85139.0156 - val_loss: 5612468224.0000 - val_MAE: 74916.4141\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4734115840.0000 - MAE: 68686.5078 - val_loss: 3652976640.0000 - val_MAE: 60439.8594\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3081226752.0000 - MAE: 55413.3008 - val_loss: 2377618432.0000 - val_MAE: 48760.8281\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2005438720.0000 - MAE: 44705.0547 - val_loss: 1547530752.0000 - val_MAE: 39338.6602\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1305254016.0000 - MAE: 36066.0820 - val_loss: 1007253760.0000 - val_MAE: 31737.2637\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 849528704.0000 - MAE: 29096.5645 - val_loss: 655607296.0000 - val_MAE: 25604.8281\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 552923264.0000 - MAE: 23473.8574 - val_loss: 426728832.0000 - val_MAE: 20657.4141\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 359873280.0000 - MAE: 18937.6816 - val_loss: 277756960.0000 - val_MAE: 16666.0410\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 234224592.0000 - MAE: 15278.0918 - val_loss: 180794336.0000 - val_MAE: 13445.9766\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 152447504.0000 - MAE: 12325.6777 - val_loss: 117682376.0000 - val_MAE: 10848.1494\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 99219920.0000 - MAE: 9943.8135 - val_loss: 76604192.0000 - val_MAE: 8752.3799\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 64577988.0000 - MAE: 8022.2495 - val_loss: 49866160.0000 - val_MAE: 7061.5947\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 42031452.0000 - MAE: 6471.9917 - val_loss: 32461752.0000 - val_MAE: 5697.5181\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 27356396.0000 - MAE: 5221.3037 - val_loss: 21132882.0000 - val_MAE: 4597.0474\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 17804214.0000 - MAE: 4212.3438 - val_loss: 13758893.0000 - val_MAE: 3709.2932\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 11588651.0000 - MAE: 3398.3457 - val_loss: 8958251.0000 - val_MAE: 2993.0276\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7542362.5000 - MAE: 2741.6240 - val_loss: 5833115.5000 - val_MAE: 2415.1763\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4909022.0000 - MAE: 2211.7954 - val_loss: 3798531.7500 - val_MAE: 1948.9725\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3195130.5000 - MAE: 1784.3478 - val_loss: 2473963.0000 - val_MAE: 1572.8716\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2079560.2500 - MAE: 1439.5171 - val_loss: 1611629.8750 - val_MAE: 1269.4849\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1353492.6250 - MAE: 1161.3534 - val_loss: 1050083.7500 - val_MAE: 1024.7173\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 880947.4375 - MAE: 936.9350 - val_loss: 684389.6250 - val_MAE: 827.2554\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 573437.6875 - MAE: 755.8813 - val_loss: 446181.7812 - val_MAE: 667.9398\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 373228.3125 - MAE: 609.8102 - val_loss: 291019.6562 - val_MAE: 539.4271\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 242973.8906 - MAE: 491.9693 - val_loss: 189887.9062 - val_MAE: 435.7175\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 158172.8438 - MAE: 396.8889 - val_loss: 123989.5938 - val_MAE: 352.0674\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 102994.0938 - MAE: 320.1898 - val_loss: 81026.0625 - val_MAE: 284.5838\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 67062.6562 - MAE: 258.3229 - val_loss: 53004.2305 - val_MAE: 230.1437\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 43706.5000 - MAE: 208.3886 - val_loss: 34707.7656 - val_MAE: 186.1979\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 28462.4062 - MAE: 168.1292 - val_loss: 22778.2031 - val_MAE: 150.7982\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 18564.3770 - MAE: 135.6750 - val_loss: 14980.2305 - val_MAE: 122.2380\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12122.8477 - MAE: 109.4719 - val_loss: 9873.4150 - val_MAE: 99.1731\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7925.7954 - MAE: 88.3195 - val_loss: 6526.9028 - val_MAE: 80.5530\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5192.9365 - MAE: 71.2458 - val_loss: 4332.2349 - val_MAE: 65.5295\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3420.2632 - MAE: 57.4505 - val_loss: 2888.3833 - val_MAE: 53.3879\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2259.6978 - MAE: 46.3274 - val_loss: 1938.5649 - val_MAE: 43.5942\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1500.9950 - MAE: 37.3744 - val_loss: 1315.4268 - val_MAE: 35.7395\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1015.1136 - MAE: 30.1531 - val_loss: 900.1519 - val_MAE: 29.3605\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 693.4856 - MAE: 24.5653 - val_loss: 626.7890 - val_MAE: 24.2626\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 489.9866 - MAE: 20.5102 - val_loss: 442.3768 - val_MAE: 20.1063\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 352.0514 - MAE: 17.2430 - val_loss: 320.1705 - val_MAE: 16.7946\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 267.1190 - MAE: 14.9082 - val_loss: 235.6831 - val_MAE: 14.0560\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 208.5530 - MAE: 13.1221 - val_loss: 179.3567 - val_MAE: 11.8846\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 171.7894 - MAE: 11.7859 - val_loss: 140.8302 - val_MAE: 10.1350\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 147.4628 - MAE: 10.8299 - val_loss: 114.5950 - val_MAE: 8.9032\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 132.9419 - MAE: 10.1344 - val_loss: 95.5889 - val_MAE: 8.0601\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 121.7618 - MAE: 9.5385 - val_loss: 82.6218 - val_MAE: 7.4199\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 114.6775 - MAE: 9.1277 - val_loss: 74.0210 - val_MAE: 6.9590\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 110.8442 - MAE: 8.8151 - val_loss: 67.0317 - val_MAE: 6.5719\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 107.8746 - MAE: 8.5449 - val_loss: 62.0678 - val_MAE: 6.2697\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 106.1377 - MAE: 8.3338 - val_loss: 58.2979 - val_MAE: 6.0636\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 105.0198 - MAE: 8.1583 - val_loss: 55.2254 - val_MAE: 5.9133\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 104.1381 - MAE: 8.0309 - val_loss: 53.0609 - val_MAE: 5.8410\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 103.5271 - MAE: 7.9354 - val_loss: 51.7442 - val_MAE: 5.7959\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 103.2926 - MAE: 7.8725 - val_loss: 50.4707 - val_MAE: 5.7501\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 103.0530 - MAE: 7.8136 - val_loss: 49.5662 - val_MAE: 5.7161\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 102.9393 - MAE: 7.7718 - val_loss: 48.7621 - val_MAE: 5.6847\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 102.8440 - MAE: 7.7368 - val_loss: 48.2469 - val_MAE: 5.6640\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 102.7759 - MAE: 7.7196 - val_loss: 48.0319 - val_MAE: 5.6552\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 102.7483 - MAE: 7.7090 - val_loss: 47.8581 - val_MAE: 5.6480\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 102.7360 - MAE: 7.6948 - val_loss: 47.4826 - val_MAE: 5.6323\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 102.7085 - MAE: 7.6779 - val_loss: 47.2500 - val_MAE: 5.6224\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 102.7434 - MAE: 7.6618 - val_loss: 46.8759 - val_MAE: 5.6062\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 102.6888 - MAE: 7.6500 - val_loss: 46.8411 - val_MAE: 5.6046\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 102.6967 - MAE: 7.6541 - val_loss: 46.9344 - val_MAE: 5.6087\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 102.6921 - MAE: 7.6482 - val_loss: 46.6757 - val_MAE: 5.5973\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 102.6759 - MAE: 7.6378 - val_loss: 46.5667 - val_MAE: 5.5925\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 102.6948 - MAE: 7.6415 - val_loss: 46.6869 - val_MAE: 5.5978\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 102.6837 - MAE: 7.6415 - val_loss: 46.6213 - val_MAE: 5.5949\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 102.6804 - MAE: 7.6335 - val_loss: 46.4832 - val_MAE: 5.5888\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.9, model__n_hidden=2, model__n_neurons=25, model__optimizer=momentum; total time=   5.3s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 0s 20ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.9, model__n_hidden=2, model__n_neurons=25, model__optimizer=momentum; total time=   1.0s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 18ms/step - loss: 9088549064064657795854630912.0000 - MAE: 26029447970816.0000 - val_loss: 496819922715070639628448545374208.0000 - val_MAE: 21817181880713216.0000\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: inf - MAE: 5567279307476854243806150656.0000 - val_loss: inf - val_MAE: 4587936954043047486041609994240.0000\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=momentum; total time=   1.0s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 18ms/step - loss: 18328778178375847161150570496.0000 - MAE: 37096895021056.0000 - val_loss: 887596586353663864192496029401088.0000 - val_MAE: 29246996413939712.0000\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: inf - MAE: 6224635673371360651080892416.0000 - val_loss: inf - val_MAE: 5125259750606424412919041097728.0000\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=momentum; total time=   1.7s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 18ms/step - loss: 190649036909915955454279680.0000 - MAE: 3923594444800.0000 - val_loss: 8294837784970623980657551867904.0000 - val_MAE: 2816100265885696.0000\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: inf - MAE: 551492346329894661505679360.0000 - val_loss: inf - val_MAE: 434201197476657880532024033280.0000\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=momentum; total time=   1.7s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 19ms/step - loss: 82014.8516 - MAE: 274.1828 - val_loss: 84860.1875 - val_MAE: 281.0420\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 81597.0234 - MAE: 273.4557 - val_loss: 84429.6016 - val_MAE: 280.3120\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 81175.1016 - MAE: 272.7266 - val_loss: 84001.7578 - val_MAE: 279.5845\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 80757.2344 - MAE: 271.9999 - val_loss: 83574.8047 - val_MAE: 278.8570\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 80345.9141 - MAE: 271.2761 - val_loss: 83149.2578 - val_MAE: 278.1298\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 79925.7109 - MAE: 270.5489 - val_loss: 82728.1328 - val_MAE: 277.4081\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 79516.8438 - MAE: 269.8280 - val_loss: 82305.0547 - val_MAE: 276.6816\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 79101.5078 - MAE: 269.1011 - val_loss: 81885.4609 - val_MAE: 275.9592\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 78692.3359 - MAE: 268.3799 - val_loss: 81465.4141 - val_MAE: 275.2340\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 78282.5781 - MAE: 267.6566 - val_loss: 81049.8438 - val_MAE: 274.5146\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 77880.8047 - MAE: 266.9399 - val_loss: 80632.8594 - val_MAE: 273.7913\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 77473.8047 - MAE: 266.2197 - val_loss: 80222.9062 - val_MAE: 273.0779\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 77070.3047 - MAE: 265.5045 - val_loss: 79813.9609 - val_MAE: 272.3643\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 76675.9609 - MAE: 264.7917 - val_loss: 79401.8984 - val_MAE: 271.6438\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 76268.3984 - MAE: 264.0724 - val_loss: 78996.5078 - val_MAE: 270.9327\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 75870.3047 - MAE: 263.3576 - val_loss: 78589.4453 - val_MAE: 270.2168\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 75478.2031 - MAE: 262.6438 - val_loss: 78179.3359 - val_MAE: 269.4943\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 75082.2500 - MAE: 261.9260 - val_loss: 77772.6797 - val_MAE: 268.7757\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 74685.1953 - MAE: 261.2104 - val_loss: 77373.9375 - val_MAE: 268.0690\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 74293.8047 - MAE: 260.5034 - val_loss: 76976.5625 - val_MAE: 267.3629\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 73904.4297 - MAE: 259.7959 - val_loss: 76578.9219 - val_MAE: 266.6546\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 73517.5391 - MAE: 259.0876 - val_loss: 76181.1172 - val_MAE: 265.9439\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 73134.0469 - MAE: 258.3796 - val_loss: 75782.7656 - val_MAE: 265.2306\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 72744.5078 - MAE: 257.6701 - val_loss: 75390.0703 - val_MAE: 264.5258\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 72362.2891 - MAE: 256.9650 - val_loss: 74998.4453 - val_MAE: 263.8206\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 71979.4062 - MAE: 256.2609 - val_loss: 74608.9688 - val_MAE: 263.1175\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 71599.0859 - MAE: 255.5571 - val_loss: 74219.3594 - val_MAE: 262.4126\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 71219.9141 - MAE: 254.8554 - val_loss: 73832.9062 - val_MAE: 261.7114\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 70847.8750 - MAE: 254.1550 - val_loss: 73443.6328 - val_MAE: 261.0031\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 70462.9453 - MAE: 253.4472 - val_loss: 73061.7109 - val_MAE: 260.3063\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 70091.4844 - MAE: 252.7500 - val_loss: 72677.8672 - val_MAE: 259.6045\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 69718.4844 - MAE: 252.0484 - val_loss: 72293.6484 - val_MAE: 258.8999\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 69349.1328 - MAE: 251.3482 - val_loss: 71910.5938 - val_MAE: 258.1956\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 68973.0391 - MAE: 250.6474 - val_loss: 71536.6641 - val_MAE: 257.5060\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 68608.4531 - MAE: 249.9569 - val_loss: 71162.5625 - val_MAE: 256.8146\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 68237.8281 - MAE: 249.2642 - val_loss: 70791.6484 - val_MAE: 256.1266\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 67880.2031 - MAE: 248.5734 - val_loss: 70410.6875 - val_MAE: 255.4190\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 67509.0156 - MAE: 247.8693 - val_loss: 70036.2812 - val_MAE: 254.7216\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 67148.3906 - MAE: 247.1732 - val_loss: 69661.2656 - val_MAE: 254.0212\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 66782.9766 - MAE: 246.4774 - val_loss: 69292.1719 - val_MAE: 253.3297\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 66421.5938 - MAE: 245.7861 - val_loss: 68925.0547 - val_MAE: 252.6400\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 66062.0938 - MAE: 245.0954 - val_loss: 68560.8203 - val_MAE: 251.9539\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 65704.0156 - MAE: 244.4082 - val_loss: 68197.6875 - val_MAE: 251.2680\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 65355.1133 - MAE: 243.7216 - val_loss: 67828.8359 - val_MAE: 250.5700\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 64992.6953 - MAE: 243.0281 - val_loss: 67469.0547 - val_MAE: 249.8868\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 64644.6953 - MAE: 242.3450 - val_loss: 67107.6250 - val_MAE: 249.1986\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 64294.2383 - MAE: 241.6598 - val_loss: 66747.5703 - val_MAE: 248.5110\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 63941.8008 - MAE: 240.9725 - val_loss: 66391.4141 - val_MAE: 247.8293\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 63595.4805 - MAE: 240.2915 - val_loss: 66035.8906 - val_MAE: 247.1468\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 63249.6836 - MAE: 239.6085 - val_loss: 65679.4297 - val_MAE: 246.4609\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 62902.5117 - MAE: 238.9247 - val_loss: 65326.7812 - val_MAE: 245.7803\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 62558.7461 - MAE: 238.2442 - val_loss: 64973.3555 - val_MAE: 245.0965\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 62215.1562 - MAE: 237.5613 - val_loss: 64622.7500 - val_MAE: 244.4159\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 61872.5273 - MAE: 236.8808 - val_loss: 64272.8984 - val_MAE: 243.7352\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 61535.4258 - MAE: 236.1998 - val_loss: 63920.8828 - val_MAE: 243.0485\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 61189.2305 - MAE: 235.5170 - val_loss: 63575.5547 - val_MAE: 242.3728\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 60854.5742 - MAE: 234.8414 - val_loss: 63228.9844 - val_MAE: 241.6929\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 60518.5000 - MAE: 234.1611 - val_loss: 62882.1250 - val_MAE: 241.0111\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 60177.6875 - MAE: 233.4816 - val_loss: 62541.3320 - val_MAE: 240.3387\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 59851.0586 - MAE: 232.8102 - val_loss: 62196.1523 - val_MAE: 239.6559\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 59513.3867 - MAE: 232.1288 - val_loss: 61856.9570 - val_MAE: 238.9830\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 59182.0273 - MAE: 231.4578 - val_loss: 61519.6445 - val_MAE: 238.3118\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 58853.8242 - MAE: 230.7852 - val_loss: 61181.7930 - val_MAE: 237.6379\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 58524.8281 - MAE: 230.1111 - val_loss: 60844.5156 - val_MAE: 236.9633\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 58196.1992 - MAE: 229.4380 - val_loss: 60509.1992 - val_MAE: 236.2906\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 57872.5938 - MAE: 228.7648 - val_loss: 60171.3242 - val_MAE: 235.6115\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 57545.6641 - MAE: 228.0886 - val_loss: 59835.9727 - val_MAE: 234.9351\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 57220.7344 - MAE: 227.4157 - val_loss: 59504.0000 - val_MAE: 234.2637\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 56899.4336 - MAE: 226.7478 - val_loss: 59174.9062 - val_MAE: 233.5957\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 56578.2852 - MAE: 226.0800 - val_loss: 58848.0898 - val_MAE: 232.9309\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 56257.8516 - MAE: 225.4150 - val_loss: 58524.8555 - val_MAE: 232.2713\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 55945.5742 - MAE: 224.7533 - val_loss: 58195.8203 - val_MAE: 231.5984\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 55625.5898 - MAE: 224.0856 - val_loss: 57874.3750 - val_MAE: 230.9386\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 55312.2617 - MAE: 223.4232 - val_loss: 57551.2695 - val_MAE: 230.2739\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 55001.6562 - MAE: 222.7604 - val_loss: 57227.2422 - val_MAE: 229.6054\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 54686.2578 - MAE: 222.0945 - val_loss: 56907.7578 - val_MAE: 228.9444\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 54372.9727 - MAE: 221.4329 - val_loss: 56589.4414 - val_MAE: 228.2840\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 54064.2734 - MAE: 220.7730 - val_loss: 56271.7188 - val_MAE: 227.6227\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 53756.2656 - MAE: 220.1122 - val_loss: 55954.0898 - val_MAE: 226.9600\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 53446.7461 - MAE: 219.4519 - val_loss: 55639.2578 - val_MAE: 226.3011\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 53142.5625 - MAE: 218.7935 - val_loss: 55324.6406 - val_MAE: 225.6409\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 52835.9727 - MAE: 218.1331 - val_loss: 55010.5820 - val_MAE: 224.9803\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 52533.7070 - MAE: 217.4759 - val_loss: 54697.7500 - val_MAE: 224.3201\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 52230.3594 - MAE: 216.8188 - val_loss: 54389.3281 - val_MAE: 223.6672\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 51931.2031 - MAE: 216.1678 - val_loss: 54082.5742 - val_MAE: 223.0159\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 51631.7969 - MAE: 215.5159 - val_loss: 53777.7734 - val_MAE: 222.3668\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 51334.0625 - MAE: 214.8676 - val_loss: 53474.4023 - val_MAE: 221.7190\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 51044.3086 - MAE: 214.2203 - val_loss: 53167.4336 - val_MAE: 221.0620\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 50747.7969 - MAE: 213.5657 - val_loss: 52862.9180 - val_MAE: 220.4083\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 50451.0664 - MAE: 212.9157 - val_loss: 52563.9414 - val_MAE: 219.7642\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 50162.9180 - MAE: 212.2709 - val_loss: 52262.3945 - val_MAE: 219.1129\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 49866.3125 - MAE: 211.6200 - val_loss: 51966.5352 - val_MAE: 218.4719\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 49577.5469 - MAE: 210.9767 - val_loss: 51668.1211 - val_MAE: 217.8237\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 49290.0820 - MAE: 210.3291 - val_loss: 51370.1562 - val_MAE: 217.1744\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 49001.0547 - MAE: 209.6830 - val_loss: 51074.3828 - val_MAE: 216.5278\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 48713.5234 - MAE: 209.0370 - val_loss: 50780.6172 - val_MAE: 215.8840\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 48430.9414 - MAE: 208.3939 - val_loss: 50485.1680 - val_MAE: 215.2348\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 48142.6445 - MAE: 207.7477 - val_loss: 50193.8945 - val_MAE: 214.5923\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 47860.3086 - MAE: 207.1048 - val_loss: 49901.9219 - val_MAE: 213.9470\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 47579.4023 - MAE: 206.4631 - val_loss: 49612.8320 - val_MAE: 213.3058\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.5, model__n_hidden=0, model__n_neurons=5, model__optimizer=adam; total time=   5.3s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 13840.5146 - MAE: 108.6646 - val_loss: 12516.1523 - val_MAE: 105.3925\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 13730.6230 - MAE: 108.1382 - val_loss: 12417.6113 - val_MAE: 104.9037\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 13621.9395 - MAE: 107.6228 - val_loss: 12320.2539 - val_MAE: 104.4154\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 13517.1387 - MAE: 107.1218 - val_loss: 12223.9189 - val_MAE: 103.9271\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 12ms/step - loss: 13411.0430 - MAE: 106.6067 - val_loss: 12129.8076 - val_MAE: 103.4453\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 13306.6670 - MAE: 106.1259 - val_loss: 12038.5293 - val_MAE: 102.9735\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 13205.1387 - MAE: 105.6517 - val_loss: 11948.8486 - val_MAE: 102.5057\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 13104.9727 - MAE: 105.1785 - val_loss: 11862.5127 - val_MAE: 102.0511\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 13011.3906 - MAE: 104.7317 - val_loss: 11775.1748 - val_MAE: 101.5858\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12914.7998 - MAE: 104.2583 - val_loss: 11689.1680 - val_MAE: 101.1221\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12818.8799 - MAE: 103.7991 - val_loss: 11605.5605 - val_MAE: 100.6682\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12726.1045 - MAE: 103.3453 - val_loss: 11521.9395 - val_MAE: 100.2083\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12632.6123 - MAE: 102.8925 - val_loss: 11440.0703 - val_MAE: 99.7530\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12538.2578 - MAE: 102.4392 - val_loss: 11360.9492 - val_MAE: 99.3094\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12450.4775 - MAE: 101.9907 - val_loss: 11281.6309 - val_MAE: 98.8598\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12361.6631 - MAE: 101.5417 - val_loss: 11202.2568 - val_MAE: 98.4046\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12272.2129 - MAE: 101.0876 - val_loss: 11124.9824 - val_MAE: 97.9566\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12187.1113 - MAE: 100.6561 - val_loss: 11048.3818 - val_MAE: 97.5066\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 12100.8535 - MAE: 100.2234 - val_loss: 10973.8867 - val_MAE: 97.0660\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 12016.7559 - MAE: 99.7826 - val_loss: 10900.4238 - val_MAE: 96.6271\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 11932.7295 - MAE: 99.3535 - val_loss: 10828.7988 - val_MAE: 96.1941\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 11853.1650 - MAE: 98.9406 - val_loss: 10757.1162 - val_MAE: 95.7542\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 11773.2461 - MAE: 98.5147 - val_loss: 10686.2705 - val_MAE: 95.3149\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 11692.0273 - MAE: 98.0976 - val_loss: 10617.6230 - val_MAE: 94.8865\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 11613.8760 - MAE: 97.6802 - val_loss: 10549.5010 - val_MAE: 94.4557\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 11537.3174 - MAE: 97.2881 - val_loss: 10482.8955 - val_MAE: 94.0312\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 11463.4033 - MAE: 96.8804 - val_loss: 10416.8916 - val_MAE: 93.6050\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 11385.6240 - MAE: 96.4707 - val_loss: 10354.5625 - val_MAE: 93.2008\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 11316.2744 - MAE: 96.0895 - val_loss: 10290.5010 - val_MAE: 92.7769\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 11243.4883 - MAE: 95.6990 - val_loss: 10228.1729 - val_MAE: 92.3622\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 11171.9727 - MAE: 95.3256 - val_loss: 10166.2480 - val_MAE: 91.9458\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 11102.2393 - MAE: 94.9458 - val_loss: 10104.9102 - val_MAE: 91.5276\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 11032.2871 - MAE: 94.5547 - val_loss: 10045.3125 - val_MAE: 91.1171\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 10965.1152 - MAE: 94.1883 - val_loss: 9986.8311 - val_MAE: 90.7127\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 10895.1943 - MAE: 93.8256 - val_loss: 9931.9102 - val_MAE: 90.3326\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 10833.4961 - MAE: 93.4954 - val_loss: 9874.6230 - val_MAE: 89.9246\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 10767.7188 - MAE: 93.1547 - val_loss: 9818.9424 - val_MAE: 89.5251\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 10703.0654 - MAE: 92.8162 - val_loss: 9764.9102 - val_MAE: 89.1339\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 10640.4961 - MAE: 92.4879 - val_loss: 9711.6016 - val_MAE: 88.7437\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 10579.6914 - MAE: 92.1567 - val_loss: 9659.1943 - val_MAE: 88.3580\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 10518.3916 - MAE: 91.8270 - val_loss: 9608.3457 - val_MAE: 87.9840\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 10458.9766 - MAE: 91.5030 - val_loss: 9557.7275 - val_MAE: 87.6021\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 10400.1865 - MAE: 91.1835 - val_loss: 9507.2324 - val_MAE: 87.2299\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 10340.1201 - MAE: 90.8524 - val_loss: 9458.7266 - val_MAE: 86.9078\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 10284.4248 - MAE: 90.5409 - val_loss: 9409.7598 - val_MAE: 86.5779\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 10228.5332 - MAE: 90.2203 - val_loss: 9361.2061 - val_MAE: 86.2447\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 10173.2334 - MAE: 89.9060 - val_loss: 9313.2891 - val_MAE: 85.9135\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 10115.5732 - MAE: 89.5855 - val_loss: 9268.2061 - val_MAE: 85.6030\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 10063.3818 - MAE: 89.3017 - val_loss: 9223.0479 - val_MAE: 85.2868\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 10010.5703 - MAE: 89.0087 - val_loss: 9179.8213 - val_MAE: 84.9907\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9960.0029 - MAE: 88.7299 - val_loss: 9136.9727 - val_MAE: 84.7331\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9910.6855 - MAE: 88.4684 - val_loss: 9094.6133 - val_MAE: 84.4751\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9860.2891 - MAE: 88.1958 - val_loss: 9053.7295 - val_MAE: 84.2259\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9812.5205 - MAE: 87.9474 - val_loss: 9012.6328 - val_MAE: 83.9705\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9764.6787 - MAE: 87.6988 - val_loss: 8972.1543 - val_MAE: 83.7160\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9715.8096 - MAE: 87.4345 - val_loss: 8932.7998 - val_MAE: 83.4666\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9669.7217 - MAE: 87.1960 - val_loss: 8892.8271 - val_MAE: 83.2093\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9621.8389 - MAE: 86.9297 - val_loss: 8854.2197 - val_MAE: 82.9596\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9576.3242 - MAE: 86.6911 - val_loss: 8815.9932 - val_MAE: 82.7470\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9531.6777 - MAE: 86.4328 - val_loss: 8777.5146 - val_MAE: 82.5363\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9486.0342 - MAE: 86.1897 - val_loss: 8739.9893 - val_MAE: 82.3286\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9440.3154 - MAE: 85.9378 - val_loss: 8703.5508 - val_MAE: 82.1257\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9396.7500 - MAE: 85.7050 - val_loss: 8667.6504 - val_MAE: 81.9238\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9355.1113 - MAE: 85.4725 - val_loss: 8631.7822 - val_MAE: 81.7185\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 9312.9512 - MAE: 85.2334 - val_loss: 8596.7354 - val_MAE: 81.5163\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 9269.6553 - MAE: 84.9982 - val_loss: 8563.6445 - val_MAE: 81.3256\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9231.5889 - MAE: 84.7877 - val_loss: 8529.4854 - val_MAE: 81.1248\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9189.0645 - MAE: 84.5513 - val_loss: 8496.9893 - val_MAE: 80.9329\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 9150.9336 - MAE: 84.3337 - val_loss: 8464.0508 - val_MAE: 80.7350\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9111.5693 - MAE: 84.1146 - val_loss: 8431.5918 - val_MAE: 80.5384\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9071.6045 - MAE: 83.8888 - val_loss: 8399.8545 - val_MAE: 80.3450\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9033.1523 - MAE: 83.6830 - val_loss: 8368.7119 - val_MAE: 80.1542\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8996.7637 - MAE: 83.4906 - val_loss: 8337.2480 - val_MAE: 79.9565\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8959.2734 - MAE: 83.2842 - val_loss: 8306.3584 - val_MAE: 79.7619\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8922.0527 - MAE: 83.0909 - val_loss: 8276.7490 - val_MAE: 79.5739\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8885.8379 - MAE: 82.8868 - val_loss: 8247.4648 - val_MAE: 79.3877\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8849.7119 - MAE: 82.6959 - val_loss: 8218.8350 - val_MAE: 79.2054\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8816.0967 - MAE: 82.5291 - val_loss: 8189.8652 - val_MAE: 79.0157\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8780.5605 - MAE: 82.3320 - val_loss: 8162.0708 - val_MAE: 78.8346\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8747.6084 - MAE: 82.1595 - val_loss: 8133.9678 - val_MAE: 78.6474\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8712.3740 - MAE: 81.9769 - val_loss: 8107.0068 - val_MAE: 78.4689\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8680.3105 - MAE: 81.8034 - val_loss: 8079.8071 - val_MAE: 78.2866\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8647.8516 - MAE: 81.6323 - val_loss: 8053.3179 - val_MAE: 78.1090\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8616.7461 - MAE: 81.4621 - val_loss: 8026.5913 - val_MAE: 77.9262\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8584.8477 - MAE: 81.2896 - val_loss: 8000.4282 - val_MAE: 77.7468\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8551.9170 - MAE: 81.1117 - val_loss: 7975.0913 - val_MAE: 77.5755\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8521.3457 - MAE: 80.9501 - val_loss: 7949.8960 - val_MAE: 77.4168\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 8490.8516 - MAE: 80.7844 - val_loss: 7924.8135 - val_MAE: 77.2726\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 8460.2578 - MAE: 80.6198 - val_loss: 7900.0449 - val_MAE: 77.1300\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8430.3369 - MAE: 80.4537 - val_loss: 7875.4644 - val_MAE: 76.9853\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8400.1602 - MAE: 80.2842 - val_loss: 7851.5425 - val_MAE: 76.8442\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8372.5518 - MAE: 80.1294 - val_loss: 7827.0688 - val_MAE: 76.6980\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8341.7715 - MAE: 79.9570 - val_loss: 7803.3613 - val_MAE: 76.5569\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8314.6445 - MAE: 79.7937 - val_loss: 7779.9229 - val_MAE: 76.4126\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8286.1719 - MAE: 79.6362 - val_loss: 7756.7021 - val_MAE: 76.2708\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8256.8438 - MAE: 79.4751 - val_loss: 7733.7969 - val_MAE: 76.1338\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8230.2031 - MAE: 79.3246 - val_loss: 7710.8252 - val_MAE: 75.9920\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8202.2598 - MAE: 79.1647 - val_loss: 7688.7686 - val_MAE: 75.8546\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8175.7783 - MAE: 79.0116 - val_loss: 7666.6792 - val_MAE: 75.7182\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8148.6704 - MAE: 78.8525 - val_loss: 7644.7207 - val_MAE: 75.5808\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.5, model__n_hidden=0, model__n_neurons=5, model__optimizer=adam; total time=   5.7s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 13469.4014 - MAE: 76.4447 - val_loss: 8889.8037 - val_MAE: 67.9599\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 13355.4033 - MAE: 76.0129 - val_loss: 8781.6758 - val_MAE: 67.4586\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 13247.7803 - MAE: 75.5961 - val_loss: 8673.2852 - val_MAE: 66.9509\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 13140.1123 - MAE: 75.1708 - val_loss: 8566.7773 - val_MAE: 66.4469\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 13035.3838 - MAE: 74.7717 - val_loss: 8460.7266 - val_MAE: 65.9399\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12925.4912 - MAE: 74.3477 - val_loss: 8359.0850 - val_MAE: 65.4494\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12820.0703 - MAE: 73.9685 - val_loss: 8258.7793 - val_MAE: 64.9601\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12721.3105 - MAE: 73.6080 - val_loss: 8157.4287 - val_MAE: 64.5023\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12615.3984 - MAE: 73.2271 - val_loss: 8061.1055 - val_MAE: 64.0909\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12518.4385 - MAE: 72.8493 - val_loss: 7962.9590 - val_MAE: 63.7025\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12417.3223 - MAE: 72.4986 - val_loss: 7868.8584 - val_MAE: 63.3483\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12321.7529 - MAE: 72.1460 - val_loss: 7775.4512 - val_MAE: 62.9932\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12228.7559 - MAE: 71.8006 - val_loss: 7681.1523 - val_MAE: 62.6343\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12132.0430 - MAE: 71.4317 - val_loss: 7590.0322 - val_MAE: 62.3243\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12037.7061 - MAE: 71.1123 - val_loss: 7501.2241 - val_MAE: 62.0195\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 11946.0127 - MAE: 70.8034 - val_loss: 7414.2666 - val_MAE: 61.7179\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 11856.8574 - MAE: 70.4679 - val_loss: 7327.7715 - val_MAE: 61.4137\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 11767.4961 - MAE: 70.1785 - val_loss: 7241.9805 - val_MAE: 61.1092\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 11677.9941 - MAE: 69.8919 - val_loss: 7156.8340 - val_MAE: 60.8033\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 11592.9180 - MAE: 69.5932 - val_loss: 7071.1528 - val_MAE: 60.4912\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 11501.8154 - MAE: 69.3095 - val_loss: 6990.8696 - val_MAE: 60.1961\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 11421.1279 - MAE: 69.0403 - val_loss: 6909.6904 - val_MAE: 59.8941\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 11335.9824 - MAE: 68.7700 - val_loss: 6831.3511 - val_MAE: 59.5993\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 11255.0215 - MAE: 68.5075 - val_loss: 6754.7656 - val_MAE: 59.3081\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 11174.6074 - MAE: 68.2536 - val_loss: 6679.9507 - val_MAE: 59.0203\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 11095.2891 - MAE: 68.0098 - val_loss: 6607.2803 - val_MAE: 58.7373\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 11021.1895 - MAE: 67.7782 - val_loss: 6532.0430 - val_MAE: 58.4409\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 10942.8779 - MAE: 67.5694 - val_loss: 6458.8418 - val_MAE: 58.1791\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 10865.3994 - MAE: 67.3524 - val_loss: 6389.9995 - val_MAE: 57.9423\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 10793.3428 - MAE: 67.1506 - val_loss: 6320.9341 - val_MAE: 57.7012\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 10719.8760 - MAE: 66.9695 - val_loss: 6252.0947 - val_MAE: 57.4591\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 10648.8740 - MAE: 66.7799 - val_loss: 6184.0479 - val_MAE: 57.2165\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 10576.0840 - MAE: 66.5977 - val_loss: 6117.9199 - val_MAE: 56.9779\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 10507.2266 - MAE: 66.4176 - val_loss: 6051.8501 - val_MAE: 56.7363\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 10437.4883 - MAE: 66.2532 - val_loss: 5987.7754 - val_MAE: 56.4997\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 10367.1523 - MAE: 66.0727 - val_loss: 5926.7456 - val_MAE: 56.2723\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 10301.0879 - MAE: 65.9131 - val_loss: 5864.6279 - val_MAE: 56.0372\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 10234.4814 - MAE: 65.7631 - val_loss: 5802.4800 - val_MAE: 55.7998\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 10169.7188 - MAE: 65.6182 - val_loss: 5740.3164 - val_MAE: 55.5599\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 10104.7227 - MAE: 65.4839 - val_loss: 5679.5244 - val_MAE: 55.3215\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 10038.2041 - MAE: 65.3083 - val_loss: 5622.4629 - val_MAE: 55.0946\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9975.5840 - MAE: 65.1768 - val_loss: 5566.1372 - val_MAE: 54.8684\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9917.1533 - MAE: 65.0513 - val_loss: 5507.9180 - val_MAE: 54.6323\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9854.8340 - MAE: 64.9275 - val_loss: 5452.2666 - val_MAE: 54.4270\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9795.3379 - MAE: 64.7922 - val_loss: 5397.7632 - val_MAE: 54.2343\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9733.7451 - MAE: 64.6667 - val_loss: 5346.3481 - val_MAE: 54.0503\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9679.6025 - MAE: 64.5476 - val_loss: 5291.9028 - val_MAE: 53.8539\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9620.6758 - MAE: 64.4149 - val_loss: 5239.4756 - val_MAE: 53.6620\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9564.5908 - MAE: 64.3189 - val_loss: 5188.1597 - val_MAE: 53.4779\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9505.5137 - MAE: 64.1692 - val_loss: 5139.8438 - val_MAE: 53.3210\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9451.9893 - MAE: 64.0640 - val_loss: 5090.5474 - val_MAE: 53.1908\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9397.2734 - MAE: 63.9489 - val_loss: 5042.0698 - val_MAE: 53.0637\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9345.0576 - MAE: 63.8503 - val_loss: 4993.3311 - val_MAE: 52.9366\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9291.9502 - MAE: 63.7332 - val_loss: 4945.8081 - val_MAE: 52.8072\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9238.9229 - MAE: 63.6263 - val_loss: 4900.1499 - val_MAE: 52.6810\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9190.6992 - MAE: 63.5109 - val_loss: 4853.6724 - val_MAE: 52.5525\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9138.0908 - MAE: 63.4180 - val_loss: 4810.0327 - val_MAE: 52.4304\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9089.5615 - MAE: 63.3227 - val_loss: 4767.3887 - val_MAE: 52.3078\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9041.3838 - MAE: 63.2144 - val_loss: 4726.2368 - val_MAE: 52.1872\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8994.2197 - MAE: 63.1367 - val_loss: 4685.1597 - val_MAE: 52.0664\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8950.5107 - MAE: 63.0577 - val_loss: 4642.9780 - val_MAE: 51.9399\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 8900.0742 - MAE: 62.9600 - val_loss: 4605.0298 - val_MAE: 51.8246\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 8855.6309 - MAE: 62.8732 - val_loss: 4566.3965 - val_MAE: 51.7041\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 8812.9004 - MAE: 62.7952 - val_loss: 4526.0044 - val_MAE: 51.5786\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8765.5352 - MAE: 62.6943 - val_loss: 4488.0288 - val_MAE: 51.4558\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8721.6377 - MAE: 62.6113 - val_loss: 4449.9185 - val_MAE: 51.3498\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8677.5566 - MAE: 62.5198 - val_loss: 4413.5454 - val_MAE: 51.2513\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8636.3086 - MAE: 62.4336 - val_loss: 4376.4111 - val_MAE: 51.1509\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8592.1328 - MAE: 62.3396 - val_loss: 4341.3613 - val_MAE: 51.0531\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8552.7656 - MAE: 62.2631 - val_loss: 4306.4272 - val_MAE: 50.9533\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8511.8213 - MAE: 62.1785 - val_loss: 4272.1011 - val_MAE: 50.8528\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8470.9648 - MAE: 62.0903 - val_loss: 4239.1177 - val_MAE: 50.7536\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8431.0498 - MAE: 62.0081 - val_loss: 4206.4810 - val_MAE: 50.6559\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8391.1328 - MAE: 61.9255 - val_loss: 4173.4248 - val_MAE: 50.5539\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8353.7549 - MAE: 61.8515 - val_loss: 4139.7148 - val_MAE: 50.4510\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8311.7354 - MAE: 61.7643 - val_loss: 4109.2275 - val_MAE: 50.3549\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 8274.8242 - MAE: 61.6913 - val_loss: 4078.1611 - val_MAE: 50.2548\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 8237.4863 - MAE: 61.6241 - val_loss: 4047.4233 - val_MAE: 50.1541\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 8199.8984 - MAE: 61.5618 - val_loss: 4017.3645 - val_MAE: 50.0539\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8164.0430 - MAE: 61.5031 - val_loss: 3986.5457 - val_MAE: 49.9515\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8128.6582 - MAE: 61.4546 - val_loss: 3955.9866 - val_MAE: 49.8489\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8089.9424 - MAE: 61.3798 - val_loss: 3928.7104 - val_MAE: 49.7523\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8056.1191 - MAE: 61.3231 - val_loss: 3901.2300 - val_MAE: 49.6515\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8023.0186 - MAE: 61.2698 - val_loss: 3874.0122 - val_MAE: 49.5503\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7987.3418 - MAE: 61.2068 - val_loss: 3848.3936 - val_MAE: 49.4530\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7955.2974 - MAE: 61.1548 - val_loss: 3822.0322 - val_MAE: 49.3530\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7921.6147 - MAE: 61.1044 - val_loss: 3796.0186 - val_MAE: 49.2554\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7888.5747 - MAE: 61.0530 - val_loss: 3770.1199 - val_MAE: 49.1550\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7856.0166 - MAE: 60.9931 - val_loss: 3744.7559 - val_MAE: 49.0531\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7822.1118 - MAE: 60.9362 - val_loss: 3719.4031 - val_MAE: 48.9512\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 7790.5469 - MAE: 60.8795 - val_loss: 3692.8643 - val_MAE: 48.8448\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7757.9985 - MAE: 60.8268 - val_loss: 3667.8623 - val_MAE: 48.7424\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7726.0312 - MAE: 60.7746 - val_loss: 3644.2268 - val_MAE: 48.6428\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7693.9536 - MAE: 60.7068 - val_loss: 3622.0720 - val_MAE: 48.5448\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7665.2437 - MAE: 60.6600 - val_loss: 3597.9973 - val_MAE: 48.4454\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7632.2324 - MAE: 60.5901 - val_loss: 3576.1160 - val_MAE: 48.3475\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7606.2383 - MAE: 60.5443 - val_loss: 3551.5847 - val_MAE: 48.2411\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7572.3647 - MAE: 60.4716 - val_loss: 3530.4487 - val_MAE: 48.1413\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7543.8936 - MAE: 60.4177 - val_loss: 3509.1816 - val_MAE: 48.0425\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7514.3545 - MAE: 60.3506 - val_loss: 3489.1626 - val_MAE: 47.9454\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.5, model__n_hidden=0, model__n_neurons=5, model__optimizer=adam; total time=   5.5s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 48824429007227190123272601600.0000 - MAE: 60328599617536.0000 - val_loss: 2677261355491490143327326031577088.0000 - val_MAE: 50645996656394240.0000\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: inf - MAE: 13074287176075105302896181248.0000 - val_loss: inf - val_MAE: 10791950227711276262651164884992.0000\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=0, model__n_neurons=25, model__optimizer=sgd; total time=   1.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 242784571623351271028686848.0000 - MAE: 4269339836416.0000 - val_loss: 11798425702795780791873520533504.0000 - val_MAE: 3371975602864128.0000\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: inf - MAE: 726206403179409579020648448.0000 - val_loss: inf - val_MAE: 598930449688507551720853733376.0000\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=0, model__n_neurons=25, model__optimizer=sgd; total time=   1.4s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 20ms/step - loss: 54898936518037452242070536192.0000 - MAE: 66577730895872.0000 - val_loss: 2397597702246510242734727428571136.0000 - val_MAE: 47877469327327232.0000\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: inf - MAE: 9490102511427650185076932608.0000 - val_loss: inf - val_MAE: 7484244560438277816612837916672.0000\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=0, model__n_neurons=25, model__optimizer=sgd; total time=   1.7s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 18ms/step - loss: 3211337597690111763792527360.0000 - MAE: 15472041918464.0000 - val_loss: 176091960799748861081100438470656.0000 - val_MAE: 12988813253017600.0000\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: inf - MAE: 3353069982940478425591185408.0000 - val_loss: inf - val_MAE: 2767735323164665481268289339392.0000\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=nesterov; total time=   1.0s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 18ms/step - loss: 25729778364166640424483553280.0000 - MAE: 43950945599488.0000 - val_loss: 1250371433159884063619661229457408.0000 - val_MAE: 34712984395710464.0000\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: inf - MAE: 7475971295481211033334317056.0000 - val_loss: inf - val_MAE: 6165722361720664819444498825216.0000\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=nesterov; total time=   1.0s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 19ms/step - loss: 26614208338884128193642496.0000 - MAE: 1465898237952.0000 - val_loss: 1162320444784801879477754789888.0000 - val_MAE: 1054158652178432.0000\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: inf - MAE: 208951619678689466924400640.0000 - val_loss: inf - val_MAE: 164786940640804406774760210432.0000\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=nesterov; total time=   1.7s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 21ms/step - loss: 56720.1289 - MAE: 152.5771 - val_loss: 446.0658 - val_MAE: 20.1696\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 589.1381 - MAE: 22.4869 - val_loss: 429.1445 - val_MAE: 19.7963\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 445.6336 - MAE: 18.9021 - val_loss: 27.8072 - val_MAE: 4.4795\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 84.0478 - MAE: 6.6845 - val_loss: 27.4188 - val_MAE: 4.4473\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 73.3419 - MAE: 6.1615 - val_loss: 21.2686 - val_MAE: 3.8049\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 70.6641 - MAE: 5.9425 - val_loss: 18.0673 - val_MAE: 3.5087\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 70.4764 - MAE: 5.9169 - val_loss: 17.6730 - val_MAE: 3.2901\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 70.1459 - MAE: 5.8097 - val_loss: 23.3708 - val_MAE: 3.8621\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 69.0611 - MAE: 5.8331 - val_loss: 27.6186 - val_MAE: 4.2478\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 69.1987 - MAE: 6.0115 - val_loss: 19.0805 - val_MAE: 3.4656\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 67.9548 - MAE: 5.8317 - val_loss: 18.4147 - val_MAE: 3.4162\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 69.0860 - MAE: 5.8551 - val_loss: 17.5574 - val_MAE: 3.3436\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 68.0279 - MAE: 5.7932 - val_loss: 24.3593 - val_MAE: 4.0377\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 69.0996 - MAE: 6.0369 - val_loss: 16.8168 - val_MAE: 3.2819\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 67.6914 - MAE: 5.8749 - val_loss: 16.4386 - val_MAE: 3.2689\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 67.8344 - MAE: 5.9359 - val_loss: 16.5827 - val_MAE: 3.3335\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 68.0723 - MAE: 5.6710 - val_loss: 18.2085 - val_MAE: 3.4565\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 67.2170 - MAE: 5.8202 - val_loss: 17.6087 - val_MAE: 3.4113\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 67.8700 - MAE: 5.8021 - val_loss: 22.4197 - val_MAE: 3.9157\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 67.6135 - MAE: 5.7871 - val_loss: 27.2337 - val_MAE: 4.3564\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 67.8604 - MAE: 5.9425 - val_loss: 20.4814 - val_MAE: 3.7266\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 67.3280 - MAE: 5.8248 - val_loss: 18.1264 - val_MAE: 3.4644\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 67.4796 - MAE: 5.7421 - val_loss: 20.6789 - val_MAE: 3.7578\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 66.5984 - MAE: 5.7697 - val_loss: 19.9309 - val_MAE: 3.6776\n",
            "Epoch 24: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=1, model__n_neurons=125, model__optimizer=momentum; total time=   3.0s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 39984.8594 - MAE: 117.9308 - val_loss: 451.1975 - val_MAE: 20.3664\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 639.7012 - MAE: 23.2118 - val_loss: 450.0269 - val_MAE: 20.3169\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 638.0840 - MAE: 23.1633 - val_loss: 449.2550 - val_MAE: 20.2865\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 636.9373 - MAE: 23.1307 - val_loss: 448.6301 - val_MAE: 20.2655\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 632.7169 - MAE: 23.0503 - val_loss: 432.2086 - val_MAE: 19.9162\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 448.5094 - MAE: 18.1036 - val_loss: 42.1280 - val_MAE: 5.0925\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 103.4460 - MAE: 7.8079 - val_loss: 34.4266 - val_MAE: 4.6878\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 100.8587 - MAE: 7.5571 - val_loss: 42.3063 - val_MAE: 4.8961\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 97.6627 - MAE: 7.5936 - val_loss: 42.2781 - val_MAE: 4.8558\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 97.4822 - MAE: 7.6407 - val_loss: 39.8593 - val_MAE: 4.6982\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 97.4075 - MAE: 7.7475 - val_loss: 31.1523 - val_MAE: 4.3972\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 96.6531 - MAE: 7.5155 - val_loss: 37.8889 - val_MAE: 4.6013\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 96.2549 - MAE: 7.6169 - val_loss: 36.9959 - val_MAE: 4.5537\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 97.8681 - MAE: 7.6367 - val_loss: 36.7637 - val_MAE: 4.5411\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 97.0797 - MAE: 7.6290 - val_loss: 36.3939 - val_MAE: 4.5294\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 94.6745 - MAE: 7.5386 - val_loss: 29.5416 - val_MAE: 4.3673\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 96.1027 - MAE: 7.4716 - val_loss: 35.4069 - val_MAE: 4.4761\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 96.2300 - MAE: 7.5438 - val_loss: 41.3045 - val_MAE: 4.7946\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 96.8954 - MAE: 7.6143 - val_loss: 39.5133 - val_MAE: 4.6806\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 96.1662 - MAE: 7.5643 - val_loss: 34.7909 - val_MAE: 4.4358\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 94.1798 - MAE: 7.4836 - val_loss: 28.7536 - val_MAE: 4.3302\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 95.4316 - MAE: 7.4583 - val_loss: 34.0005 - val_MAE: 4.3937\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 94.3862 - MAE: 7.4328 - val_loss: 34.3832 - val_MAE: 4.4027\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 97.7063 - MAE: 7.7000 - val_loss: 30.8645 - val_MAE: 4.3027\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 95.4639 - MAE: 7.5519 - val_loss: 28.1436 - val_MAE: 4.3084\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 95.0452 - MAE: 7.5649 - val_loss: 32.3916 - val_MAE: 4.3190\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 95.8965 - MAE: 7.5353 - val_loss: 30.6807 - val_MAE: 4.2813\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 96.5653 - MAE: 7.5426 - val_loss: 26.9827 - val_MAE: 4.3046\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 93.6727 - MAE: 7.3065 - val_loss: 31.5535 - val_MAE: 4.2887\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 93.3718 - MAE: 7.3810 - val_loss: 31.8881 - val_MAE: 4.2951\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 95.7383 - MAE: 7.4467 - val_loss: 27.4736 - val_MAE: 4.2409\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 92.7302 - MAE: 7.0418 - val_loss: 57.2623 - val_MAE: 5.8044\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 94.7541 - MAE: 7.5404 - val_loss: 36.3977 - val_MAE: 4.4404\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 93.9763 - MAE: 7.4279 - val_loss: 48.4884 - val_MAE: 5.2474\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 93.0579 - MAE: 7.4550 - val_loss: 40.9175 - val_MAE: 4.7292\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 92.1132 - MAE: 7.5200 - val_loss: 27.8436 - val_MAE: 4.1870\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 94.5255 - MAE: 7.3902 - val_loss: 32.2597 - val_MAE: 4.2469\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 92.6974 - MAE: 7.3655 - val_loss: 28.6893 - val_MAE: 4.1833\n",
            "Epoch 38: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=1, model__n_neurons=125, model__optimizer=momentum; total time=   3.0s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 126070.4766 - MAE: 181.2288 - val_loss: 550.3503 - val_MAE: 21.8683\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 557.8572 - MAE: 21.8920 - val_loss: 549.3784 - val_MAE: 21.8512\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 555.7729 - MAE: 21.8186 - val_loss: 548.8112 - val_MAE: 21.8408\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 513.0737 - MAE: 20.6344 - val_loss: 216.1621 - val_MAE: 11.8220\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 142.6655 - MAE: 9.0495 - val_loss: 125.6873 - val_MAE: 8.5124\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 106.6213 - MAE: 7.4101 - val_loss: 104.5067 - val_MAE: 7.4473\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 94.6530 - MAE: 6.7655 - val_loss: 97.2798 - val_MAE: 7.2462\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 84.3683 - MAE: 6.4424 - val_loss: 82.6633 - val_MAE: 6.2699\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 79.0555 - MAE: 6.2200 - val_loss: 76.8149 - val_MAE: 6.2764\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 74.7377 - MAE: 6.1650 - val_loss: 69.9616 - val_MAE: 5.7725\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 72.3637 - MAE: 6.0545 - val_loss: 68.1909 - val_MAE: 5.9633\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 71.8608 - MAE: 6.0968 - val_loss: 61.9444 - val_MAE: 5.5521\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 68.4407 - MAE: 5.8173 - val_loss: 57.8435 - val_MAE: 5.5753\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 67.5742 - MAE: 5.8540 - val_loss: 54.5515 - val_MAE: 5.4659\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 64.3973 - MAE: 5.6800 - val_loss: 54.1759 - val_MAE: 5.2188\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 63.0078 - MAE: 5.5834 - val_loss: 50.5019 - val_MAE: 5.2745\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 59.4924 - MAE: 5.6674 - val_loss: 58.4026 - val_MAE: 5.6204\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 62.0413 - MAE: 5.6673 - val_loss: 51.5850 - val_MAE: 5.3107\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 59.0170 - MAE: 5.5146 - val_loss: 48.7582 - val_MAE: 5.2813\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 59.3836 - MAE: 5.2803 - val_loss: 45.8986 - val_MAE: 5.3840\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 59.2420 - MAE: 5.5217 - val_loss: 45.3768 - val_MAE: 5.3738\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 57.0687 - MAE: 5.2861 - val_loss: 46.1393 - val_MAE: 5.4997\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 60.3398 - MAE: 5.6758 - val_loss: 44.7404 - val_MAE: 5.3737\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 57.5222 - MAE: 5.2405 - val_loss: 46.4126 - val_MAE: 5.4392\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 56.9356 - MAE: 5.4334 - val_loss: 44.0375 - val_MAE: 5.3461\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 56.2182 - MAE: 5.3328 - val_loss: 45.6113 - val_MAE: 5.4239\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 57.6181 - MAE: 5.5677 - val_loss: 49.0979 - val_MAE: 5.2998\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 57.7235 - MAE: 5.3987 - val_loss: 44.6664 - val_MAE: 5.2787\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 55.7709 - MAE: 5.2935 - val_loss: 45.1275 - val_MAE: 5.2925\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 56.7846 - MAE: 5.3173 - val_loss: 44.3298 - val_MAE: 5.2659\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 55.8509 - MAE: 5.3261 - val_loss: 46.0742 - val_MAE: 5.3201\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 56.6449 - MAE: 5.3294 - val_loss: 44.2709 - val_MAE: 5.3412\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 55.7036 - MAE: 5.3958 - val_loss: 46.5049 - val_MAE: 5.2871\n",
            "Epoch 33: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=1, model__n_neurons=125, model__optimizer=momentum; total time=   2.1s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 26244.9570 - MAE: 127.7041 - val_loss: 31605.3887 - val_MAE: 140.4734\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 26042.7188 - MAE: 126.9988 - val_loss: 31380.0469 - val_MAE: 139.7490\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 25837.1758 - MAE: 126.2940 - val_loss: 31158.0039 - val_MAE: 139.0309\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 25636.0840 - MAE: 125.5951 - val_loss: 30936.5332 - val_MAE: 138.3119\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 25439.8008 - MAE: 124.9017 - val_loss: 30717.0293 - val_MAE: 137.5954\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 25237.9707 - MAE: 124.2057 - val_loss: 30500.8828 - val_MAE: 136.8859\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 25043.3926 - MAE: 123.5220 - val_loss: 30284.0801 - val_MAE: 136.1715\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 24847.3555 - MAE: 122.8150 - val_loss: 30068.1836 - val_MAE: 135.4585\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 24650.7891 - MAE: 122.1285 - val_loss: 29854.1074 - val_MAE: 134.7461\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 24457.0781 - MAE: 121.4358 - val_loss: 29643.5703 - val_MAE: 134.0420\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 24271.3184 - MAE: 120.7639 - val_loss: 29430.3242 - val_MAE: 133.3272\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 24079.8340 - MAE: 120.0925 - val_loss: 29224.3535 - val_MAE: 132.6323\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23890.7988 - MAE: 119.4274 - val_loss: 29020.3281 - val_MAE: 131.9404\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 23713.2656 - MAE: 118.7649 - val_loss: 28811.7617 - val_MAE: 131.2307\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23521.5117 - MAE: 118.0965 - val_loss: 28611.4883 - val_MAE: 130.5438\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23338.3730 - MAE: 117.4443 - val_loss: 28410.9238 - val_MAE: 129.8526\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 23163.3477 - MAE: 116.7927 - val_loss: 28206.0137 - val_MAE: 129.1454\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 22981.4082 - MAE: 116.1387 - val_loss: 28004.7852 - val_MAE: 128.4464\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 22798.6582 - MAE: 115.4824 - val_loss: 27810.6406 - val_MAE: 127.7684\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 22623.6328 - MAE: 114.8489 - val_loss: 27615.7480 - val_MAE: 127.0855\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 22448.9766 - MAE: 114.1990 - val_loss: 27421.3984 - val_MAE: 126.4006\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 22274.7031 - MAE: 113.5773 - val_loss: 27229.2227 - val_MAE: 125.7181\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 22105.8633 - MAE: 112.9354 - val_loss: 27035.8750 - val_MAE: 125.0342\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 21932.9688 - MAE: 112.2843 - val_loss: 26846.2266 - val_MAE: 124.3979\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 21764.1191 - MAE: 111.6694 - val_loss: 26658.8379 - val_MAE: 123.7644\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 21594.6855 - MAE: 111.0368 - val_loss: 26473.5527 - val_MAE: 123.1351\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 21430.7695 - MAE: 110.4020 - val_loss: 26286.8867 - val_MAE: 122.4991\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 21265.8125 - MAE: 109.7860 - val_loss: 26104.3652 - val_MAE: 121.8736\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 21104.5508 - MAE: 109.1614 - val_loss: 25922.0215 - val_MAE: 121.2439\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 20938.4863 - MAE: 108.5357 - val_loss: 25742.8145 - val_MAE: 120.6236\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 20780.9414 - MAE: 107.9157 - val_loss: 25562.5801 - val_MAE: 119.9971\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 20619.8086 - MAE: 107.2964 - val_loss: 25385.0527 - val_MAE: 119.3762\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 20465.2793 - MAE: 106.6882 - val_loss: 25207.1426 - val_MAE: 118.7500\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 20306.0410 - MAE: 106.0615 - val_loss: 25035.2539 - val_MAE: 118.1429\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 20153.4316 - MAE: 105.4650 - val_loss: 24864.1875 - val_MAE: 117.5367\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 19994.7852 - MAE: 104.8784 - val_loss: 24697.7168 - val_MAE: 116.9410\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 19849.3867 - MAE: 104.2710 - val_loss: 24522.3027 - val_MAE: 116.3119\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 19696.1914 - MAE: 103.6652 - val_loss: 24349.9492 - val_MAE: 115.6921\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 19544.9219 - MAE: 103.0716 - val_loss: 24180.4238 - val_MAE: 115.0785\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 19395.6328 - MAE: 102.4725 - val_loss: 24013.3652 - val_MAE: 114.4705\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 19246.8457 - MAE: 101.8808 - val_loss: 23848.3730 - val_MAE: 113.8671\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 19100.1934 - MAE: 101.2946 - val_loss: 23685.5664 - val_MAE: 113.2683\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 18953.9961 - MAE: 100.7170 - val_loss: 23524.1211 - val_MAE: 112.6721\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 18816.2734 - MAE: 100.1415 - val_loss: 23357.5664 - val_MAE: 112.0572\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 18666.8711 - MAE: 99.5690 - val_loss: 23199.1367 - val_MAE: 111.5017\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 18528.3164 - MAE: 99.0036 - val_loss: 23040.7461 - val_MAE: 110.9426\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 18388.0391 - MAE: 98.4440 - val_loss: 22884.4141 - val_MAE: 110.3877\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 18250.0352 - MAE: 97.8828 - val_loss: 22728.2383 - val_MAE: 109.8324\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 18112.6074 - MAE: 97.3142 - val_loss: 22574.2461 - val_MAE: 109.2809\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 17977.5547 - MAE: 96.7600 - val_loss: 22418.5137 - val_MAE: 108.7212\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 17841.3203 - MAE: 96.1977 - val_loss: 22266.1172 - val_MAE: 108.1699\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 17706.3809 - MAE: 95.6496 - val_loss: 22114.2285 - val_MAE: 107.6178\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 17573.1133 - MAE: 95.0834 - val_loss: 21964.2676 - val_MAE: 107.0692\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 17442.2734 - MAE: 94.5557 - val_loss: 21814.2266 - val_MAE: 106.5176\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 17312.0918 - MAE: 94.0026 - val_loss: 21664.1855 - val_MAE: 105.9638\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 17178.9023 - MAE: 93.4750 - val_loss: 21518.1777 - val_MAE: 105.4218\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 17051.4395 - MAE: 92.9465 - val_loss: 21372.9258 - val_MAE: 104.8795\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 16926.7539 - MAE: 92.4094 - val_loss: 21224.5781 - val_MAE: 104.3253\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16794.7129 - MAE: 91.8834 - val_loss: 21083.9766 - val_MAE: 103.7948\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 16674.4258 - MAE: 91.3598 - val_loss: 20941.1504 - val_MAE: 103.2522\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16548.2246 - MAE: 90.8378 - val_loss: 20801.3984 - val_MAE: 102.7202\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 16426.1113 - MAE: 90.3497 - val_loss: 20663.2500 - val_MAE: 102.1902\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16305.9463 - MAE: 89.8633 - val_loss: 20525.5137 - val_MAE: 101.6596\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 16185.9629 - MAE: 89.3559 - val_loss: 20387.3828 - val_MAE: 101.1255\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 16065.0742 - MAE: 88.8717 - val_loss: 20251.8262 - val_MAE: 100.5969\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 15949.5967 - MAE: 88.3885 - val_loss: 20113.1914 - val_MAE: 100.0556\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 15829.3887 - MAE: 87.9015 - val_loss: 19977.6328 - val_MAE: 99.5223\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 15714.3828 - MAE: 87.4299 - val_loss: 19842.2539 - val_MAE: 99.0219\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 15596.3770 - MAE: 86.9736 - val_loss: 19712.9043 - val_MAE: 98.5415\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 15484.1689 - MAE: 86.5028 - val_loss: 19583.3301 - val_MAE: 98.0931\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 15371.3535 - MAE: 86.0482 - val_loss: 19455.9531 - val_MAE: 97.6506\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 15262.8740 - MAE: 85.5992 - val_loss: 19325.5859 - val_MAE: 97.2074\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 15149.6387 - MAE: 85.1557 - val_loss: 19201.8887 - val_MAE: 96.8079\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 15043.6357 - MAE: 84.7206 - val_loss: 19075.4473 - val_MAE: 96.3982\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 14935.6973 - MAE: 84.2931 - val_loss: 18949.5469 - val_MAE: 95.9874\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 14827.3477 - MAE: 83.8497 - val_loss: 18826.3477 - val_MAE: 95.5837\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 14721.4160 - MAE: 83.4204 - val_loss: 18702.8945 - val_MAE: 95.1772\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 14614.3037 - MAE: 82.9976 - val_loss: 18582.8359 - val_MAE: 94.7788\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 14512.1611 - MAE: 82.5812 - val_loss: 18461.8398 - val_MAE: 94.3757\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 14406.9629 - MAE: 82.1433 - val_loss: 18343.9395 - val_MAE: 94.0001\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 14307.4189 - MAE: 81.7473 - val_loss: 18224.4160 - val_MAE: 93.6443\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 14203.7217 - MAE: 81.3291 - val_loss: 18104.7441 - val_MAE: 93.3093\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 14103.3369 - MAE: 80.9172 - val_loss: 17986.9336 - val_MAE: 92.9801\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 14003.3262 - MAE: 80.5232 - val_loss: 17872.4219 - val_MAE: 92.6902\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 13905.7354 - MAE: 80.1279 - val_loss: 17759.7031 - val_MAE: 92.4033\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 13808.8818 - MAE: 79.7569 - val_loss: 17648.4141 - val_MAE: 92.1184\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 13713.0996 - MAE: 79.3862 - val_loss: 17538.5898 - val_MAE: 91.8357\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 13623.1768 - MAE: 79.0467 - val_loss: 17425.0703 - val_MAE: 91.5420\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 13527.7441 - MAE: 78.6657 - val_loss: 17313.4551 - val_MAE: 91.2744\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 13432.2812 - MAE: 78.3372 - val_loss: 17205.9980 - val_MAE: 91.0277\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 13342.7100 - MAE: 78.0103 - val_loss: 17097.8301 - val_MAE: 90.7779\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 13249.5127 - MAE: 77.6854 - val_loss: 16991.7988 - val_MAE: 90.5314\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 13158.8652 - MAE: 77.3698 - val_loss: 16885.9395 - val_MAE: 90.2842\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 13070.4238 - MAE: 77.0797 - val_loss: 16780.6387 - val_MAE: 90.0369\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12979.7148 - MAE: 76.7696 - val_loss: 16677.8242 - val_MAE: 89.7940\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12893.4404 - MAE: 76.4869 - val_loss: 16574.2969 - val_MAE: 89.5477\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12808.4609 - MAE: 76.2015 - val_loss: 16470.0293 - val_MAE: 89.2983\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12717.7686 - MAE: 75.9200 - val_loss: 16371.2295 - val_MAE: 89.0609\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 12635.6484 - MAE: 75.6172 - val_loss: 16268.8438 - val_MAE: 88.8132\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12551.4189 - MAE: 75.3473 - val_loss: 16169.4082 - val_MAE: 88.5710\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.5, model__n_hidden=0, model__n_neurons=25, model__optimizer=adam; total time=   5.6s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 8742.3604 - MAE: 89.4930 - val_loss: 8938.5137 - val_MAE: 89.2821\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8617.0215 - MAE: 88.8560 - val_loss: 8812.0791 - val_MAE: 88.6071\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8494.3535 - MAE: 88.2386 - val_loss: 8686.5459 - val_MAE: 87.9311\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8373.6035 - MAE: 87.6218 - val_loss: 8562.6211 - val_MAE: 87.2579\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8252.6953 - MAE: 87.0096 - val_loss: 8441.0117 - val_MAE: 86.5915\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8133.4019 - MAE: 86.3942 - val_loss: 8321.4248 - val_MAE: 85.9303\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8016.3066 - MAE: 85.7776 - val_loss: 8202.9863 - val_MAE: 85.2698\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7900.7466 - MAE: 85.1744 - val_loss: 8086.5635 - val_MAE: 84.6145\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7787.7383 - MAE: 84.5894 - val_loss: 7971.0942 - val_MAE: 83.9589\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7676.0674 - MAE: 84.0133 - val_loss: 7856.8882 - val_MAE: 83.3046\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7564.4536 - MAE: 83.4212 - val_loss: 7745.1807 - val_MAE: 82.6590\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7455.6528 - MAE: 82.8466 - val_loss: 7634.4282 - val_MAE: 82.0551\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7347.4668 - MAE: 82.2710 - val_loss: 7525.2339 - val_MAE: 81.4682\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7240.4570 - MAE: 81.6897 - val_loss: 7417.8281 - val_MAE: 80.8857\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7136.0068 - MAE: 81.1268 - val_loss: 7311.6021 - val_MAE: 80.3041\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7031.5483 - MAE: 80.5523 - val_loss: 7206.7246 - val_MAE: 79.7246\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6929.7510 - MAE: 79.9825 - val_loss: 7102.9536 - val_MAE: 79.1459\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6830.0093 - MAE: 79.4311 - val_loss: 7000.1177 - val_MAE: 78.5671\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6729.7764 - MAE: 78.8590 - val_loss: 6899.9888 - val_MAE: 77.9981\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6630.9561 - MAE: 78.2992 - val_loss: 6801.6099 - val_MAE: 77.4338\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6534.3066 - MAE: 77.7419 - val_loss: 6703.9863 - val_MAE: 76.8686\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6440.1343 - MAE: 77.1968 - val_loss: 6606.6855 - val_MAE: 76.2999\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6345.6455 - MAE: 76.6430 - val_loss: 6510.9653 - val_MAE: 75.7350\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6251.6963 - MAE: 76.0884 - val_loss: 6417.7275 - val_MAE: 75.1981\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6161.3560 - MAE: 75.5509 - val_loss: 6324.9277 - val_MAE: 74.6926\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 6071.0430 - MAE: 75.0026 - val_loss: 6234.0586 - val_MAE: 74.1927\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5982.6704 - MAE: 74.4630 - val_loss: 6144.4678 - val_MAE: 73.6951\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5894.4546 - MAE: 73.9207 - val_loss: 6056.8491 - val_MAE: 73.2040\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5810.1436 - MAE: 73.4057 - val_loss: 5969.0918 - val_MAE: 72.7071\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5725.1665 - MAE: 72.8712 - val_loss: 5883.2134 - val_MAE: 72.2160\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5640.6606 - MAE: 72.3333 - val_loss: 5799.0269 - val_MAE: 71.7295\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5559.1172 - MAE: 71.8089 - val_loss: 5715.2788 - val_MAE: 71.2408\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5477.9082 - MAE: 71.2903 - val_loss: 5633.4189 - val_MAE: 70.7581\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5398.2856 - MAE: 70.7636 - val_loss: 5553.3115 - val_MAE: 70.2808\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5319.0322 - MAE: 70.2389 - val_loss: 5475.3179 - val_MAE: 69.8117\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5242.9111 - MAE: 69.7419 - val_loss: 5397.3013 - val_MAE: 69.3377\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5167.0815 - MAE: 69.2292 - val_loss: 5319.7783 - val_MAE: 68.8620\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 5091.5620 - MAE: 68.7182 - val_loss: 5243.9038 - val_MAE: 68.3916\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5018.0352 - MAE: 68.2159 - val_loss: 5168.8882 - val_MAE: 67.9217\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 4945.2881 - MAE: 67.7071 - val_loss: 5095.5693 - val_MAE: 67.4576\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 4872.3770 - MAE: 67.1970 - val_loss: 5024.7075 - val_MAE: 67.0041\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4803.0752 - MAE: 66.7096 - val_loss: 4953.5962 - val_MAE: 66.5449\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 4733.6665 - MAE: 66.2043 - val_loss: 4883.1230 - val_MAE: 66.0848\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 4664.6362 - MAE: 65.7085 - val_loss: 4814.1519 - val_MAE: 65.6298\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 4596.9854 - MAE: 65.2202 - val_loss: 4746.5806 - val_MAE: 65.1792\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 4532.3628 - MAE: 64.7428 - val_loss: 4678.6777 - val_MAE: 64.7218\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 4466.8799 - MAE: 64.2551 - val_loss: 4612.0566 - val_MAE: 64.2681\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4401.4717 - MAE: 63.7550 - val_loss: 4547.8496 - val_MAE: 63.8261\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4339.3540 - MAE: 63.2862 - val_loss: 4484.3970 - val_MAE: 63.3845\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4277.0942 - MAE: 62.8057 - val_loss: 4422.9883 - val_MAE: 62.9526\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4216.7520 - MAE: 62.3379 - val_loss: 4362.1895 - val_MAE: 62.5204\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 4158.0815 - MAE: 61.8771 - val_loss: 4301.7554 - val_MAE: 62.0862\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4098.6938 - MAE: 61.4053 - val_loss: 4243.3037 - val_MAE: 61.6617\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 4041.3315 - MAE: 60.9439 - val_loss: 4185.2671 - val_MAE: 61.2356\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3985.3767 - MAE: 60.4874 - val_loss: 4127.7554 - val_MAE: 60.8088\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3929.2568 - MAE: 60.0318 - val_loss: 4071.5125 - val_MAE: 60.3869\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3874.5391 - MAE: 59.5751 - val_loss: 4015.8545 - val_MAE: 59.9646\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3820.2837 - MAE: 59.1261 - val_loss: 3961.4097 - val_MAE: 59.5470\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3767.1074 - MAE: 58.6732 - val_loss: 3907.8584 - val_MAE: 59.1317\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3715.1614 - MAE: 58.2348 - val_loss: 3854.6787 - val_MAE: 58.7146\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3663.0618 - MAE: 57.7841 - val_loss: 3802.8328 - val_MAE: 58.3034\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3612.0569 - MAE: 57.3440 - val_loss: 3752.1719 - val_MAE: 57.8972\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3562.4160 - MAE: 56.9062 - val_loss: 3702.3313 - val_MAE: 57.4930\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3514.2034 - MAE: 56.4760 - val_loss: 3652.7100 - val_MAE: 57.0859\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 3466.5496 - MAE: 56.0456 - val_loss: 3603.9221 - val_MAE: 56.6811\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 3418.7068 - MAE: 55.6209 - val_loss: 3557.0066 - val_MAE: 56.2875\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3373.2454 - MAE: 55.2037 - val_loss: 3510.4326 - val_MAE: 55.8923\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3327.4990 - MAE: 54.7899 - val_loss: 3464.8223 - val_MAE: 55.5007\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3283.1882 - MAE: 54.3776 - val_loss: 3419.7886 - val_MAE: 55.1096\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3239.5122 - MAE: 53.9689 - val_loss: 3375.5188 - val_MAE: 54.7208\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3195.7444 - MAE: 53.5610 - val_loss: 3332.4541 - val_MAE: 54.3381\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3153.4836 - MAE: 53.1612 - val_loss: 3290.0723 - val_MAE: 53.9572\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3112.7603 - MAE: 52.7763 - val_loss: 3247.7842 - val_MAE: 53.5957\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3071.6355 - MAE: 52.3836 - val_loss: 3206.5610 - val_MAE: 53.2587\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3031.7432 - MAE: 52.0066 - val_loss: 3166.1741 - val_MAE: 52.9246\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 2992.2996 - MAE: 51.6195 - val_loss: 3126.7678 - val_MAE: 52.5948\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2953.3008 - MAE: 51.2468 - val_loss: 3088.4917 - val_MAE: 52.2705\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2916.0259 - MAE: 50.8813 - val_loss: 3050.5647 - val_MAE: 51.9454\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2879.1436 - MAE: 50.5183 - val_loss: 3013.2087 - val_MAE: 51.6214\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2843.2065 - MAE: 50.1532 - val_loss: 2976.0974 - val_MAE: 51.2956\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2806.3474 - MAE: 49.7884 - val_loss: 2940.6084 - val_MAE: 50.9802\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2772.1714 - MAE: 49.4400 - val_loss: 2905.2354 - val_MAE: 50.6621\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2738.0461 - MAE: 49.0890 - val_loss: 2870.8643 - val_MAE: 50.3493\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2704.9573 - MAE: 48.7459 - val_loss: 2836.9473 - val_MAE: 50.0369\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2672.3582 - MAE: 48.4024 - val_loss: 2803.6565 - val_MAE: 49.7265\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2639.6458 - MAE: 48.0613 - val_loss: 2771.4944 - val_MAE: 49.4230\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2607.6052 - MAE: 47.7122 - val_loss: 2740.1238 - val_MAE: 49.1236\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2576.9707 - MAE: 47.3782 - val_loss: 2708.7734 - val_MAE: 48.8206\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2545.8298 - MAE: 47.0463 - val_loss: 2678.6177 - val_MAE: 48.5257\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2516.6089 - MAE: 46.7223 - val_loss: 2648.0906 - val_MAE: 48.2234\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2487.0544 - MAE: 46.3945 - val_loss: 2618.2283 - val_MAE: 47.9240\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2458.6697 - MAE: 46.0661 - val_loss: 2588.9111 - val_MAE: 47.6264\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2429.5508 - MAE: 45.7398 - val_loss: 2560.5422 - val_MAE: 47.3351\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2403.3843 - MAE: 45.4313 - val_loss: 2531.7854 - val_MAE: 47.0355\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2375.5266 - MAE: 45.1067 - val_loss: 2504.6479 - val_MAE: 46.7496\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2348.4170 - MAE: 44.7873 - val_loss: 2478.5251 - val_MAE: 46.4714\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2323.0134 - MAE: 44.4848 - val_loss: 2452.2417 - val_MAE: 46.1878\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2297.8376 - MAE: 44.1775 - val_loss: 2426.4314 - val_MAE: 45.9056\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2272.8782 - MAE: 43.8806 - val_loss: 2401.4644 - val_MAE: 45.6295\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2248.4387 - MAE: 43.5856 - val_loss: 2376.7793 - val_MAE: 45.3530\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.5, model__n_hidden=0, model__n_neurons=25, model__optimizer=adam; total time=   5.7s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 57479.3320 - MAE: 225.3856 - val_loss: 58975.1055 - val_MAE: 230.3616\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 57133.8789 - MAE: 224.6680 - val_loss: 58613.9258 - val_MAE: 229.6225\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 56800.6797 - MAE: 223.9567 - val_loss: 58250.8320 - val_MAE: 228.8770\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 56462.0469 - MAE: 223.2388 - val_loss: 57891.0898 - val_MAE: 228.1362\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 56128.7930 - MAE: 222.5241 - val_loss: 57531.5234 - val_MAE: 227.3934\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 55789.4844 - MAE: 221.8091 - val_loss: 57177.1289 - val_MAE: 226.6588\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 55456.2305 - MAE: 221.1014 - val_loss: 56824.3281 - val_MAE: 225.9251\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 55128.6641 - MAE: 220.3911 - val_loss: 56470.5547 - val_MAE: 225.1874\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 54795.3008 - MAE: 219.6817 - val_loss: 56121.2969 - val_MAE: 224.4563\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 54469.9297 - MAE: 218.9747 - val_loss: 55770.1484 - val_MAE: 223.7194\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 54140.6484 - MAE: 218.2674 - val_loss: 55423.2969 - val_MAE: 222.9889\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 53816.7266 - MAE: 217.5615 - val_loss: 55077.5000 - val_MAE: 222.2583\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 53497.8867 - MAE: 216.8576 - val_loss: 54729.2852 - val_MAE: 221.5206\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 53171.5508 - MAE: 216.1481 - val_loss: 54385.6836 - val_MAE: 220.7903\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 52849.6484 - MAE: 215.4456 - val_loss: 54045.1406 - val_MAE: 220.0638\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 52529.4375 - MAE: 214.7452 - val_loss: 53708.5547 - val_MAE: 219.3432\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 52212.6445 - MAE: 214.0461 - val_loss: 53371.9102 - val_MAE: 218.6205\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 51899.7969 - MAE: 213.3503 - val_loss: 53033.5000 - val_MAE: 217.8918\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 51584.3711 - MAE: 212.6492 - val_loss: 52696.0469 - val_MAE: 217.1629\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 51269.8867 - MAE: 211.9451 - val_loss: 52360.3789 - val_MAE: 216.4359\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 50955.5195 - MAE: 211.2486 - val_loss: 52029.2734 - val_MAE: 215.7156\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 50647.2188 - MAE: 210.5523 - val_loss: 51698.6445 - val_MAE: 214.9946\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 50336.5391 - MAE: 209.8586 - val_loss: 51371.9766 - val_MAE: 214.2797\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 50032.4297 - MAE: 209.1695 - val_loss: 51045.7852 - val_MAE: 213.5632\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 49725.5859 - MAE: 208.4789 - val_loss: 50723.2695 - val_MAE: 212.8524\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 49421.5273 - MAE: 207.7935 - val_loss: 50403.6523 - val_MAE: 212.1454\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 49123.5312 - MAE: 207.1082 - val_loss: 50081.0820 - val_MAE: 211.4303\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 48825.5312 - MAE: 206.4209 - val_loss: 49758.0156 - val_MAE: 210.7115\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 48521.3398 - MAE: 205.7315 - val_loss: 49443.3672 - val_MAE: 210.0087\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 48224.5820 - MAE: 205.0515 - val_loss: 49129.0156 - val_MAE: 209.3047\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 47932.2148 - MAE: 204.3728 - val_loss: 48812.1523 - val_MAE: 208.5926\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 47638.5508 - MAE: 203.6876 - val_loss: 48497.0742 - val_MAE: 207.8824\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 47342.5703 - MAE: 203.0031 - val_loss: 48185.4180 - val_MAE: 207.1773\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 47051.2031 - MAE: 202.3218 - val_loss: 47874.7656 - val_MAE: 206.4725\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 46760.9961 - MAE: 201.6449 - val_loss: 47566.6328 - val_MAE: 205.7710\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 46469.8203 - MAE: 200.9678 - val_loss: 47261.8242 - val_MAE: 205.0741\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 46180.6914 - MAE: 200.2915 - val_loss: 46957.1250 - val_MAE: 204.3757\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 45895.9688 - MAE: 199.6168 - val_loss: 46650.0898 - val_MAE: 203.6697\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 45613.1719 - MAE: 198.9375 - val_loss: 46341.5312 - val_MAE: 202.9579\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 45325.8633 - MAE: 198.2542 - val_loss: 46037.9336 - val_MAE: 202.2554\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 45039.1367 - MAE: 197.5782 - val_loss: 45739.9727 - val_MAE: 201.5631\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 44757.7578 - MAE: 196.9093 - val_loss: 45443.3398 - val_MAE: 200.8718\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 44482.4180 - MAE: 196.2395 - val_loss: 45143.7539 - val_MAE: 200.1715\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 44201.4414 - MAE: 195.5656 - val_loss: 44847.8320 - val_MAE: 199.4770\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 43925.9414 - MAE: 194.8963 - val_loss: 44552.3281 - val_MAE: 198.7813\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 43645.8555 - MAE: 194.2264 - val_loss: 44261.7227 - val_MAE: 198.0944\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 43375.4883 - MAE: 193.5605 - val_loss: 43967.9766 - val_MAE: 197.3982\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 43101.6406 - MAE: 192.8916 - val_loss: 43676.1523 - val_MAE: 196.7044\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 42832.4336 - MAE: 192.2247 - val_loss: 43384.4727 - val_MAE: 196.0081\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 42554.2578 - MAE: 191.5559 - val_loss: 43100.0039 - val_MAE: 195.3266\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 42288.0664 - MAE: 190.8951 - val_loss: 42813.9922 - val_MAE: 194.6393\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 42018.2930 - MAE: 190.2319 - val_loss: 42529.1914 - val_MAE: 193.9527\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 41755.4961 - MAE: 189.5717 - val_loss: 42242.8789 - val_MAE: 193.2600\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 41488.2969 - MAE: 188.9055 - val_loss: 41961.1055 - val_MAE: 192.5763\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 41223.0117 - MAE: 188.2467 - val_loss: 41683.3164 - val_MAE: 191.8998\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 40964.5664 - MAE: 187.5916 - val_loss: 41403.6992 - val_MAE: 191.2165\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 40702.5430 - MAE: 186.9344 - val_loss: 41126.1836 - val_MAE: 190.5357\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 40443.1055 - MAE: 186.2775 - val_loss: 40850.5977 - val_MAE: 189.8570\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 40183.9102 - MAE: 185.6254 - val_loss: 40578.9492 - val_MAE: 189.1859\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 39929.4609 - MAE: 184.9774 - val_loss: 40306.6016 - val_MAE: 188.5106\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 39678.5664 - MAE: 184.3257 - val_loss: 40032.6836 - val_MAE: 187.8295\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 39419.2109 - MAE: 183.6726 - val_loss: 39766.5703 - val_MAE: 187.1646\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 39167.4141 - MAE: 183.0295 - val_loss: 39500.6289 - val_MAE: 186.4983\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 38921.6914 - MAE: 182.3849 - val_loss: 39229.7461 - val_MAE: 185.8178\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 38663.8945 - MAE: 181.7295 - val_loss: 38966.1602 - val_MAE: 185.1530\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 38417.7852 - MAE: 181.0871 - val_loss: 38700.9180 - val_MAE: 184.4818\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 38169.7383 - MAE: 180.4411 - val_loss: 38437.8281 - val_MAE: 183.8136\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 37923.5273 - MAE: 179.7965 - val_loss: 38175.2656 - val_MAE: 183.1445\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 37676.6406 - MAE: 179.1516 - val_loss: 37914.0898 - val_MAE: 182.4766\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 37433.9570 - MAE: 178.5084 - val_loss: 37655.0430 - val_MAE: 181.8115\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 37192.5352 - MAE: 177.8683 - val_loss: 37396.6172 - val_MAE: 181.1459\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 36949.0117 - MAE: 177.2277 - val_loss: 37142.5117 - val_MAE: 180.4888\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 36711.3711 - MAE: 176.5946 - val_loss: 36888.0664 - val_MAE: 179.8285\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 36470.1992 - MAE: 175.9549 - val_loss: 36634.5117 - val_MAE: 179.1685\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 36234.3438 - MAE: 175.3168 - val_loss: 36379.4727 - val_MAE: 178.5025\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 35994.6719 - MAE: 174.6786 - val_loss: 36128.7422 - val_MAE: 177.8445\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 35758.3789 - MAE: 174.0429 - val_loss: 35879.3867 - val_MAE: 177.1884\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 35524.7969 - MAE: 173.4097 - val_loss: 35630.7422 - val_MAE: 176.5318\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 35291.8125 - MAE: 172.7772 - val_loss: 35383.9883 - val_MAE: 175.8777\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 35061.8008 - MAE: 172.1449 - val_loss: 35135.4062 - val_MAE: 175.2169\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 34832.0430 - MAE: 171.5085 - val_loss: 34887.0938 - val_MAE: 174.5546\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 34597.6953 - MAE: 170.8735 - val_loss: 34645.1797 - val_MAE: 173.9058\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 34370.0742 - MAE: 170.2483 - val_loss: 34404.9102 - val_MAE: 173.2596\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 34144.9492 - MAE: 169.6239 - val_loss: 34166.0547 - val_MAE: 172.6149\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 33917.2344 - MAE: 169.0015 - val_loss: 33929.8789 - val_MAE: 171.9746\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 33695.8984 - MAE: 168.3810 - val_loss: 33691.6406 - val_MAE: 171.3268\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 33474.5859 - MAE: 167.7570 - val_loss: 33452.1758 - val_MAE: 170.6732\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 33249.3789 - MAE: 167.1287 - val_loss: 33216.3164 - val_MAE: 170.0274\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 33028.0234 - MAE: 166.5047 - val_loss: 32981.8828 - val_MAE: 169.3833\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 32806.7188 - MAE: 165.8835 - val_loss: 32748.2207 - val_MAE: 168.7389\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 32588.8379 - MAE: 165.2607 - val_loss: 32513.2754 - val_MAE: 168.0892\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 32370.8027 - MAE: 164.6361 - val_loss: 32281.5371 - val_MAE: 167.4454\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 32154.4785 - MAE: 164.0171 - val_loss: 32052.0527 - val_MAE: 166.8051\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 31935.3223 - MAE: 163.3995 - val_loss: 31827.0859 - val_MAE: 166.1747\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 31726.0918 - MAE: 162.7878 - val_loss: 31596.9023 - val_MAE: 165.5281\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 31508.4551 - MAE: 162.1680 - val_loss: 31371.4766 - val_MAE: 164.8919\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 31300.8965 - MAE: 161.5533 - val_loss: 31143.7754 - val_MAE: 164.2481\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 31084.1152 - MAE: 160.9351 - val_loss: 30922.7734 - val_MAE: 163.6197\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 30878.2832 - MAE: 160.3297 - val_loss: 30700.3770 - val_MAE: 162.9852\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 30667.6504 - MAE: 159.7185 - val_loss: 30482.5469 - val_MAE: 162.3609\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.5, model__n_hidden=0, model__n_neurons=25, model__optimizer=adam; total time=   5.7s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 21ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.1, model__n_hidden=3, model__n_neurons=125, model__optimizer=momentum; total time=   1.8s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 23ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.1, model__n_hidden=3, model__n_neurons=125, model__optimizer=momentum; total time=   1.2s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 21ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.1, model__n_hidden=3, model__n_neurons=125, model__optimizer=momentum; total time=   1.5s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 23ms/step - loss: 308.1538 - MAE: 14.0842 - val_loss: 318.6310 - val_MAE: 15.0098\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 307.3532 - MAE: 14.0675 - val_loss: 317.6241 - val_MAE: 14.9866\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 306.5305 - MAE: 14.0513 - val_loss: 316.6253 - val_MAE: 14.9630\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 305.7691 - MAE: 14.0361 - val_loss: 315.6035 - val_MAE: 14.9386\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 305.0114 - MAE: 14.0212 - val_loss: 314.6057 - val_MAE: 14.9144\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 304.2092 - MAE: 14.0046 - val_loss: 313.6371 - val_MAE: 14.8907\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 303.4708 - MAE: 13.9903 - val_loss: 312.6588 - val_MAE: 14.8672\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 302.7462 - MAE: 13.9743 - val_loss: 311.6754 - val_MAE: 14.8442\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 301.9585 - MAE: 13.9587 - val_loss: 310.7256 - val_MAE: 14.8213\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 301.2018 - MAE: 13.9429 - val_loss: 309.8112 - val_MAE: 14.7988\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 300.5374 - MAE: 13.9287 - val_loss: 308.8640 - val_MAE: 14.7764\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 299.7924 - MAE: 13.9132 - val_loss: 307.9554 - val_MAE: 14.7542\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 299.0805 - MAE: 13.8986 - val_loss: 307.0409 - val_MAE: 14.7320\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 298.4225 - MAE: 13.8847 - val_loss: 306.1041 - val_MAE: 14.7088\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 297.6723 - MAE: 13.8688 - val_loss: 305.2355 - val_MAE: 14.6874\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 296.9959 - MAE: 13.8552 - val_loss: 304.3370 - val_MAE: 14.6649\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 296.3279 - MAE: 13.8401 - val_loss: 303.4058 - val_MAE: 14.6426\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 295.6503 - MAE: 13.8258 - val_loss: 302.4693 - val_MAE: 14.6194\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 294.9411 - MAE: 13.8107 - val_loss: 301.5928 - val_MAE: 14.5980\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 294.2702 - MAE: 13.7960 - val_loss: 300.7032 - val_MAE: 14.5759\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 293.5810 - MAE: 13.7812 - val_loss: 299.8443 - val_MAE: 14.5544\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 292.9141 - MAE: 13.7669 - val_loss: 298.9546 - val_MAE: 14.5316\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 292.2620 - MAE: 13.7522 - val_loss: 298.0372 - val_MAE: 14.5084\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 291.5663 - MAE: 13.7371 - val_loss: 297.1529 - val_MAE: 14.4864\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 290.8957 - MAE: 13.7229 - val_loss: 296.2549 - val_MAE: 14.4626\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 290.2091 - MAE: 13.7077 - val_loss: 295.3955 - val_MAE: 14.4405\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 289.5800 - MAE: 13.6941 - val_loss: 294.4987 - val_MAE: 14.4173\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 288.8975 - MAE: 13.6788 - val_loss: 293.6391 - val_MAE: 14.3948\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 288.2534 - MAE: 13.6651 - val_loss: 292.7886 - val_MAE: 14.3724\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 287.5787 - MAE: 13.6507 - val_loss: 291.9558 - val_MAE: 14.3504\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 286.9691 - MAE: 13.6373 - val_loss: 291.1028 - val_MAE: 14.3281\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 286.3153 - MAE: 13.6225 - val_loss: 290.2765 - val_MAE: 14.3067\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 285.6931 - MAE: 13.6085 - val_loss: 289.4341 - val_MAE: 14.2840\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 285.0551 - MAE: 13.5949 - val_loss: 288.6171 - val_MAE: 14.2621\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 284.4558 - MAE: 13.5811 - val_loss: 287.7969 - val_MAE: 14.2402\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 283.7946 - MAE: 13.5666 - val_loss: 287.0117 - val_MAE: 14.2182\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 283.2153 - MAE: 13.5542 - val_loss: 286.1937 - val_MAE: 14.1970\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 282.5842 - MAE: 13.5397 - val_loss: 285.3534 - val_MAE: 14.1741\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 281.9857 - MAE: 13.5264 - val_loss: 284.5207 - val_MAE: 14.1516\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 281.3608 - MAE: 13.5127 - val_loss: 283.7321 - val_MAE: 14.1307\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 280.7435 - MAE: 13.4984 - val_loss: 282.9495 - val_MAE: 14.1094\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 280.1533 - MAE: 13.4853 - val_loss: 282.1727 - val_MAE: 14.0882\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 279.5495 - MAE: 13.4722 - val_loss: 281.4113 - val_MAE: 14.0679\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 279.0104 - MAE: 13.4603 - val_loss: 280.5981 - val_MAE: 14.0464\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 278.3795 - MAE: 13.4455 - val_loss: 279.8356 - val_MAE: 14.0262\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 277.8135 - MAE: 13.4336 - val_loss: 279.0787 - val_MAE: 14.0059\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 277.2315 - MAE: 13.4206 - val_loss: 278.3363 - val_MAE: 13.9864\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 276.6425 - MAE: 13.4077 - val_loss: 277.5761 - val_MAE: 13.9660\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 276.0652 - MAE: 13.3946 - val_loss: 276.8239 - val_MAE: 13.9455\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 275.4861 - MAE: 13.3814 - val_loss: 276.0705 - val_MAE: 13.9250\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 274.9014 - MAE: 13.3684 - val_loss: 275.3403 - val_MAE: 13.9051\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 274.3305 - MAE: 13.3555 - val_loss: 274.5704 - val_MAE: 13.8836\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 273.7246 - MAE: 13.3421 - val_loss: 273.8306 - val_MAE: 13.8635\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 273.1202 - MAE: 13.3283 - val_loss: 273.0832 - val_MAE: 13.8421\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 272.5570 - MAE: 13.3150 - val_loss: 272.3140 - val_MAE: 13.8212\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 271.9276 - MAE: 13.3010 - val_loss: 271.5657 - val_MAE: 13.8000\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 271.3707 - MAE: 13.2882 - val_loss: 270.7863 - val_MAE: 13.7771\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 270.7923 - MAE: 13.2746 - val_loss: 269.9912 - val_MAE: 13.7545\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 270.1458 - MAE: 13.2599 - val_loss: 269.2643 - val_MAE: 13.7332\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 269.5970 - MAE: 13.2477 - val_loss: 268.4925 - val_MAE: 13.7105\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 268.9952 - MAE: 13.2331 - val_loss: 267.7135 - val_MAE: 13.6871\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 268.3695 - MAE: 13.2190 - val_loss: 266.9778 - val_MAE: 13.6647\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 267.8275 - MAE: 13.2056 - val_loss: 266.1927 - val_MAE: 13.6404\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 267.2205 - MAE: 13.1921 - val_loss: 265.4337 - val_MAE: 13.6176\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 266.6627 - MAE: 13.1789 - val_loss: 264.6675 - val_MAE: 13.5939\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 266.0930 - MAE: 13.1659 - val_loss: 263.8816 - val_MAE: 13.5700\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 265.4962 - MAE: 13.1515 - val_loss: 263.1236 - val_MAE: 13.5470\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 264.9297 - MAE: 13.1380 - val_loss: 262.3621 - val_MAE: 13.5234\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 264.3456 - MAE: 13.1242 - val_loss: 261.6308 - val_MAE: 13.4998\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 263.7945 - MAE: 13.1109 - val_loss: 260.9065 - val_MAE: 13.4777\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 263.2357 - MAE: 13.0975 - val_loss: 260.1903 - val_MAE: 13.4555\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 262.7113 - MAE: 13.0846 - val_loss: 259.4649 - val_MAE: 13.4337\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 262.1251 - MAE: 13.0707 - val_loss: 258.8018 - val_MAE: 13.4132\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 261.6246 - MAE: 13.0577 - val_loss: 258.1056 - val_MAE: 13.3923\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 261.0871 - MAE: 13.0452 - val_loss: 257.4199 - val_MAE: 13.3710\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 260.5280 - MAE: 13.0317 - val_loss: 256.7642 - val_MAE: 13.3514\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 260.0141 - MAE: 13.0184 - val_loss: 256.0829 - val_MAE: 13.3306\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 259.4687 - MAE: 13.0051 - val_loss: 255.4308 - val_MAE: 13.3098\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 258.9763 - MAE: 12.9928 - val_loss: 254.7663 - val_MAE: 13.2893\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 258.4510 - MAE: 12.9797 - val_loss: 254.1181 - val_MAE: 13.2694\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 257.9288 - MAE: 12.9667 - val_loss: 253.4879 - val_MAE: 13.2502\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 257.4457 - MAE: 12.9546 - val_loss: 252.8160 - val_MAE: 13.2294\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 256.9290 - MAE: 12.9415 - val_loss: 252.1849 - val_MAE: 13.2098\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 256.4556 - MAE: 12.9292 - val_loss: 251.5395 - val_MAE: 13.1892\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 255.9715 - MAE: 12.9165 - val_loss: 250.9305 - val_MAE: 13.1705\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 255.4947 - MAE: 12.9050 - val_loss: 250.3083 - val_MAE: 13.1504\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 255.0173 - MAE: 12.8924 - val_loss: 249.7228 - val_MAE: 13.1322\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 254.5821 - MAE: 12.8818 - val_loss: 249.0801 - val_MAE: 13.1114\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 254.1431 - MAE: 12.8698 - val_loss: 248.4235 - val_MAE: 13.0901\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 253.6127 - MAE: 12.8566 - val_loss: 247.8329 - val_MAE: 13.0708\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 253.1714 - MAE: 12.8453 - val_loss: 247.2428 - val_MAE: 13.0524\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 252.7076 - MAE: 12.8333 - val_loss: 246.6341 - val_MAE: 13.0327\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 252.2587 - MAE: 12.8222 - val_loss: 246.0211 - val_MAE: 13.0131\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 251.8094 - MAE: 12.8100 - val_loss: 245.4097 - val_MAE: 12.9930\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 251.3304 - MAE: 12.7981 - val_loss: 244.8369 - val_MAE: 12.9744\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 250.8974 - MAE: 12.7865 - val_loss: 244.2292 - val_MAE: 12.9547\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 250.4610 - MAE: 12.7749 - val_loss: 243.6251 - val_MAE: 12.9353\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 249.9833 - MAE: 12.7626 - val_loss: 243.0477 - val_MAE: 12.9153\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 249.5599 - MAE: 12.7514 - val_loss: 242.4414 - val_MAE: 12.8955\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 249.1225 - MAE: 12.7401 - val_loss: 241.8599 - val_MAE: 12.8769\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=3, model__n_neurons=5, model__optimizer=adam; total time=   6.0s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 22ms/step - loss: 257.4968 - MAE: 12.7564 - val_loss: 103.8573 - val_MAE: 9.1454\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 256.8885 - MAE: 12.7366 - val_loss: 103.3879 - val_MAE: 9.1236\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 256.2648 - MAE: 12.7162 - val_loss: 102.9225 - val_MAE: 9.1019\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 255.6745 - MAE: 12.6965 - val_loss: 102.4491 - val_MAE: 9.0797\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 255.0597 - MAE: 12.6771 - val_loss: 101.9826 - val_MAE: 9.0579\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 254.4769 - MAE: 12.6573 - val_loss: 101.5197 - val_MAE: 9.0359\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 253.8864 - MAE: 12.6381 - val_loss: 101.0688 - val_MAE: 9.0144\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 253.2893 - MAE: 12.6181 - val_loss: 100.6371 - val_MAE: 8.9938\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 252.7386 - MAE: 12.5999 - val_loss: 100.1947 - val_MAE: 8.9724\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 252.1469 - MAE: 12.5802 - val_loss: 99.7552 - val_MAE: 8.9509\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 251.5738 - MAE: 12.5611 - val_loss: 99.3122 - val_MAE: 8.9294\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 250.9850 - MAE: 12.5420 - val_loss: 98.8777 - val_MAE: 8.9081\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 250.4147 - MAE: 12.5224 - val_loss: 98.4403 - val_MAE: 8.8865\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 249.8299 - MAE: 12.5019 - val_loss: 98.0041 - val_MAE: 8.8645\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 249.2330 - MAE: 12.4824 - val_loss: 97.5717 - val_MAE: 8.8428\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 248.6811 - MAE: 12.4630 - val_loss: 97.1255 - val_MAE: 8.8230\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 248.0517 - MAE: 12.4428 - val_loss: 96.7071 - val_MAE: 8.8047\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 247.4755 - MAE: 12.4226 - val_loss: 96.2800 - val_MAE: 8.7861\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 246.8857 - MAE: 12.4027 - val_loss: 95.8569 - val_MAE: 8.7677\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 246.3269 - MAE: 12.3835 - val_loss: 95.4227 - val_MAE: 8.7487\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 245.7405 - MAE: 12.3633 - val_loss: 95.0012 - val_MAE: 8.7296\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 245.1238 - MAE: 12.3431 - val_loss: 94.5863 - val_MAE: 8.7113\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 244.5500 - MAE: 12.3248 - val_loss: 94.1709 - val_MAE: 8.6931\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 243.9627 - MAE: 12.3052 - val_loss: 93.7598 - val_MAE: 8.6747\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 243.3672 - MAE: 12.2865 - val_loss: 93.3502 - val_MAE: 8.6565\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 242.8034 - MAE: 12.2677 - val_loss: 92.9351 - val_MAE: 8.6378\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 242.2287 - MAE: 12.2501 - val_loss: 92.5270 - val_MAE: 8.6190\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 241.6443 - MAE: 12.2300 - val_loss: 92.1354 - val_MAE: 8.6013\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 241.0856 - MAE: 12.2129 - val_loss: 91.7417 - val_MAE: 8.5833\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 240.5231 - MAE: 12.1946 - val_loss: 91.3523 - val_MAE: 8.5645\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 239.9709 - MAE: 12.1771 - val_loss: 90.9634 - val_MAE: 8.5455\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 239.4129 - MAE: 12.1594 - val_loss: 90.5822 - val_MAE: 8.5269\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 238.8510 - MAE: 12.1415 - val_loss: 90.2015 - val_MAE: 8.5083\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 238.3067 - MAE: 12.1240 - val_loss: 89.8304 - val_MAE: 8.4902\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 237.7408 - MAE: 12.1052 - val_loss: 89.4809 - val_MAE: 8.4728\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 237.2213 - MAE: 12.0880 - val_loss: 89.1158 - val_MAE: 8.4547\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 236.6453 - MAE: 12.0695 - val_loss: 88.7663 - val_MAE: 8.4376\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 236.1179 - MAE: 12.0522 - val_loss: 88.4151 - val_MAE: 8.4203\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 235.5790 - MAE: 12.0341 - val_loss: 88.0732 - val_MAE: 8.4033\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 235.0482 - MAE: 12.0169 - val_loss: 87.7195 - val_MAE: 8.3863\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 234.5517 - MAE: 12.0002 - val_loss: 87.3703 - val_MAE: 8.3683\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 234.0008 - MAE: 11.9811 - val_loss: 87.0273 - val_MAE: 8.3511\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 233.4711 - MAE: 11.9638 - val_loss: 86.6883 - val_MAE: 8.3342\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 232.9514 - MAE: 11.9457 - val_loss: 86.3554 - val_MAE: 8.3176\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 232.4835 - MAE: 11.9297 - val_loss: 86.0182 - val_MAE: 8.2999\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 231.9261 - MAE: 11.9112 - val_loss: 85.6885 - val_MAE: 8.2835\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 231.4227 - MAE: 11.8949 - val_loss: 85.3706 - val_MAE: 8.2672\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 230.9267 - MAE: 11.8783 - val_loss: 85.0566 - val_MAE: 8.2506\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 230.4102 - MAE: 11.8612 - val_loss: 84.7302 - val_MAE: 8.2342\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 229.9160 - MAE: 11.8452 - val_loss: 84.4128 - val_MAE: 8.2170\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 229.4127 - MAE: 11.8287 - val_loss: 84.0966 - val_MAE: 8.2004\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 228.9255 - MAE: 11.8119 - val_loss: 83.7712 - val_MAE: 8.1838\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 228.4272 - MAE: 11.7953 - val_loss: 83.4548 - val_MAE: 8.1675\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 227.9376 - MAE: 11.7789 - val_loss: 83.1510 - val_MAE: 8.1512\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 227.4387 - MAE: 11.7623 - val_loss: 82.8552 - val_MAE: 8.1356\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 226.9525 - MAE: 11.7456 - val_loss: 82.5580 - val_MAE: 8.1197\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 226.4808 - MAE: 11.7292 - val_loss: 82.2421 - val_MAE: 8.1035\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 225.9879 - MAE: 11.7122 - val_loss: 81.9412 - val_MAE: 8.0873\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 225.4957 - MAE: 11.6962 - val_loss: 81.6547 - val_MAE: 8.0711\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 225.0253 - MAE: 11.6801 - val_loss: 81.3489 - val_MAE: 8.0547\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 224.5420 - MAE: 11.6632 - val_loss: 81.0483 - val_MAE: 8.0380\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 224.0502 - MAE: 11.6465 - val_loss: 80.7559 - val_MAE: 8.0217\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 223.5743 - MAE: 11.6310 - val_loss: 80.4574 - val_MAE: 8.0053\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 223.0862 - MAE: 11.6149 - val_loss: 80.1637 - val_MAE: 7.9894\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 222.6175 - MAE: 11.5991 - val_loss: 79.8631 - val_MAE: 7.9738\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 222.1419 - MAE: 11.5825 - val_loss: 79.5737 - val_MAE: 7.9582\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 221.6969 - MAE: 11.5675 - val_loss: 79.2791 - val_MAE: 7.9416\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 221.2018 - MAE: 11.5510 - val_loss: 78.9998 - val_MAE: 7.9259\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 220.7609 - MAE: 11.5353 - val_loss: 78.6969 - val_MAE: 7.9095\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 220.2630 - MAE: 11.5192 - val_loss: 78.4158 - val_MAE: 7.8936\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 219.8241 - MAE: 11.5042 - val_loss: 78.1215 - val_MAE: 7.8771\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 219.3253 - MAE: 11.4877 - val_loss: 77.8457 - val_MAE: 7.8617\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 218.8850 - MAE: 11.4728 - val_loss: 77.5610 - val_MAE: 7.8456\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 218.4243 - MAE: 11.4575 - val_loss: 77.2802 - val_MAE: 7.8297\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 217.9646 - MAE: 11.4413 - val_loss: 77.0044 - val_MAE: 7.8167\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 217.4970 - MAE: 11.4261 - val_loss: 76.7290 - val_MAE: 7.8044\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 217.0624 - MAE: 11.4104 - val_loss: 76.4527 - val_MAE: 7.7920\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 216.6341 - MAE: 11.3952 - val_loss: 76.1767 - val_MAE: 7.7794\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 216.1533 - MAE: 11.3786 - val_loss: 75.8990 - val_MAE: 7.7668\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 215.7021 - MAE: 11.3631 - val_loss: 75.6334 - val_MAE: 7.7546\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 215.2443 - MAE: 11.3481 - val_loss: 75.3746 - val_MAE: 7.7428\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 214.8034 - MAE: 11.3329 - val_loss: 75.1065 - val_MAE: 7.7304\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 214.3517 - MAE: 11.3175 - val_loss: 74.8530 - val_MAE: 7.7186\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 213.9233 - MAE: 11.3021 - val_loss: 74.5754 - val_MAE: 7.7056\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 213.4438 - MAE: 11.2867 - val_loss: 74.3112 - val_MAE: 7.6932\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 212.9785 - MAE: 11.2705 - val_loss: 74.0526 - val_MAE: 7.6811\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 212.5385 - MAE: 11.2555 - val_loss: 73.7966 - val_MAE: 7.6690\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 212.0768 - MAE: 11.2408 - val_loss: 73.5459 - val_MAE: 7.6569\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 211.6678 - MAE: 11.2255 - val_loss: 73.2703 - val_MAE: 7.6426\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 211.1825 - MAE: 11.2093 - val_loss: 73.0109 - val_MAE: 7.6290\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 210.7216 - MAE: 11.1941 - val_loss: 72.7589 - val_MAE: 7.6161\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 210.3119 - MAE: 11.1803 - val_loss: 72.4968 - val_MAE: 7.6023\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 209.8427 - MAE: 11.1649 - val_loss: 72.2383 - val_MAE: 7.5889\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 209.4083 - MAE: 11.1498 - val_loss: 71.9696 - val_MAE: 7.5750\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 208.9893 - MAE: 11.1353 - val_loss: 71.7028 - val_MAE: 7.5610\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 208.5027 - MAE: 11.1196 - val_loss: 71.4519 - val_MAE: 7.5479\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 208.0813 - MAE: 11.1050 - val_loss: 71.1874 - val_MAE: 7.5340\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 207.6324 - MAE: 11.0898 - val_loss: 70.9364 - val_MAE: 7.5205\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 207.1964 - MAE: 11.0753 - val_loss: 70.6734 - val_MAE: 7.5066\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 206.7586 - MAE: 11.0602 - val_loss: 70.4071 - val_MAE: 7.4928\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=3, model__n_neurons=5, model__optimizer=adam; total time=   6.1s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 22ms/step - loss: 1403.0488 - MAE: 35.5867 - val_loss: 1388.6388 - val_MAE: 36.0088\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1400.0385 - MAE: 35.5474 - val_loss: 1385.6473 - val_MAE: 35.9678\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1397.1044 - MAE: 35.5085 - val_loss: 1382.6375 - val_MAE: 35.9265\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1394.1348 - MAE: 35.4691 - val_loss: 1379.6306 - val_MAE: 35.8851\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1391.2087 - MAE: 35.4301 - val_loss: 1376.6276 - val_MAE: 35.8438\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1388.2804 - MAE: 35.3911 - val_loss: 1373.6431 - val_MAE: 35.8027\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1385.3624 - MAE: 35.3527 - val_loss: 1370.6622 - val_MAE: 35.7616\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1382.4330 - MAE: 35.3133 - val_loss: 1367.6837 - val_MAE: 35.7204\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1379.5513 - MAE: 35.2752 - val_loss: 1364.7178 - val_MAE: 35.6795\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1376.6519 - MAE: 35.2365 - val_loss: 1361.7737 - val_MAE: 35.6387\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1373.8021 - MAE: 35.1985 - val_loss: 1358.8164 - val_MAE: 35.5977\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1370.9203 - MAE: 35.1599 - val_loss: 1355.8799 - val_MAE: 35.5569\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1368.0504 - MAE: 35.1219 - val_loss: 1352.9678 - val_MAE: 35.5164\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1365.2026 - MAE: 35.0836 - val_loss: 1350.0529 - val_MAE: 35.4758\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1362.3716 - MAE: 35.0454 - val_loss: 1347.1328 - val_MAE: 35.4352\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1359.5282 - MAE: 35.0072 - val_loss: 1344.2290 - val_MAE: 35.3947\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1356.7263 - MAE: 34.9694 - val_loss: 1341.3452 - val_MAE: 35.3545\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1353.9160 - MAE: 34.9320 - val_loss: 1338.4211 - val_MAE: 35.3135\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1351.1206 - MAE: 34.8946 - val_loss: 1335.5232 - val_MAE: 35.2729\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1348.3408 - MAE: 34.8567 - val_loss: 1332.6580 - val_MAE: 35.2328\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1345.6182 - MAE: 34.8202 - val_loss: 1329.7523 - val_MAE: 35.1920\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1342.8246 - MAE: 34.7825 - val_loss: 1326.8989 - val_MAE: 35.1519\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1340.0690 - MAE: 34.7455 - val_loss: 1324.0641 - val_MAE: 35.1120\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1337.3782 - MAE: 34.7089 - val_loss: 1321.1982 - val_MAE: 35.0716\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1334.6696 - MAE: 34.6719 - val_loss: 1318.3385 - val_MAE: 35.0314\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1331.9102 - MAE: 34.6348 - val_loss: 1315.5177 - val_MAE: 34.9916\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1329.2057 - MAE: 34.5980 - val_loss: 1312.6935 - val_MAE: 34.9516\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1326.5513 - MAE: 34.5617 - val_loss: 1309.8490 - val_MAE: 34.9113\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1323.8112 - MAE: 34.5245 - val_loss: 1307.0494 - val_MAE: 34.8716\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1321.1410 - MAE: 34.4880 - val_loss: 1304.2567 - val_MAE: 34.8320\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1318.4849 - MAE: 34.4520 - val_loss: 1301.4530 - val_MAE: 34.7921\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1315.8351 - MAE: 34.4161 - val_loss: 1298.6708 - val_MAE: 34.7526\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1313.1888 - MAE: 34.3797 - val_loss: 1295.8881 - val_MAE: 34.7130\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1310.5135 - MAE: 34.3433 - val_loss: 1293.1594 - val_MAE: 34.6742\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1307.8866 - MAE: 34.3078 - val_loss: 1290.4266 - val_MAE: 34.6351\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1305.3149 - MAE: 34.2727 - val_loss: 1287.6688 - val_MAE: 34.5958\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1302.6855 - MAE: 34.2362 - val_loss: 1284.9377 - val_MAE: 34.5567\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1300.1061 - MAE: 34.2010 - val_loss: 1282.2013 - val_MAE: 34.5174\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1297.5126 - MAE: 34.1656 - val_loss: 1279.5135 - val_MAE: 34.4787\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1294.9244 - MAE: 34.1301 - val_loss: 1276.8602 - val_MAE: 34.4405\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1292.3461 - MAE: 34.0947 - val_loss: 1274.2361 - val_MAE: 34.4028\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1289.7794 - MAE: 34.0595 - val_loss: 1271.6166 - val_MAE: 34.3652\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1287.2043 - MAE: 34.0239 - val_loss: 1268.9584 - val_MAE: 34.3267\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1284.6317 - MAE: 33.9883 - val_loss: 1266.3253 - val_MAE: 34.2887\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1282.0698 - MAE: 33.9530 - val_loss: 1263.6991 - val_MAE: 34.2507\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1279.5286 - MAE: 33.9178 - val_loss: 1261.0831 - val_MAE: 34.2129\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1277.0017 - MAE: 33.8823 - val_loss: 1258.4541 - val_MAE: 34.1748\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1274.4540 - MAE: 33.8476 - val_loss: 1255.8643 - val_MAE: 34.1372\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1271.9746 - MAE: 33.8131 - val_loss: 1253.2338 - val_MAE: 34.0988\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1269.4232 - MAE: 33.7777 - val_loss: 1250.6888 - val_MAE: 34.0619\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1266.9652 - MAE: 33.7431 - val_loss: 1248.0981 - val_MAE: 34.0241\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1264.4302 - MAE: 33.7082 - val_loss: 1245.5531 - val_MAE: 33.9870\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1261.9719 - MAE: 33.6741 - val_loss: 1242.9893 - val_MAE: 33.9494\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1259.5251 - MAE: 33.6398 - val_loss: 1240.4768 - val_MAE: 33.9128\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1257.0496 - MAE: 33.6054 - val_loss: 1237.9711 - val_MAE: 33.8762\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1254.6249 - MAE: 33.5713 - val_loss: 1235.4553 - val_MAE: 33.8393\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1252.1960 - MAE: 33.5375 - val_loss: 1232.9241 - val_MAE: 33.8022\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1249.7472 - MAE: 33.5032 - val_loss: 1230.4065 - val_MAE: 33.7652\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1247.2975 - MAE: 33.4692 - val_loss: 1227.9242 - val_MAE: 33.7288\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1244.9045 - MAE: 33.4353 - val_loss: 1225.4067 - val_MAE: 33.6917\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1242.4872 - MAE: 33.4013 - val_loss: 1222.9111 - val_MAE: 33.6549\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1240.0839 - MAE: 33.3674 - val_loss: 1220.4016 - val_MAE: 33.6179\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1237.6584 - MAE: 33.3332 - val_loss: 1217.9435 - val_MAE: 33.5817\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1235.2616 - MAE: 33.2998 - val_loss: 1215.4772 - val_MAE: 33.5451\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1232.8866 - MAE: 33.2658 - val_loss: 1213.0381 - val_MAE: 33.5091\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1230.4995 - MAE: 33.2324 - val_loss: 1210.5848 - val_MAE: 33.4727\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1228.1305 - MAE: 33.1990 - val_loss: 1208.1429 - val_MAE: 33.4365\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1225.7631 - MAE: 33.1652 - val_loss: 1205.7046 - val_MAE: 33.4003\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1223.3981 - MAE: 33.1320 - val_loss: 1203.2817 - val_MAE: 33.3643\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1221.0134 - MAE: 33.0981 - val_loss: 1200.8635 - val_MAE: 33.3283\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1218.6790 - MAE: 33.0650 - val_loss: 1198.4587 - val_MAE: 33.2925\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1216.3633 - MAE: 33.0320 - val_loss: 1196.0494 - val_MAE: 33.2566\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1214.0116 - MAE: 32.9988 - val_loss: 1193.6514 - val_MAE: 33.2207\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1211.6783 - MAE: 32.9654 - val_loss: 1191.2782 - val_MAE: 33.1852\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1209.3491 - MAE: 32.9323 - val_loss: 1188.8889 - val_MAE: 33.1494\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1207.0697 - MAE: 32.8996 - val_loss: 1186.4839 - val_MAE: 33.1133\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1204.7430 - MAE: 32.8661 - val_loss: 1184.0984 - val_MAE: 33.0775\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1202.4192 - MAE: 32.8329 - val_loss: 1181.7284 - val_MAE: 33.0419\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1200.1455 - MAE: 32.7999 - val_loss: 1179.3612 - val_MAE: 33.0063\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1197.8291 - MAE: 32.7671 - val_loss: 1177.0258 - val_MAE: 32.9711\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1195.5570 - MAE: 32.7346 - val_loss: 1174.6958 - val_MAE: 32.9359\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1193.3009 - MAE: 32.7020 - val_loss: 1172.3451 - val_MAE: 32.9003\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1191.0332 - MAE: 32.6696 - val_loss: 1170.0157 - val_MAE: 32.8652\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1188.7396 - MAE: 32.6365 - val_loss: 1167.6931 - val_MAE: 32.8301\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1186.5048 - MAE: 32.6039 - val_loss: 1165.3607 - val_MAE: 32.7948\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1184.2281 - MAE: 32.5711 - val_loss: 1163.0443 - val_MAE: 32.7596\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1181.9902 - MAE: 32.5390 - val_loss: 1160.7189 - val_MAE: 32.7242\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1179.7629 - MAE: 32.5066 - val_loss: 1158.4208 - val_MAE: 32.6894\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1177.4929 - MAE: 32.4739 - val_loss: 1156.1515 - val_MAE: 32.6549\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1175.2958 - MAE: 32.4422 - val_loss: 1153.8744 - val_MAE: 32.6202\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1173.0771 - MAE: 32.4099 - val_loss: 1151.6102 - val_MAE: 32.5855\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1170.8805 - MAE: 32.3782 - val_loss: 1149.3531 - val_MAE: 32.5510\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1168.7034 - MAE: 32.3462 - val_loss: 1147.0719 - val_MAE: 32.5161\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1166.5177 - MAE: 32.3141 - val_loss: 1144.7994 - val_MAE: 32.4813\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1164.3076 - MAE: 32.2821 - val_loss: 1142.5377 - val_MAE: 32.4465\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1162.1587 - MAE: 32.2507 - val_loss: 1140.2710 - val_MAE: 32.4117\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1159.9385 - MAE: 32.2181 - val_loss: 1138.0392 - val_MAE: 32.3773\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1157.7742 - MAE: 32.1863 - val_loss: 1135.8070 - val_MAE: 32.3430\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1155.6176 - MAE: 32.1550 - val_loss: 1133.5771 - val_MAE: 32.3087\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1153.4408 - MAE: 32.1228 - val_loss: 1131.3330 - val_MAE: 32.2741\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=3, model__n_neurons=5, model__optimizer=adam; total time=  10.8s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 57932580.0000 - MAE: 2756.1287 - val_loss: 300.7874 - val_MAE: 16.2072\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 429.9835 - MAE: 18.6203 - val_loss: 299.7962 - val_MAE: 16.1766\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 428.8460 - MAE: 18.5894 - val_loss: 298.8137 - val_MAE: 16.1462\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 427.7162 - MAE: 18.5591 - val_loss: 297.8152 - val_MAE: 16.1153\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 426.5698 - MAE: 18.5282 - val_loss: 296.8333 - val_MAE: 16.0848\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 425.4422 - MAE: 18.4973 - val_loss: 295.8597 - val_MAE: 16.0545\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 424.3222 - MAE: 18.4673 - val_loss: 294.8860 - val_MAE: 16.0241\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 423.2023 - MAE: 18.4373 - val_loss: 293.9090 - val_MAE: 15.9936\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 422.0775 - MAE: 18.4066 - val_loss: 292.9271 - val_MAE: 15.9629\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 420.9486 - MAE: 18.3757 - val_loss: 291.9622 - val_MAE: 15.9326\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 419.8375 - MAE: 18.3458 - val_loss: 290.9924 - val_MAE: 15.9022\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 418.7212 - MAE: 18.3153 - val_loss: 290.0266 - val_MAE: 15.8718\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 417.6083 - MAE: 18.2849 - val_loss: 289.0556 - val_MAE: 15.8412\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 416.4912 - MAE: 18.2541 - val_loss: 288.0963 - val_MAE: 15.8109\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 415.3848 - MAE: 18.2237 - val_loss: 287.1371 - val_MAE: 15.7805\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 414.2805 - MAE: 18.1932 - val_loss: 286.1896 - val_MAE: 15.7505\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 413.1866 - MAE: 18.1637 - val_loss: 285.2338 - val_MAE: 15.7201\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 412.0841 - MAE: 18.1333 - val_loss: 284.2788 - val_MAE: 15.6897\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 410.9825 - MAE: 18.1033 - val_loss: 283.3247 - val_MAE: 15.6592\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 409.8807 - MAE: 18.0725 - val_loss: 282.3690 - val_MAE: 15.6287\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 408.7771 - MAE: 18.0417 - val_loss: 281.4142 - val_MAE: 15.5981\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 407.6758 - MAE: 18.0112 - val_loss: 280.4718 - val_MAE: 15.5679\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 406.5860 - MAE: 17.9813 - val_loss: 279.5206 - val_MAE: 15.5373\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 405.4863 - MAE: 17.9505 - val_loss: 278.5692 - val_MAE: 15.5067\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 404.3867 - MAE: 17.9194 - val_loss: 277.6192 - val_MAE: 15.4760\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 403.2878 - MAE: 17.8890 - val_loss: 276.6697 - val_MAE: 15.4453\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 402.1901 - MAE: 17.8586 - val_loss: 275.7232 - val_MAE: 15.4146\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 401.0951 - MAE: 17.8276 - val_loss: 274.7776 - val_MAE: 15.3839\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 399.9993 - MAE: 17.7971 - val_loss: 273.8256 - val_MAE: 15.3529\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 398.8982 - MAE: 17.7656 - val_loss: 272.8855 - val_MAE: 15.3223\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 397.8091 - MAE: 17.7350 - val_loss: 271.9480 - val_MAE: 15.2917\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 396.7212 - MAE: 17.7046 - val_loss: 270.9913 - val_MAE: 15.2604\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 395.6122 - MAE: 17.6735 - val_loss: 270.0379 - val_MAE: 15.2291\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 394.5062 - MAE: 17.6431 - val_loss: 269.0821 - val_MAE: 15.1977\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 393.3980 - MAE: 17.6122 - val_loss: 268.1279 - val_MAE: 15.1662\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 392.2910 - MAE: 17.5804 - val_loss: 267.1808 - val_MAE: 15.1350\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 391.1910 - MAE: 17.5500 - val_loss: 266.2309 - val_MAE: 15.1036\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 390.0886 - MAE: 17.5193 - val_loss: 265.2862 - val_MAE: 15.0723\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 388.9891 - MAE: 17.4884 - val_loss: 264.3216 - val_MAE: 15.0402\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 387.8685 - MAE: 17.4572 - val_loss: 263.3554 - val_MAE: 15.0081\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 386.7445 - MAE: 17.4249 - val_loss: 262.3915 - val_MAE: 14.9759\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 385.6230 - MAE: 17.3932 - val_loss: 261.4262 - val_MAE: 14.9437\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 384.5000 - MAE: 17.3614 - val_loss: 260.4615 - val_MAE: 14.9114\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 383.3755 - MAE: 17.3296 - val_loss: 259.4848 - val_MAE: 14.8786\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 382.2383 - MAE: 17.2971 - val_loss: 258.5042 - val_MAE: 14.8456\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 381.0951 - MAE: 17.2644 - val_loss: 257.5257 - val_MAE: 14.8126\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 379.9553 - MAE: 17.2323 - val_loss: 256.5430 - val_MAE: 14.7794\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 378.8065 - MAE: 17.1993 - val_loss: 255.5406 - val_MAE: 14.7454\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 377.6369 - MAE: 17.1663 - val_loss: 254.5386 - val_MAE: 14.7114\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 376.4671 - MAE: 17.1326 - val_loss: 253.5339 - val_MAE: 14.6772\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 375.2924 - MAE: 17.0988 - val_loss: 252.5152 - val_MAE: 14.6425\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 374.1026 - MAE: 17.0645 - val_loss: 251.4926 - val_MAE: 14.6075\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 372.9059 - MAE: 17.0300 - val_loss: 250.4581 - val_MAE: 14.5721\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 371.6976 - MAE: 16.9946 - val_loss: 249.4347 - val_MAE: 14.5369\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 370.4973 - MAE: 16.9601 - val_loss: 248.3883 - val_MAE: 14.5009\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 369.2710 - MAE: 16.9246 - val_loss: 247.3302 - val_MAE: 14.4643\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 368.0318 - MAE: 16.8885 - val_loss: 246.2728 - val_MAE: 14.4277\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 366.7907 - MAE: 16.8527 - val_loss: 245.1901 - val_MAE: 14.3902\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 365.5212 - MAE: 16.8150 - val_loss: 244.1074 - val_MAE: 14.3525\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 364.2498 - MAE: 16.7774 - val_loss: 243.0098 - val_MAE: 14.3142\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 362.9613 - MAE: 16.7404 - val_loss: 241.9058 - val_MAE: 14.2756\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 361.6623 - MAE: 16.7013 - val_loss: 240.7849 - val_MAE: 14.2363\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 360.3441 - MAE: 16.6624 - val_loss: 239.6511 - val_MAE: 14.1964\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 359.0088 - MAE: 16.6234 - val_loss: 238.4902 - val_MAE: 14.1555\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 357.6441 - MAE: 16.5832 - val_loss: 237.3378 - val_MAE: 14.1147\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 356.2846 - MAE: 16.5425 - val_loss: 236.1579 - val_MAE: 14.0729\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 354.8930 - MAE: 16.5013 - val_loss: 234.9570 - val_MAE: 14.0301\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 353.4753 - MAE: 16.4589 - val_loss: 233.7274 - val_MAE: 13.9862\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 352.0253 - MAE: 16.4142 - val_loss: 232.4871 - val_MAE: 13.9418\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 350.5588 - MAE: 16.3711 - val_loss: 231.2343 - val_MAE: 13.8968\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 349.0796 - MAE: 16.3258 - val_loss: 229.9752 - val_MAE: 13.8514\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 347.5874 - MAE: 16.2821 - val_loss: 228.6699 - val_MAE: 13.8042\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 346.0436 - MAE: 16.2338 - val_loss: 227.3628 - val_MAE: 13.7568\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 344.4929 - MAE: 16.1878 - val_loss: 226.0060 - val_MAE: 13.7074\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 342.8856 - MAE: 16.1384 - val_loss: 224.6416 - val_MAE: 13.6576\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 341.2694 - MAE: 16.0900 - val_loss: 223.2653 - val_MAE: 13.6071\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 339.6320 - MAE: 16.0385 - val_loss: 221.8369 - val_MAE: 13.5545\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 337.9349 - MAE: 15.9860 - val_loss: 220.3883 - val_MAE: 13.5009\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 336.2121 - MAE: 15.9339 - val_loss: 218.9137 - val_MAE: 13.4462\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 334.4579 - MAE: 15.8788 - val_loss: 217.3952 - val_MAE: 13.3896\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 332.6534 - MAE: 15.8223 - val_loss: 215.8851 - val_MAE: 13.3331\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 330.8485 - MAE: 15.7675 - val_loss: 214.3145 - val_MAE: 13.2744\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 328.9756 - MAE: 15.7081 - val_loss: 212.7099 - val_MAE: 13.2183\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 327.0599 - MAE: 15.6480 - val_loss: 211.0733 - val_MAE: 13.1608\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 325.1069 - MAE: 15.5869 - val_loss: 209.4111 - val_MAE: 13.1021\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 323.1162 - MAE: 15.5242 - val_loss: 207.6911 - val_MAE: 13.0412\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 321.0594 - MAE: 15.4583 - val_loss: 205.9577 - val_MAE: 12.9794\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 318.9811 - MAE: 15.3935 - val_loss: 204.1544 - val_MAE: 12.9148\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 316.8235 - MAE: 15.3241 - val_loss: 202.3489 - val_MAE: 12.8497\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 314.6547 - MAE: 15.2537 - val_loss: 200.4917 - val_MAE: 12.7824\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 312.4236 - MAE: 15.1841 - val_loss: 198.5864 - val_MAE: 12.7130\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 310.1365 - MAE: 15.1098 - val_loss: 196.6564 - val_MAE: 12.6423\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 307.8130 - MAE: 15.0361 - val_loss: 194.6779 - val_MAE: 12.5693\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 305.4276 - MAE: 14.9595 - val_loss: 192.6440 - val_MAE: 12.4938\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 302.9749 - MAE: 14.8785 - val_loss: 190.5643 - val_MAE: 12.4161\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 300.4671 - MAE: 14.7968 - val_loss: 188.4604 - val_MAE: 12.3369\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 297.9213 - MAE: 14.7139 - val_loss: 186.2629 - val_MAE: 12.2536\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 295.2660 - MAE: 14.6229 - val_loss: 184.0567 - val_MAE: 12.1694\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 292.5917 - MAE: 14.5356 - val_loss: 181.7753 - val_MAE: 12.0816\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 289.8282 - MAE: 14.4461 - val_loss: 179.4563 - val_MAE: 11.9917\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=2, model__n_neurons=25, model__optimizer=nesterov; total time=  10.7s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 739350372414714288090906624.0000 - MAE: 9892732600320.0000 - val_loss: 223383801581203881984.0000 - val_MAE: 14946029568.0000\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 223089396347750580224.0000 - MAE: 14936176640.0000 - val_loss: 222670016224637747200.0000 - val_MAE: 14922132480.0000\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 222376596153602932736.0000 - MAE: 14912296960.0000 - val_loss: 221958570628815519744.0000 - val_MAE: 14898274304.0000\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 221666082943641059328.0000 - MAE: 14888452096.0000 - val_loss: 221249376832806977536.0000 - val_MAE: 14874455040.0000\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 220957803941306826752.0000 - MAE: 14864649216.0000 - val_loss: 220542434836612120576.0000 - val_MAE: 14850672640.0000\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 220251794330972323840.0000 - MAE: 14840881152.0000 - val_loss: 219837779824603037696.0000 - val_MAE: 14826928128.0000\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 219548036520451506176.0000 - MAE: 14817152000.0000 - val_loss: 219135359020221595648.0000 - val_MAE: 14803219456.0000\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 218846512917558329344.0000 - MAE: 14793460736.0000 - val_loss: 218435154831281750016.0000 - val_MAE: 14779551744.0000\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 218147276298850926592.0000 - MAE: 14769808384.0000 - val_loss: 217737202442155589632.0000 - val_MAE: 14755920896.0000\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 217450256295585120256.0000 - MAE: 14746192896.0000 - val_loss: 217041484260657070080.0000 - val_MAE: 14732327936.0000\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 216755435315574865920.0000 - MAE: 14722614272.0000 - val_loss: 216348000286786191360.0000 - val_MAE: 14708772864.0000\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 216062866135378296832.0000 - MAE: 14699075584.0000 - val_loss: 215656697743984820224.0000 - val_MAE: 14685254656.0000\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 215372495978437279744.0000 - MAE: 14675574784.0000 - val_loss: 214967664593183178752.0000 - val_MAE: 14661775360.0000\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 214684360029123903488.0000 - MAE: 14652110848.0000 - val_loss: 214280812873451044864.0000 - val_MAE: 14638331904.0000\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 213998405510880034816.0000 - MAE: 14628683776.0000 - val_loss: 213596124992602374144.0000 - val_MAE: 14614928384.0000\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 213314614831519629312.0000 - MAE: 14605293568.0000 - val_loss: 212913671319381344256.0000 - val_MAE: 14591561728.0000\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 212633075951972909056.0000 - MAE: 14581942272.0000 - val_loss: 212233381485043777536.0000 - val_MAE: 14568232960.0000\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 211953700911309651968.0000 - MAE: 14558627840.0000 - val_loss: 211555255489589673984.0000 - val_MAE: 14544940032.0000\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 211276436932971724800.0000 - MAE: 14535349248.0000 - val_loss: 210879258148646944768.0000 - val_MAE: 14521682944.0000\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 210601371977889349632.0000 - MAE: 14512108544.0000 - val_loss: 210205477423145811968.0000 - val_MAE: 14498464768.0000\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 209928453269504393216.0000 - MAE: 14488906752.0000 - val_loss: 209533807759970009088.0000 - val_MAE: 14475283456.0000\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 209257680807816855552.0000 - MAE: 14465740800.0000 - val_loss: 208864337120049758208.0000 - val_MAE: 14452139008.0000\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 208589054592826736640.0000 - MAE: 14442611712.0000 - val_loss: 208196942358082748416.0000 - val_MAE: 14429030400.0000\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 207922557032347992064.0000 - MAE: 14419519488.0000 - val_loss: 207531729027185246208.0000 - val_MAE: 14405961728.0000\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 207258223310752710656.0000 - MAE: 14396464128.0000 - val_loss: 206868591574240985088.0000 - val_MAE: 14382927872.0000\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 206595983059296714752.0000 - MAE: 14373444608.0000 - val_loss: 206207635552366231552.0000 - val_MAE: 14359931904.0000\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 205935889054538137600.0000 - MAE: 14350464000.0000 - val_loss: 205548755408444719104.0000 - val_MAE: 14336971776.0000\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 205277870927732801536.0000 - MAE: 14327519232.0000 - val_loss: 204891986326848536576.0000 - val_MAE: 14314047488.0000\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 204621963863252795392.0000 - MAE: 14304613376.0000 - val_loss: 204237310715391639552.0000 - val_MAE: 14291162112.0000\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 203968132676726030336.0000 - MAE: 14281740288.0000 - val_loss: 203584763758446116864.0000 - val_MAE: 14268312576.0000\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 203316430144710639616.0000 - MAE: 14258905088.0000 - val_loss: 202934239902895702016.0000 - val_MAE: 14245498880.0000\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 202666803490648489984.0000 - MAE: 14236106752.0000 - val_loss: 202285844701856661504.0000 - val_MAE: 14222722048.0000\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 202019217530167492608.0000 - MAE: 14213346304.0000 - val_loss: 201639455010026684416.0000 - val_MAE: 14199981056.0000\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 201373742632011825152.0000 - MAE: 14190619648.0000 - val_loss: 200995193972708081664.0000 - val_MAE: 14177276928.0000\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 200730290835251265536.0000 - MAE: 14167930880.0000 - val_loss: 200352973628970631168.0000 - val_MAE: 14154609664.0000\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 200088932508629991424.0000 - MAE: 14145277952.0000 - val_loss: 199712811571000377344.0000 - val_MAE: 14131978240.0000\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 199449614875589869568.0000 - MAE: 14122660864.0000 - val_loss: 199074707798797320192.0000 - val_MAE: 14109383680.0000\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 198812337936130899968.0000 - MAE: 14100081664.0000 - val_loss: 198438609535803326464.0000 - val_MAE: 14086823936.0000\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 198177101690253082624.0000 - MAE: 14077536256.0000 - val_loss: 197804551966390484992.0000 - val_MAE: 14064301056.0000\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 197543888545770373120.0000 - MAE: 14055030784.0000 - val_loss: 197172535090558795776.0000 - val_MAE: 14041814016.0000\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 196912680910496727040.0000 - MAE: 14032557056.0000 - val_loss: 196542558908308258816.0000 - val_MAE: 14019362816.0000\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 196283513968804233216.0000 - MAE: 14010119168.0000 - val_loss: 195914535458708652032.0000 - val_MAE: 13996947456.0000\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 195656334944134758400.0000 - MAE: 13987719168.0000 - val_loss: 195288499926132064256.0000 - val_MAE: 13974566912.0000\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 195031179020860391424.0000 - MAE: 13965355008.0000 - val_loss: 194664557863694761984.0000 - val_MAE: 13952223232.0000\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 194407993422422999040.0000 - MAE: 13943026688.0000 - val_loss: 194042550941722345472.0000 - val_MAE: 13929917440.0000\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 193786813333194670080.0000 - MAE: 13920732160.0000 - val_loss: 193422531936772947968.0000 - val_MAE: 13907643392.0000\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 193167638753175404544.0000 - MAE: 13898474496.0000 - val_loss: 192804500848846569472.0000 - val_MAE: 13885406208.0000\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 192550434497993113600.0000 - MAE: 13876252672.0000 - val_loss: 192188457677943209984.0000 - val_MAE: 13863204864.0000\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 191935182975461752832.0000 - MAE: 13854065664.0000 - val_loss: 191574420016248913920.0000 - val_MAE: 13841040384.0000\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 191321936962139455488.0000 - MAE: 13831915520.0000 - val_loss: 190962264718461370368.0000 - val_MAE: 13818909696.0000\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 190710608497095999488.0000 - MAE: 13809802240.0000 - val_loss: 190352114929882890240.0000 - val_MAE: 13796815872.0000\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 190101250356889518080.0000 - MAE: 13787720704.0000 - val_loss: 189743900281769295872.0000 - val_MAE: 13774754816.0000\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 189493844949333966848.0000 - MAE: 13765675008.0000 - val_loss: 189137638366306631680.0000 - val_MAE: 13752731648.0000\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 188888357090057256960.0000 - MAE: 13743665152.0000 - val_loss: 188533311591308853248.0000 - val_MAE: 13730743296.0000\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 188284839555617521664.0000 - MAE: 13721691136.0000 - val_loss: 187930867180217827328.0000 - val_MAE: 13708788736.0000\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 187683239569456627712.0000 - MAE: 13699751936.0000 - val_loss: 187330445870521909248.0000 - val_MAE: 13686871040.0000\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 187083539539388530688.0000 - MAE: 13677848576.0000 - val_loss: 186731871740360654848.0000 - val_MAE: 13664988160.0000\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 186485792241971363840.0000 - MAE: 13655979008.0000 - val_loss: 186135250342850330624.0000 - val_MAE: 13643140096.0000\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 185889909716274905088.0000 - MAE: 13634144256.0000 - val_loss: 185540493717060714496.0000 - val_MAE: 13621324800.0000\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 185295979923229376512.0000 - MAE: 13612345344.0000 - val_loss: 184947654639549939712.0000 - val_MAE: 13599546368.0000\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 184703897309718511616.0000 - MAE: 13590580224.0000 - val_loss: 184356697925945917440.0000 - val_MAE: 13577801728.0000\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 184113732244486488064.0000 - MAE: 13568850944.0000 - val_loss: 183767658760620736512.0000 - val_MAE: 13556092928.0000\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 183525467135347261440.0000 - MAE: 13547155456.0000 - val_loss: 183180466774830219264.0000 - val_MAE: 13534417920.0000\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 182939066797928742912.0000 - MAE: 13525494784.0000 - val_loss: 182595192337318543360.0000 - val_MAE: 13512778752.0000\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 182354548824416976896.0000 - MAE: 13503872000.0000 - val_loss: 182011729894969442304.0000 - val_MAE: 13491173376.0000\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 181771860438253830144.0000 - MAE: 13482278912.0000 - val_loss: 181430202593085227008.0000 - val_MAE: 13469602816.0000\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 181191089600369524736.0000 - MAE: 13460721664.0000 - val_loss: 180850487286363586560.0000 - val_MAE: 13448066048.0000\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 180612130757647794176.0000 - MAE: 13439200256.0000 - val_loss: 180272636751362654208.0000 - val_MAE: 13426564096.0000\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 180035036686646771712.0000 - MAE: 13417712640.0000 - val_loss: 179696598211524296704.0000 - val_MAE: 13405095936.0000\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 179459807387366457344.0000 - MAE: 13396258816.0000 - val_loss: 179122424443406647296.0000 - val_MAE: 13383662592.0000\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 178886390083248717824.0000 - MAE: 13374838784.0000 - val_loss: 178550133039195750400.0000 - val_MAE: 13362265088.0000\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 178314819958665641984.0000 - MAE: 13353454592.0000 - val_loss: 177979600853589295104.0000 - val_MAE: 13340900352.0000\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 177745044237059096576.0000 - MAE: 13332105216.0000 - val_loss: 177410898255331459072.0000 - val_MAE: 13319568384.0000\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 177177115694987214848.0000 - MAE: 13310788608.0000 - val_loss: 176844078020980375552.0000 - val_MAE: 13298272256.0000\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 176610999148077907968.0000 - MAE: 13289504768.0000 - val_loss: 176279017005233733632.0000 - val_MAE: 13277010944.0000\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 176046694596331175936.0000 - MAE: 13268256768.0000 - val_loss: 175715732800277577728.0000 - val_MAE: 13255781376.0000\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 175484149263188885504.0000 - MAE: 13247042560.0000 - val_loss: 175154313367042129920.0000 - val_MAE: 13234588672.0000\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 174923486293953347584.0000 - MAE: 13225863168.0000 - val_loss: 174594670744597168128.0000 - val_MAE: 13213428736.0000\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 174364582543322251264.0000 - MAE: 13204717568.0000 - val_loss: 174036787340756647936.0000 - val_MAE: 13192300544.0000\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 173807420419109552128.0000 - MAE: 13183602688.0000 - val_loss: 173480663155520569344.0000 - val_MAE: 13171207168.0000\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 173252052697873383424.0000 - MAE: 13162524672.0000 - val_loss: 172926386149819154432.0000 - val_MAE: 13150147584.0000\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 172698496971799789568.0000 - MAE: 13141479424.0000 - val_loss: 172373868362722181120.0000 - val_MAE: 13129122816.0000\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 172146682872144592896.0000 - MAE: 13120468992.0000 - val_loss: 171823092202043604992.0000 - val_MAE: 13108130816.0000\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 171596663175465926656.0000 - MAE: 13099489280.0000 - val_loss: 171274057667783426048.0000 - val_MAE: 13087171584.0000\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 171048332328647524352.0000 - MAE: 13078544384.0000 - val_loss: 170726764759941644288.0000 - val_MAE: 13066245120.0000\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 170501778292619608064.0000 - MAE: 13057632256.0000 - val_loss: 170181301439448481792.0000 - val_MAE: 13045355520.0000\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 169957036251754266624.0000 - MAE: 13036755968.0000 - val_loss: 169637562153187672064.0000 - val_MAE: 13024498688.0000\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 169414018245121277952.0000 - MAE: 13015912448.0000 - val_loss: 169095546901159215104.0000 - val_MAE: 13003674624.0000\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 168872706680534597632.0000 - MAE: 12995101696.0000 - val_loss: 168555255683363110912.0000 - val_MAE: 12982883328.0000\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 168333101557994225664.0000 - MAE: 12974323712.0000 - val_loss: 168016670907613315072.0000 - val_MAE: 12962124800.0000\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 167795238061872250880.0000 - MAE: 12953579520.0000 - val_loss: 167479827758281916416.0000 - val_MAE: 12941399040.0000\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 167259081007796584448.0000 - MAE: 12932867072.0000 - val_loss: 166944638274438692864.0000 - val_MAE: 12920706048.0000\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 166724595211395137536.0000 - MAE: 12912187392.0000 - val_loss: 166411190417013866496.0000 - val_MAE: 12900045824.0000\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 166191851041412087808.0000 - MAE: 12891540480.0000 - val_loss: 165879413817263259648.0000 - val_MAE: 12879418368.0000\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 165660778129103257600.0000 - MAE: 12870927360.0000 - val_loss: 165349396436117094400.0000 - val_MAE: 12858825728.0000\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 165131517211957002240.0000 - MAE: 12850349056.0000 - val_loss: 164821120681389326336.0000 - val_MAE: 12838266880.0000\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 164603927552484966400.0000 - MAE: 12829805568.0000 - val_loss: 164294533776521822208.0000 - val_MAE: 12817741824.0000\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 164078044335059238912.0000 - MAE: 12809292800.0000 - val_loss: 163769618129328537600.0000 - val_MAE: 12797249536.0000\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 163553814783121686528.0000 - MAE: 12788814848.0000 - val_loss: 163246391331995516928.0000 - val_MAE: 12776791040.0000\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 163031274081044398080.0000 - MAE: 12768367616.0000 - val_loss: 162724835792336715776.0000 - val_MAE: 12756364288.0000\n",
            "5/5 [==============================] - 0s 6ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=2, model__n_neurons=25, model__optimizer=nesterov; total time=   6.4s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 0s 22ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=2, model__n_neurons=25, model__optimizer=nesterov; total time=   1.8s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 22ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.5, model__n_hidden=3, model__n_neurons=125, model__optimizer=momentum; total time=   1.2s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 23ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.5, model__n_hidden=3, model__n_neurons=125, model__optimizer=momentum; total time=   1.8s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 33ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 13ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.5, model__n_hidden=3, model__n_neurons=125, model__optimizer=momentum; total time=   1.6s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 32ms/step - loss: 1283.5951 - MAE: 26.7295 - val_loss: 173.8707 - val_MAE: 11.4112\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 203.0350 - MAE: 11.4609 - val_loss: 111.8188 - val_MAE: 9.4025\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 154.2232 - MAE: 9.6529 - val_loss: 92.7484 - val_MAE: 8.6385\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 140.7834 - MAE: 9.0779 - val_loss: 86.8237 - val_MAE: 8.0559\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 127.5164 - MAE: 8.4831 - val_loss: 71.4967 - val_MAE: 7.4159\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 118.9638 - MAE: 8.1033 - val_loss: 58.4110 - val_MAE: 6.8345\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 109.8529 - MAE: 7.6463 - val_loss: 52.4959 - val_MAE: 6.3119\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 104.1131 - MAE: 7.3406 - val_loss: 48.3035 - val_MAE: 5.8936\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 96.8138 - MAE: 7.0059 - val_loss: 47.6599 - val_MAE: 5.7004\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 92.3749 - MAE: 6.8451 - val_loss: 38.7271 - val_MAE: 5.1815\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 88.7252 - MAE: 6.6406 - val_loss: 37.7165 - val_MAE: 4.9940\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 86.1157 - MAE: 6.5600 - val_loss: 32.4222 - val_MAE: 4.5689\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 83.1435 - MAE: 6.3778 - val_loss: 35.3886 - val_MAE: 4.6245\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 83.4807 - MAE: 6.5045 - val_loss: 28.2841 - val_MAE: 4.2131\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 78.9703 - MAE: 6.3400 - val_loss: 26.6704 - val_MAE: 4.1578\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 78.3025 - MAE: 6.2898 - val_loss: 25.4875 - val_MAE: 4.1585\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 76.9503 - MAE: 6.1212 - val_loss: 26.6663 - val_MAE: 4.1264\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 75.3325 - MAE: 6.1482 - val_loss: 27.1386 - val_MAE: 4.1274\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 73.5821 - MAE: 6.0137 - val_loss: 29.2919 - val_MAE: 4.3257\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 73.6366 - MAE: 6.1324 - val_loss: 28.7272 - val_MAE: 4.3457\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 72.2934 - MAE: 6.2027 - val_loss: 24.6240 - val_MAE: 4.0635\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 72.4356 - MAE: 6.0519 - val_loss: 23.7676 - val_MAE: 4.0081\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 70.8268 - MAE: 5.9323 - val_loss: 27.5414 - val_MAE: 4.3977\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 70.9736 - MAE: 6.0922 - val_loss: 24.5896 - val_MAE: 4.1487\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 70.3624 - MAE: 6.0707 - val_loss: 23.5574 - val_MAE: 4.0462\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 69.5610 - MAE: 5.9214 - val_loss: 26.9907 - val_MAE: 4.4033\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 70.3402 - MAE: 6.0904 - val_loss: 22.8208 - val_MAE: 3.9700\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 70.5407 - MAE: 5.9953 - val_loss: 22.1021 - val_MAE: 3.8711\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 69.2895 - MAE: 5.8983 - val_loss: 25.4598 - val_MAE: 4.2890\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 68.9234 - MAE: 6.0596 - val_loss: 21.2377 - val_MAE: 3.7320\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 68.8858 - MAE: 5.9592 - val_loss: 20.6854 - val_MAE: 3.5693\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 69.2519 - MAE: 5.7479 - val_loss: 26.6631 - val_MAE: 4.4390\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 69.4747 - MAE: 6.0514 - val_loss: 22.8806 - val_MAE: 4.0435\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 67.9312 - MAE: 5.8922 - val_loss: 25.0120 - val_MAE: 4.3013\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 68.1384 - MAE: 5.8801 - val_loss: 26.3794 - val_MAE: 4.4353\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 68.2905 - MAE: 6.0087 - val_loss: 22.8456 - val_MAE: 4.0573\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 67.7905 - MAE: 5.9361 - val_loss: 21.7873 - val_MAE: 3.8889\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 68.8903 - MAE: 5.9315 - val_loss: 20.6641 - val_MAE: 3.7048\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 68.0745 - MAE: 5.8154 - val_loss: 24.6038 - val_MAE: 4.2757\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 68.0474 - MAE: 5.9291 - val_loss: 26.9084 - val_MAE: 4.4907\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 68.1088 - MAE: 6.1017 - val_loss: 21.1627 - val_MAE: 3.8023\n",
            "Epoch 41: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=2, model__n_neurons=5, model__optimizer=sgd; total time=   5.8s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 503.6803 - MAE: 13.8712 - val_loss: 38.9523 - val_MAE: 5.0989\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 99.1053 - MAE: 7.5801 - val_loss: 35.2425 - val_MAE: 5.0062\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 100.6441 - MAE: 7.5738 - val_loss: 50.6685 - val_MAE: 5.4360\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 99.0538 - MAE: 7.6085 - val_loss: 45.9607 - val_MAE: 5.1991\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 98.7395 - MAE: 7.6245 - val_loss: 39.0166 - val_MAE: 4.9343\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 103.1090 - MAE: 7.6970 - val_loss: 32.5614 - val_MAE: 4.7469\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 99.4255 - MAE: 7.5204 - val_loss: 34.4393 - val_MAE: 4.7357\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 102.9438 - MAE: 7.6131 - val_loss: 41.7681 - val_MAE: 4.9379\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 95.0578 - MAE: 7.5155 - val_loss: 49.0760 - val_MAE: 5.3721\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 96.2310 - MAE: 7.5308 - val_loss: 49.5683 - val_MAE: 5.4214\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 96.6993 - MAE: 7.6535 - val_loss: 31.9680 - val_MAE: 4.5219\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 95.1438 - MAE: 7.4040 - val_loss: 40.8426 - val_MAE: 4.8399\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 95.2910 - MAE: 7.6161 - val_loss: 37.2146 - val_MAE: 4.6613\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 98.5227 - MAE: 7.7115 - val_loss: 35.8700 - val_MAE: 4.5880\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 93.8862 - MAE: 7.4926 - val_loss: 36.2808 - val_MAE: 4.5883\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 93.2508 - MAE: 7.5186 - val_loss: 32.3339 - val_MAE: 4.4491\n",
            "Epoch 16: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=2, model__n_neurons=5, model__optimizer=sgd; total time=   1.7s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 22ms/step - loss: 5079.9648 - MAE: 38.4828 - val_loss: 319.7058 - val_MAE: 15.4839\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 514.9766 - MAE: 19.7446 - val_loss: 547.7706 - val_MAE: 21.8169\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 556.1591 - MAE: 21.8701 - val_loss: 547.2204 - val_MAE: 21.8043\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 555.6077 - MAE: 21.8577 - val_loss: 546.6729 - val_MAE: 21.7917\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 555.0589 - MAE: 21.8450 - val_loss: 546.1269 - val_MAE: 21.7792\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 554.5117 - MAE: 21.8324 - val_loss: 545.5814 - val_MAE: 21.7667\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 553.9648 - MAE: 21.8200 - val_loss: 545.0320 - val_MAE: 21.7541\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 553.4144 - MAE: 21.8072 - val_loss: 544.4862 - val_MAE: 21.7415\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 552.8674 - MAE: 21.7948 - val_loss: 543.9424 - val_MAE: 21.7290\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 552.3220 - MAE: 21.7823 - val_loss: 543.3967 - val_MAE: 21.7164\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 551.7750 - MAE: 21.7698 - val_loss: 542.8521 - val_MAE: 21.7039\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=2, model__n_neurons=5, model__optimizer=sgd; total time=   1.6s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 7949035008.0000 - MAE: 31438.1758 - val_loss: 3089751343104.0000 - val_MAE: 1720453.6250\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2964270119160190599168.0000 - MAE: 20063752192.0000 - val_loss: 1113027177955718363873280.0000 - val_MAE: 1034732175360.0000\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1083602727623816336567961615597568.0000 - MAE: 11775105961033728.0000 - val_loss: 441145221479193088104837286281084928.0000 - val_MAE: 649183894473539584.0000\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: inf - MAE: 7620198092951982702592.0000 - val_loss: inf - val_MAE: 359872918447981215088640.0000\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: inf - MAE: 4317137558643454343733313536.0000 - val_loss: inf - val_MAE: 218031433840500341773374062592.0000\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: inf - MAE: 2432525716227432515173403131379712.0000 - val_loss: inf - val_MAE: 130109112402704695027776424400388096.0000\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan   \n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=nesterov; total time=   1.1s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 21ms/step - loss: 13805210828800.0000 - MAE: 1369726.5000 - val_loss: 4475892256473088.0000 - val_MAE: 65696136.0000\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2898383845972245286486016.0000 - MAE: 619442667520.0000 - val_loss: 1066120571728935565385531392.0000 - val_MAE: 32062465638400.0000\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 686390016815828518168164271977922560.0000 - MAE: 302808216708841472.0000 - val_loss: inf - val_MAE: 14471242080183123968.0000\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: inf - MAE: 140911048077767604174848.0000 - val_loss: inf - val_MAE: 6396433871831888373481472.0000\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: inf - MAE: 61638627126891766846379261952.0000 - val_loss: inf - val_MAE: 2947330018380610851232408403968.0000\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: inf - MAE: 26070215042204651662957421524942848.0000 - val_loss: inf - val_MAE: 1322216461858728507555453607763509248.0000\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan    \n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=nesterov; total time=   1.1s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 21ms/step - loss: 12917422424064.0000 - MAE: 1358383.0000 - val_loss: 3654889930489856.0000 - val_MAE: 59130952.0000\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1836634365143391238356992.0000 - MAE: 487010205696.0000 - val_loss: 638141499397567589929451520.0000 - val_MAE: 24681067315200.0000\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 338889472758320335054067380323876864.0000 - MAE: 221602359510302720.0000 - val_loss: inf - val_MAE: 10082719538798919680.0000\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: inf - MAE: 90132484897604760502272.0000 - val_loss: inf - val_MAE: 4250397312371749480824832.0000\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: inf - MAE: 40030239803435056281228935168.0000 - val_loss: inf - val_MAE: 1775240168489639427479529586688.0000\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: inf - MAE: 16104780394796790454695622934003712.0000 - val_loss: inf - val_MAE: 713112487609452165273402743267000320.0000\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan   \n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=nesterov; total time=   1.7s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 22ms/step - loss: 9421986421980331372155315945472.0000 - MAE: 1124049648353280.0000 - val_loss: 2809414911074367967330304.0000 - val_MAE: 1676130910208.0000\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2805712015431946926555136.0000 - MAE: 1675025842176.0000 - val_loss: 2800437399548370601639936.0000 - val_MAE: 1673450749952.0000\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2796747186042500236181504.0000 - MAE: 1672347779072.0000 - val_loss: 2791490152211869165682688.0000 - val_MAE: 1670775308288.0000\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2787811179690668716982272.0000 - MAE: 1669674041344.0000 - val_loss: 2782571151452230597476352.0000 - val_MAE: 1668104060928.0000\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2778903996376452368957440.0000 - MAE: 1667004628992.0000 - val_loss: 2773680397269454897020928.0000 - val_MAE: 1665437007872.0000\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2770025347869475040395264.0000 - MAE: 1664339148800.0000 - val_loss: 2764817889663542064316416.0000 - val_MAE: 1662774149120.0000\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2761174081248232124448768.0000 - MAE: 1661678125056.0000 - val_loss: 2755983340404115947651072.0000 - val_MAE: 1660115484672.0000\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2752351349434228227964928.0000 - MAE: 1659021164544.0000 - val_loss: 2747177037721552698736640.0000 - val_MAE: 1657460883456.0000\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2743556287736334895808512.0000 - MAE: 1656368398336.0000 - val_loss: 2738398693385476165861376.0000 - val_MAE: 1654810738688.0000\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2734789472615304431403008.0000 - MAE: 1653719957504.0000 - val_loss: 2729648019165510197313536.0000 - val_MAE: 1652164657152.0000\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2726050904071136834748416.0000 - MAE: 1651075448832.0000 - val_loss: 2720927032674287855075328.0000 - val_MAE: 1649523294208.0000\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2717341446794960560979968.0000 - MAE: 1648435920896.0000 - val_loss: 2712234004529552228876288.0000 - val_MAE: 1646886125568.0000\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2708659659634894851538944.0000 - MAE: 1645800456192.0000 - val_loss: 2703568358270551015292928.0000 - val_MAE: 1644253282304.0000\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2700005830821315858137088.0000 - MAE: 1643169316864.0000 - val_loss: 2694930670358036517748736.0000 - val_MAE: 1641624371200.0000\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2691379095663095125639168.0000 - MAE: 1640542240768.0000 - val_loss: 2686320364331256432820224.0000 - val_MAE: 1638999785472.0000\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2682780030620984957468672.0000 - MAE: 1637919358976.0000 - val_loss: 2677737440190210760507392.0000 - val_MAE: 1636379394048.0000\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2674208635694985353625600.0000 - MAE: 1635300671488.0000 - val_loss: 2669182186165275652521984.0000 - val_MAE: 1633763065856.0000\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2665664910885096314109952.0000 - MAE: 1632686309376.0000 - val_loss: 2660654314026074957152256.0000 - val_MAE: 1631151194112.0000\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2657148279730565535498240.0000 - MAE: 1630076010496.0000 - val_loss: 2652153535542232522686464.0000 - val_MAE: 1628543385600.0000\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2648658454001016866078720.0000 - MAE: 1627469774848.0000 - val_loss: 2643680138944124500836352.0000 - val_MAE: 1625939771392.0000\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2640196298387578760986624.0000 - MAE: 1624867995648.0000 - val_loss: 2635232971310246284754944.0000 - val_MAE: 1623339958272.0000\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2631760083507994309951488.0000 - MAE: 1622269886464.0000 - val_loss: 2626812320870974026153984.0000 - val_MAE: 1620744470528.0000\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2623350097592639664685056.0000 - MAE: 1619675709440.0000 - val_loss: 2618418475856683876745216.0000 - val_MAE: 1618152652800.0000\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2614967781793395583746048.0000 - MAE: 1617085988864.0000 - val_loss: 2610051724497751988240384.0000 - val_MAE: 1615565422592.0000\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2606611694958381308575744.0000 - MAE: 1614500331520.0000 - val_loss: 2601711778563802208927744.0000 - val_MAE: 1612982255616.0000\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2598282990009101446021120.0000 - MAE: 1611918868480.0000 - val_loss: 2593398638054834538807296.0000 - val_MAE: 1610403282944.0000\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2589980802254427540946944.0000 - MAE: 1609341730816.0000 - val_loss: 2585112591201225129590784.0000 - val_MAE: 1607828373504.0000\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2581705708155111896776704.0000 - MAE: 1606768656384.0000 - val_loss: 2576853061542221677854720.0000 - val_MAE: 1605258051584.0000\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2573457419480778361798656.0000 - MAE: 1604199776256.0000 - val_loss: 2568620049077824183599104.0000 - val_MAE: 1602691399680.0000\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2565234495079546177454080.0000 - MAE: 1601634828288.0000 - val_loss: 2560412112656151888265216.0000 - val_MAE: 1600128679936.0000\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2557037799642543798878208.0000 - MAE: 1599073812480.0000 - val_loss: 2552230693429085550411776.0000 - val_MAE: 1597570154496.0000\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2548867044939395074359296.0000 - MAE: 1596516990976.0000 - val_loss: 2544075791396625170038784.0000 - val_MAE: 1595015823360.0000\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2540723095661228459032576.0000 - MAE: 1593964363776.0000 - val_loss: 2535947406558770747146240.0000 - val_MAE: 1592465686528.0000\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2532605663577667801186304.0000 - MAE: 1591415930880.0000 - val_loss: 2527844674224393826598912.0000 - val_MAE: 1589919744000.0000\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2524513307536832342261760.0000 - MAE: 1588871561216.0000 - val_loss: 2519767306163118256685056.0000 - val_MAE: 1587377471488.0000\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2516446315769098233970688.0000 - MAE: 1586330861568.0000 - val_loss: 2511715878835696340828160.0000 - val_MAE: 1584839524352.0000\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2508405841195970083160064.0000 - MAE: 1583794749440.0000 - val_loss: 2503690680472504230739968.0000 - val_MAE: 1582305509376.0000\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2500391595587071738118144.0000 - MAE: 1581262307328.0000 - val_loss: 2495691134612789622996992.0000 - val_MAE: 1579775688704.0000\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2492401849560146288574464.0000 - MAE: 1578734059520.0000 - val_loss: 2487716376565424062464000.0000 - val_MAE: 1577249669120.0000\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2484437756036698341376000.0000 - MAE: 1576209743872.0000 - val_loss: 2479767847482288307699712.0000 - val_MAE: 1574727843840.0000\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2476499603247104048234496.0000 - MAE: 1573689753600.0000 - val_loss: 2471844970902630055280640.0000 - val_MAE: 1572210212864.0000\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2468586814730611105726464.0000 - MAE: 1571173695488.0000 - val_loss: 2463946305674568546648064.0000 - val_MAE: 1569696251904.0000\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2460699102256843362140160.0000 - MAE: 1568661438464.0000 - val_loss: 2456073581180360692072448.0000 - val_MAE: 1567186485248.0000\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2452836754056176969187328.0000 - MAE: 1566153375744.0000 - val_loss: 2448226220959254188130304.0000 - val_MAE: 1564680912896.0000\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2444999481898235775156224.0000 - MAE: 1563649245184.0000 - val_loss: 2440403360320120579686400.0000 - val_MAE: 1562179141632.0000\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2437186997552643628335104.0000 - MAE: 1561149046784.0000 - val_loss: 2432605863954088321875968.0000 - val_MAE: 1559681302528.0000\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2429399877480152832147456.0000 - MAE: 1558653173760.0000 - val_loss: 2424833155400405111275520.0000 - val_MAE: 1557187395584.0000\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2421637256989634931458048.0000 - MAE: 1556160839680.0000 - val_loss: 2417084946428694796173312.0000 - val_MAE: 1554697682944.0000\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2413899568426654153834496.0000 - MAE: 1553672830976.0000 - val_loss: 2409362389960461983416320.0000 - val_MAE: 1552211902464.0000\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2406187100021586650988544.0000 - MAE: 1551188623360.0000 - val_loss: 2401663612498261686878208.0000 - val_MAE: 1549730054144.0000\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2398498410622551664361472.0000 - MAE: 1548708478976.0000 - val_loss: 2393990055193974665117696.0000 - val_MAE: 1547252400128.0000\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2390834941381429952512000.0000 - MAE: 1546232266752.0000 - val_loss: 2386340276895720159576064.0000 - val_MAE: 1544778416128.0000\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2383195395261528832737280.0000 - MAE: 1543760117760.0000 - val_loss: 2378715286409814701244416.0000 - val_MAE: 1542308495360.0000\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2375580781069164836028416.0000 - MAE: 1541291900928.0000 - val_loss: 2371115227851446365978624.0000 - val_MAE: 1539842506752.0000\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2367990378228397583106048.0000 - MAE: 1538827485184.0000 - val_loss: 2363538948299110546931712.0000 - val_MAE: 1537380450304.0000\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2360424330854415149826048.0000 - MAE: 1536367001600.0000 - val_loss: 2355986880098371471671296.0000 - val_MAE: 1534922457088.0000\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2352881918371277156909056.0000 - MAE: 1533910319104.0000 - val_loss: 2348459023249229140197376.0000 - val_MAE: 1532468396032.0000\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2345364149585300135346176.0000 - MAE: 1531458093056.0000 - val_loss: 2340955665982059704221696.0000 - val_MAE: 1530018136064.0000\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2337870159805355630002176.0000 - MAE: 1529009274880.0000 - val_loss: 2333475655375358556897280.0000 - val_MAE: 1527571677184.0000\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2330400525492195944300544.0000 - MAE: 1526564782080.0000 - val_loss: 2326019712005066077503488.0000 - val_MAE: 1525129412608.0000\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2322954381954692623106048.0000 - MAE: 1524123828224.0000 - val_loss: 2318587835871182266040320.0000 - val_MAE: 1522691080192.0000\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2315532305653597969842176.0000 - MAE: 1521687068672.0000 - val_loss: 2311179018167390591516672.0000 - val_MAE: 1520256286720.0000\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2308133287782595453517824.0000 - MAE: 1519253979136.0000 - val_loss: 2303794844160759888347136.0000 - val_MAE: 1517825687552.0000\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2300758481263189680979968.0000 - MAE: 1516824952832.0000 - val_loss: 2296433440353845170405376.0000 - val_MAE: 1515398758400.0000\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2293407165519440272949248.0000 - MAE: 1514399858688.0000 - val_loss: 2289096103783339120394240.0000 - val_MAE: 1512976023552.0000\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2286079196436159153569792.0000 - MAE: 1511978565632.0000 - val_loss: 2281781969758113283178496.0000 - val_MAE: 1510556827648.0000\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2278774718128534398697472.0000 - MAE: 1509560942592.0000 - val_loss: 2274491038278167658758144.0000 - val_MAE: 1508141563904.0000\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2271493586481377932476416.0000 - MAE: 1507147251712.0000 - val_loss: 2267224029919442626412544.0000 - val_MAE: 1505730232320.0000\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2264235945609877830762496.0000 - MAE: 1504737624064.0000 - val_loss: 2259979503530057427582976.0000 - val_MAE: 1503322701824.0000\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2257001074938093714276352.0000 - MAE: 1502331797504.0000 - val_loss: 2252758179685952441548800.0000 - val_MAE: 1500919103488.0000\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2249789262696401734729728.0000 - MAE: 1499929509888.0000 - val_loss: 2245560634847879971733504.0000 - val_MAE: 1498519437312.0000\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2242601085345554195546112.0000 - MAE: 1497531416576.0000 - val_loss: 2238385139633583107866624.0000 - val_MAE: 1496123441152.0000\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2235435101733670338166784.0000 - MAE: 1495137124352.0000 - val_loss: 2231233135194942608506880.0000 - val_MAE: 1493731246080.0000\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2228292608897442845294592.0000 - MAE: 1492746633216.0000 - val_loss: 2224103900956018094374912.0000 - val_MAE: 1491342983168.0000\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2221172742145743261794304.0000 - MAE: 1490359812096.0000 - val_loss: 2216997292801621489614848.0000 - val_MAE: 1488958521344.0000\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2214075501478571587665920.0000 - MAE: 1487976792064.0000 - val_loss: 2209913454846940870082560.0000 - val_MAE: 1486577729536.0000\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2207001031011115898765312.0000 - MAE: 1485597835264.0000 - val_loss: 2202852098861600084066304.0000 - val_MAE: 1484200869888.0000\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2199949186628188119236608.0000 - MAE: 1483222417408.0000 - val_loss: 2195813657191163359133696.0000 - val_MAE: 1481827811328.0000\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 2192919824214600173223936.0000 - MAE: 1480850800640.0000 - val_loss: 2188797697490066467717120.0000 - val_MAE: 1479458553856.0000\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2185913087885540136583168.0000 - MAE: 1478483116032.0000 - val_loss: 2181804075643121333960704.0000 - val_MAE: 1477092966400.0000\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2178928689410631857602560.0000 - MAE: 1476119363584.0000 - val_loss: 2174832503419951806152704.0000 - val_MAE: 1474731311104.0000\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2171966340559499184570368.0000 - MAE: 1473759150080.0000 - val_loss: 2167883557281310187716608.0000 - val_MAE: 1472373456896.0000\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2165026473677706345054208.0000 - MAE: 1471402868736.0000 - val_loss: 2160956660766444175228928.0000 - val_MAE: 1470019403776.0000\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2158108800534877187342336.0000 - MAE: 1469049995264.0000 - val_loss: 2154052102105729920401408.0000 - val_MAE: 1467669020672.0000\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2151213465246199787290624.0000 - MAE: 1466701185024.0000 - val_loss: 2147169593068791271522304.0000 - val_MAE: 1465322307584.0000\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2144339747235733765619712.0000 - MAE: 1464356306944.0000 - val_loss: 2140308557194875925168128.0000 - val_MAE: 1462979264512.0000\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2137488078849043349897216.0000 - MAE: 1462014967808.0000 - val_loss: 2133470147405488488185856.0000 - val_MAE: 1460640284672.0000\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2130658604201316615979008.0000 - MAE: 1459677298688.0000 - val_loss: 2126653066663936277872640.0000 - val_MAE: 1458304843776.0000\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2123850314486237032873984.0000 - MAE: 1457343430656.0000 - val_loss: 2119858035546159673507840.0000 - val_MAE: 1455973203968.0000\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2117064074394933055717376.0000 - MAE: 1455013363712.0000 - val_loss: 2113084477591406371667968.0000 - val_MAE: 1453645234176.0000\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2110299739812216608653312.0000 - MAE: 1452686835712.0000 - val_loss: 2106332825145240599920640.0000 - val_MAE: 1451321196544.0000\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 2103556734277335388258304.0000 - MAE: 1450364239872.0000 - val_loss: 2099602645862098130698240.0000 - val_MAE: 1449000566784.0000\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2096835634251041697955840.0000 - MAE: 1448045314048.0000 - val_loss: 2092893795626790888144896.0000 - val_MAE: 1446683738112.0000\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2090135719157395158466560.0000 - MAE: 1445730058240.0000 - val_loss: 2086206706784883099828224.0000 - val_MAE: 1444370710528.0000\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2083457421341959997358080.0000 - MAE: 1443418603520.0000 - val_loss: 2079541091105998614036480.0000 - val_MAE: 1442061352960.0000\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2076800308459171987062784.0000 - MAE: 1441110687744.0000 - val_loss: 2072896372129385127346176.0000 - val_MAE: 1439755796480.0000\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2070164524624219203436544.0000 - MAE: 1438806573056.0000 - val_loss: 2066273126315794943180800.0000 - val_MAE: 1437453647872.0000\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2063549925721913570623488.0000 - MAE: 1436505866240.0000 - val_loss: 2059670921319663833972736.0000 - val_MAE: 1435155431424.0000\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2056956511752255088623616.0000 - MAE: 1434209222656.0000 - val_loss: 2053089468910615648010240.0000 - val_MAE: 1432860622848.0000\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2050383994484867605725184.0000 - MAE: 1431916118016.0000 - val_loss: 2046529777894966916284416.0000 - val_MAE: 1430569746432.0000\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=sgd; total time=   6.2s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 22ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=sgd; total time=   1.8s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 22ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=sgd; total time=   1.8s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 22ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "5/5 [==============================] - 0s 5ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=3, model__n_neurons=125, model__optimizer=momentum; total time=   1.3s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 24ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=3, model__n_neurons=125, model__optimizer=momentum; total time=   1.4s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 23ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=3, model__n_neurons=125, model__optimizer=momentum; total time=   1.3s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 22ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.5, model__n_hidden=2, model__n_neurons=125, model__optimizer=nesterov; total time=   1.2s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 21ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.5, model__n_hidden=2, model__n_neurons=125, model__optimizer=nesterov; total time=   1.8s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 23ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.5, model__n_hidden=2, model__n_neurons=125, model__optimizer=nesterov; total time=   1.8s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 72ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=momentum; total time=   1.8s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 23ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=momentum; total time=   1.8s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 22ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=momentum; total time=   1.2s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 20ms/step - loss: 85264.5859 - MAE: 288.0197 - val_loss: 80950.3516 - val_MAE: 279.4344\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 85222.1328 - MAE: 287.9469 - val_loss: 80909.2812 - val_MAE: 279.3614\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 85179.5859 - MAE: 287.8737 - val_loss: 80868.2188 - val_MAE: 279.2883\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 85136.9922 - MAE: 287.8008 - val_loss: 80827.2812 - val_MAE: 279.2154\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 85094.7812 - MAE: 287.7282 - val_loss: 80786.2344 - val_MAE: 279.1422\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 85052.0547 - MAE: 287.6549 - val_loss: 80745.3125 - val_MAE: 279.0693\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 85009.7656 - MAE: 287.5822 - val_loss: 80704.3125 - val_MAE: 278.9962\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 84966.8359 - MAE: 287.5092 - val_loss: 80663.6172 - val_MAE: 278.9236\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 84924.9141 - MAE: 287.4365 - val_loss: 80622.5938 - val_MAE: 278.8505\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 84882.4141 - MAE: 287.3634 - val_loss: 80581.7812 - val_MAE: 278.7776\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 84840.1797 - MAE: 287.2910 - val_loss: 80540.9297 - val_MAE: 278.7047\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 84797.7422 - MAE: 287.2182 - val_loss: 80500.3359 - val_MAE: 278.6323\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 84755.4375 - MAE: 287.1454 - val_loss: 80459.6094 - val_MAE: 278.5596\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 84713.2812 - MAE: 287.0731 - val_loss: 80418.7812 - val_MAE: 278.4867\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 84670.6250 - MAE: 286.9999 - val_loss: 80378.0547 - val_MAE: 278.4140\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 84628.3125 - MAE: 286.9269 - val_loss: 80337.1328 - val_MAE: 278.3409\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 84586.0078 - MAE: 286.8544 - val_loss: 80296.2188 - val_MAE: 278.2677\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 84543.8516 - MAE: 286.7816 - val_loss: 80255.3516 - val_MAE: 278.1946\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 84501.4062 - MAE: 286.7085 - val_loss: 80214.8203 - val_MAE: 278.1222\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 84459.1797 - MAE: 286.6360 - val_loss: 80174.3594 - val_MAE: 278.0498\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 84416.9922 - MAE: 286.5634 - val_loss: 80133.7188 - val_MAE: 277.9771\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 84375.0078 - MAE: 286.4907 - val_loss: 80092.8359 - val_MAE: 277.9039\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 84332.8906 - MAE: 286.4179 - val_loss: 80051.9844 - val_MAE: 277.8308\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 84290.2734 - MAE: 286.3451 - val_loss: 80011.5156 - val_MAE: 277.7583\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 84248.4219 - MAE: 286.2725 - val_loss: 79970.8828 - val_MAE: 277.6855\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 84206.2656 - MAE: 286.1996 - val_loss: 79930.2500 - val_MAE: 277.6128\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 84163.9219 - MAE: 286.1271 - val_loss: 79889.7422 - val_MAE: 277.5402\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 84121.8594 - MAE: 286.0546 - val_loss: 79849.2578 - val_MAE: 277.4676\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 84080.3203 - MAE: 285.9821 - val_loss: 79808.3828 - val_MAE: 277.3943\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 84037.5547 - MAE: 285.9087 - val_loss: 79767.9688 - val_MAE: 277.3218\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 83995.6250 - MAE: 285.8364 - val_loss: 79727.4688 - val_MAE: 277.2492\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 83953.6797 - MAE: 285.7637 - val_loss: 79686.7500 - val_MAE: 277.1761\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 83911.8359 - MAE: 285.6910 - val_loss: 79645.9609 - val_MAE: 277.1029\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 83869.2734 - MAE: 285.6180 - val_loss: 79605.8125 - val_MAE: 277.0308\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 83827.4219 - MAE: 285.5459 - val_loss: 79565.6562 - val_MAE: 276.9587\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 83785.7188 - MAE: 285.4731 - val_loss: 79525.2422 - val_MAE: 276.8861\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 83744.0156 - MAE: 285.4009 - val_loss: 79484.3359 - val_MAE: 276.8126\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 83701.4219 - MAE: 285.3277 - val_loss: 79443.9688 - val_MAE: 276.7400\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 83659.7656 - MAE: 285.2552 - val_loss: 79403.3906 - val_MAE: 276.6670\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 83617.5469 - MAE: 285.1825 - val_loss: 79363.0938 - val_MAE: 276.5946\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 83575.7031 - MAE: 285.1099 - val_loss: 79322.7344 - val_MAE: 276.5220\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 83533.6641 - MAE: 285.0373 - val_loss: 79282.5703 - val_MAE: 276.4497\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 83491.8516 - MAE: 284.9648 - val_loss: 79242.3828 - val_MAE: 276.3774\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 83450.2578 - MAE: 284.8929 - val_loss: 79201.9453 - val_MAE: 276.3046\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 83408.1328 - MAE: 284.8199 - val_loss: 79161.8438 - val_MAE: 276.2324\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 83366.7031 - MAE: 284.7478 - val_loss: 79121.4688 - val_MAE: 276.1596\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 83325.0312 - MAE: 284.6752 - val_loss: 79081.0469 - val_MAE: 276.0868\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 83282.8594 - MAE: 284.6025 - val_loss: 79041.0859 - val_MAE: 276.0148\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 83241.4297 - MAE: 284.5305 - val_loss: 79000.9375 - val_MAE: 275.9424\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 83199.6797 - MAE: 284.4582 - val_loss: 78960.7578 - val_MAE: 275.8700\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 83157.9062 - MAE: 284.3858 - val_loss: 78920.7031 - val_MAE: 275.7977\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 83116.3828 - MAE: 284.3135 - val_loss: 78880.4141 - val_MAE: 275.7250\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 83074.6562 - MAE: 284.2408 - val_loss: 78840.2344 - val_MAE: 275.6525\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 83032.8203 - MAE: 284.1683 - val_loss: 78800.0156 - val_MAE: 275.5799\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 82991.3906 - MAE: 284.0959 - val_loss: 78759.6328 - val_MAE: 275.5070\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 82949.2500 - MAE: 284.0230 - val_loss: 78719.6562 - val_MAE: 275.4348\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 82907.8203 - MAE: 283.9509 - val_loss: 78679.4688 - val_MAE: 275.3622\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 82865.9844 - MAE: 283.8785 - val_loss: 78639.4688 - val_MAE: 275.2899\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 82824.2422 - MAE: 283.8059 - val_loss: 78599.5156 - val_MAE: 275.2176\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 82783.2891 - MAE: 283.7339 - val_loss: 78559.0156 - val_MAE: 275.1444\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 82740.9844 - MAE: 283.6608 - val_loss: 78519.0938 - val_MAE: 275.0722\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 82699.5625 - MAE: 283.5886 - val_loss: 78479.0391 - val_MAE: 274.9997\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 82657.8984 - MAE: 283.5160 - val_loss: 78438.9375 - val_MAE: 274.9272\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 82616.2500 - MAE: 283.4436 - val_loss: 78398.9062 - val_MAE: 274.8547\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 82574.7188 - MAE: 283.3711 - val_loss: 78358.7969 - val_MAE: 274.7821\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 82533.1094 - MAE: 283.2987 - val_loss: 78318.6953 - val_MAE: 274.7094\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 82491.6719 - MAE: 283.2261 - val_loss: 78278.4375 - val_MAE: 274.6365\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 82449.7812 - MAE: 283.1535 - val_loss: 78238.4609 - val_MAE: 274.5640\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 82408.7422 - MAE: 283.0810 - val_loss: 78198.2031 - val_MAE: 274.4911\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 82366.6562 - MAE: 283.0083 - val_loss: 78158.3984 - val_MAE: 274.4189\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 82325.2422 - MAE: 282.9359 - val_loss: 78118.6406 - val_MAE: 274.3468\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 82283.8516 - MAE: 282.8639 - val_loss: 78078.5859 - val_MAE: 274.2741\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 82242.4531 - MAE: 282.7912 - val_loss: 78038.6406 - val_MAE: 274.2017\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 82200.7031 - MAE: 282.7189 - val_loss: 77998.8203 - val_MAE: 274.1294\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 82159.7812 - MAE: 282.6467 - val_loss: 77958.6797 - val_MAE: 274.0565\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 82117.9609 - MAE: 282.5741 - val_loss: 77918.9141 - val_MAE: 273.9843\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 82076.4062 - MAE: 282.5017 - val_loss: 77879.1641 - val_MAE: 273.9121\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 82035.3672 - MAE: 282.4293 - val_loss: 77839.1406 - val_MAE: 273.8394\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 81993.7969 - MAE: 282.3568 - val_loss: 77799.2422 - val_MAE: 273.7668\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 81952.3438 - MAE: 282.2842 - val_loss: 77759.3359 - val_MAE: 273.6942\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 81911.0938 - MAE: 282.2119 - val_loss: 77719.4609 - val_MAE: 273.6217\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 81869.5859 - MAE: 282.1396 - val_loss: 77679.8047 - val_MAE: 273.5496\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 81828.6953 - MAE: 282.0675 - val_loss: 77639.8906 - val_MAE: 273.4769\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 81787.2188 - MAE: 281.9951 - val_loss: 77600.2891 - val_MAE: 273.4049\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 81746.1484 - MAE: 281.9231 - val_loss: 77560.7031 - val_MAE: 273.3328\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 81704.9219 - MAE: 281.8509 - val_loss: 77521.1641 - val_MAE: 273.2608\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 81663.6719 - MAE: 281.7789 - val_loss: 77481.6328 - val_MAE: 273.1888\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 81622.8750 - MAE: 281.7072 - val_loss: 77441.8438 - val_MAE: 273.1163\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 81581.5703 - MAE: 281.6350 - val_loss: 77402.1797 - val_MAE: 273.0440\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 81540.4141 - MAE: 281.5626 - val_loss: 77362.7109 - val_MAE: 272.9720\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 81499.4844 - MAE: 281.4908 - val_loss: 77323.0391 - val_MAE: 272.8997\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 81457.9531 - MAE: 281.4184 - val_loss: 77283.7578 - val_MAE: 272.8281\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 81416.9219 - MAE: 281.3466 - val_loss: 77244.2266 - val_MAE: 272.7559\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 81376.0156 - MAE: 281.2744 - val_loss: 77204.5391 - val_MAE: 272.6835\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 81335.1016 - MAE: 281.2020 - val_loss: 77164.7031 - val_MAE: 272.6108\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 81293.6094 - MAE: 281.1295 - val_loss: 77125.2031 - val_MAE: 272.5387\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 81252.7109 - MAE: 281.0576 - val_loss: 77085.5312 - val_MAE: 272.4662\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 81211.7031 - MAE: 280.9850 - val_loss: 77045.7656 - val_MAE: 272.3936\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 81170.1562 - MAE: 280.9126 - val_loss: 77006.3125 - val_MAE: 272.3214\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 81129.2891 - MAE: 280.8406 - val_loss: 76966.7734 - val_MAE: 272.2492\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=0, model__n_neurons=25, model__optimizer=adam; total time=   6.0s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 8332.5273 - MAE: 88.6437 - val_loss: 7107.7837 - val_MAE: 82.2639\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8319.8779 - MAE: 88.5714 - val_loss: 7095.8740 - val_MAE: 82.1908\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8307.1758 - MAE: 88.4994 - val_loss: 7083.9966 - val_MAE: 82.1178\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8294.5645 - MAE: 88.4273 - val_loss: 7072.1211 - val_MAE: 82.0448\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8281.8535 - MAE: 88.3550 - val_loss: 7060.2993 - val_MAE: 81.9720\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8269.2500 - MAE: 88.2829 - val_loss: 7048.4990 - val_MAE: 81.8993\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 8256.5898 - MAE: 88.2110 - val_loss: 7036.7217 - val_MAE: 81.8267\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8243.9795 - MAE: 88.1395 - val_loss: 7024.9438 - val_MAE: 81.7539\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8231.4297 - MAE: 88.0672 - val_loss: 7013.1646 - val_MAE: 81.6811\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8218.8994 - MAE: 87.9954 - val_loss: 7001.3638 - val_MAE: 81.6082\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8206.3223 - MAE: 87.9233 - val_loss: 6989.5781 - val_MAE: 81.5352\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8193.7383 - MAE: 87.8512 - val_loss: 6977.8657 - val_MAE: 81.4626\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8181.1675 - MAE: 87.7793 - val_loss: 6966.1445 - val_MAE: 81.3899\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8168.6660 - MAE: 87.7079 - val_loss: 6954.4053 - val_MAE: 81.3170\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8156.1719 - MAE: 87.6358 - val_loss: 6942.7075 - val_MAE: 81.2444\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8143.6572 - MAE: 87.5636 - val_loss: 6931.0288 - val_MAE: 81.1718\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8131.1729 - MAE: 87.4921 - val_loss: 6919.3647 - val_MAE: 81.0992\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8118.7520 - MAE: 87.4204 - val_loss: 6907.6885 - val_MAE: 81.0264\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8106.1929 - MAE: 87.3487 - val_loss: 6896.1226 - val_MAE: 80.9543\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8093.8330 - MAE: 87.2768 - val_loss: 6884.5029 - val_MAE: 80.8818\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 8081.4443 - MAE: 87.2052 - val_loss: 6872.8589 - val_MAE: 80.8090\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8069.0332 - MAE: 87.1334 - val_loss: 6861.2368 - val_MAE: 80.7363\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8056.6406 - MAE: 87.0615 - val_loss: 6849.6289 - val_MAE: 80.6637\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8044.2563 - MAE: 86.9898 - val_loss: 6838.0620 - val_MAE: 80.5912\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8031.8994 - MAE: 86.9183 - val_loss: 6826.5195 - val_MAE: 80.5188\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8019.5879 - MAE: 86.8466 - val_loss: 6814.9722 - val_MAE: 80.4463\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8007.2422 - MAE: 86.7750 - val_loss: 6803.4800 - val_MAE: 80.3741\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7994.9380 - MAE: 86.7041 - val_loss: 6792.0586 - val_MAE: 80.3023\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 7982.7983 - MAE: 86.6328 - val_loss: 6780.5854 - val_MAE: 80.2300\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 7970.5786 - MAE: 86.5617 - val_loss: 6769.1387 - val_MAE: 80.1579\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7958.2808 - MAE: 86.4902 - val_loss: 6757.7568 - val_MAE: 80.0861\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 7946.1255 - MAE: 86.4192 - val_loss: 6746.3530 - val_MAE: 80.0141\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7933.9673 - MAE: 86.3479 - val_loss: 6734.9263 - val_MAE: 79.9419\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7921.7070 - MAE: 86.2765 - val_loss: 6723.5977 - val_MAE: 79.8703\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7909.5269 - MAE: 86.2062 - val_loss: 6712.2769 - val_MAE: 79.7986\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 7897.4766 - MAE: 86.1346 - val_loss: 6700.8716 - val_MAE: 79.7264\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7885.2959 - MAE: 86.0635 - val_loss: 6689.4741 - val_MAE: 79.6541\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7873.1084 - MAE: 85.9922 - val_loss: 6678.1675 - val_MAE: 79.5823\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7861.0576 - MAE: 85.9215 - val_loss: 6666.8633 - val_MAE: 79.5104\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 7848.9214 - MAE: 85.8503 - val_loss: 6655.6001 - val_MAE: 79.4388\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7836.8433 - MAE: 85.7791 - val_loss: 6644.3379 - val_MAE: 79.3671\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 7824.7896 - MAE: 85.7083 - val_loss: 6633.0786 - val_MAE: 79.2954\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7812.6499 - MAE: 85.6375 - val_loss: 6621.8691 - val_MAE: 79.2239\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7800.6938 - MAE: 85.5669 - val_loss: 6610.5850 - val_MAE: 79.1518\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 7788.6592 - MAE: 85.4951 - val_loss: 6599.3257 - val_MAE: 79.0799\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 7776.6421 - MAE: 85.4245 - val_loss: 6588.1006 - val_MAE: 79.0081\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7764.6567 - MAE: 85.3534 - val_loss: 6576.8960 - val_MAE: 78.9363\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7752.5776 - MAE: 85.2829 - val_loss: 6565.8042 - val_MAE: 78.8652\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7740.7344 - MAE: 85.2125 - val_loss: 6554.6255 - val_MAE: 78.7935\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7728.7432 - MAE: 85.1415 - val_loss: 6543.4756 - val_MAE: 78.7219\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 7716.7964 - MAE: 85.0705 - val_loss: 6532.3354 - val_MAE: 78.6503\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7704.8711 - MAE: 84.9998 - val_loss: 6521.2031 - val_MAE: 78.5787\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7692.8936 - MAE: 84.9290 - val_loss: 6510.1284 - val_MAE: 78.5074\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7681.0518 - MAE: 84.8581 - val_loss: 6499.0020 - val_MAE: 78.4356\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 7669.1606 - MAE: 84.7875 - val_loss: 6487.8979 - val_MAE: 78.3640\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7657.3091 - MAE: 84.7169 - val_loss: 6476.8188 - val_MAE: 78.2924\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7645.4438 - MAE: 84.6461 - val_loss: 6465.7646 - val_MAE: 78.2210\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7633.5967 - MAE: 84.5755 - val_loss: 6454.7056 - val_MAE: 78.1494\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7621.6548 - MAE: 84.5048 - val_loss: 6443.7256 - val_MAE: 78.0783\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7609.9873 - MAE: 84.4342 - val_loss: 6432.6226 - val_MAE: 78.0063\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 7598.0981 - MAE: 84.3630 - val_loss: 6421.6074 - val_MAE: 77.9349\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7586.2861 - MAE: 84.2926 - val_loss: 6410.6538 - val_MAE: 77.8637\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7574.4487 - MAE: 84.2223 - val_loss: 6399.7627 - val_MAE: 77.7929\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7562.8408 - MAE: 84.1519 - val_loss: 6388.7842 - val_MAE: 77.7215\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 7551.0649 - MAE: 84.0816 - val_loss: 6377.8276 - val_MAE: 77.6501\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7539.3315 - MAE: 84.0115 - val_loss: 6366.9595 - val_MAE: 77.5792\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7527.6938 - MAE: 83.9408 - val_loss: 6356.0342 - val_MAE: 77.5079\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7515.9443 - MAE: 83.8709 - val_loss: 6345.1567 - val_MAE: 77.4369\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7504.3633 - MAE: 83.8003 - val_loss: 6334.2261 - val_MAE: 77.3654\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 7492.5601 - MAE: 83.7299 - val_loss: 6323.4092 - val_MAE: 77.2946\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7481.0137 - MAE: 83.6597 - val_loss: 6312.5317 - val_MAE: 77.2233\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7469.2847 - MAE: 83.5895 - val_loss: 6301.7373 - val_MAE: 77.1525\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7457.6812 - MAE: 83.5192 - val_loss: 6290.9619 - val_MAE: 77.0818\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7446.0649 - MAE: 83.4491 - val_loss: 6280.1738 - val_MAE: 77.0110\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7434.5649 - MAE: 83.3791 - val_loss: 6269.3091 - val_MAE: 76.9395\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7422.8208 - MAE: 83.3089 - val_loss: 6258.5913 - val_MAE: 76.8690\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 7411.3682 - MAE: 83.2391 - val_loss: 6247.8472 - val_MAE: 76.7981\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7399.8906 - MAE: 83.1684 - val_loss: 6237.0400 - val_MAE: 76.7269\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7388.2822 - MAE: 83.0985 - val_loss: 6226.2900 - val_MAE: 76.6559\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7376.7808 - MAE: 83.0284 - val_loss: 6215.5220 - val_MAE: 76.5847\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7365.2246 - MAE: 82.9582 - val_loss: 6204.8130 - val_MAE: 76.5139\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7353.6636 - MAE: 82.8883 - val_loss: 6194.2100 - val_MAE: 76.4437\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7342.3076 - MAE: 82.8186 - val_loss: 6183.5381 - val_MAE: 76.3729\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 7330.8345 - MAE: 82.7485 - val_loss: 6172.9082 - val_MAE: 76.3024\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7319.4712 - MAE: 82.6789 - val_loss: 6162.2412 - val_MAE: 76.2316\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7308.0347 - MAE: 82.6093 - val_loss: 6151.6558 - val_MAE: 76.1612\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7296.5610 - MAE: 82.5396 - val_loss: 6141.1050 - val_MAE: 76.0910\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7285.1836 - MAE: 82.4700 - val_loss: 6130.5151 - val_MAE: 76.0204\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 7273.8096 - MAE: 82.4002 - val_loss: 6119.9175 - val_MAE: 75.9498\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7262.4536 - MAE: 82.3303 - val_loss: 6109.3135 - val_MAE: 75.8790\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7251.1421 - MAE: 82.2608 - val_loss: 6098.7012 - val_MAE: 75.8081\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 7239.6724 - MAE: 82.1902 - val_loss: 6088.2148 - val_MAE: 75.7380\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 7228.2710 - MAE: 82.1213 - val_loss: 6077.7632 - val_MAE: 75.6681\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 7217.0991 - MAE: 82.0517 - val_loss: 6067.1821 - val_MAE: 75.5972\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 7205.8271 - MAE: 81.9814 - val_loss: 6056.6274 - val_MAE: 75.5265\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7194.4355 - MAE: 81.9120 - val_loss: 6046.1938 - val_MAE: 75.4564\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7183.1978 - MAE: 81.8423 - val_loss: 6035.7246 - val_MAE: 75.3861\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7172.0156 - MAE: 81.7730 - val_loss: 6025.2134 - val_MAE: 75.3154\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7160.6895 - MAE: 81.7030 - val_loss: 6014.7881 - val_MAE: 75.2452\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7149.4556 - MAE: 81.6340 - val_loss: 6004.3706 - val_MAE: 75.1750\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=0, model__n_neurons=25, model__optimizer=adam; total time=   6.1s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 9653.9238 - MAE: 92.5400 - val_loss: 9581.8799 - val_MAE: 91.6850\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9639.7539 - MAE: 92.4682 - val_loss: 9567.4023 - val_MAE: 91.6111\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9626.0537 - MAE: 92.3971 - val_loss: 9552.8057 - val_MAE: 91.5365\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 9612.1162 - MAE: 92.3251 - val_loss: 9538.2998 - val_MAE: 91.4623\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9598.2578 - MAE: 92.2537 - val_loss: 9523.7832 - val_MAE: 91.3880\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 9584.1963 - MAE: 92.1820 - val_loss: 9509.4014 - val_MAE: 91.3142\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9570.3291 - MAE: 92.1109 - val_loss: 9495.0146 - val_MAE: 91.2405\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9556.5898 - MAE: 92.0395 - val_loss: 9480.5664 - val_MAE: 91.1663\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9542.6230 - MAE: 91.9681 - val_loss: 9466.2207 - val_MAE: 91.0926\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9528.8740 - MAE: 91.8969 - val_loss: 9451.7676 - val_MAE: 91.0183\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9515.0068 - MAE: 91.8254 - val_loss: 9437.3838 - val_MAE: 90.9444\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9501.1963 - MAE: 91.7541 - val_loss: 9423.0723 - val_MAE: 90.8707\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9487.6709 - MAE: 91.6830 - val_loss: 9408.5713 - val_MAE: 90.7960\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 9473.7168 - MAE: 91.6111 - val_loss: 9394.2070 - val_MAE: 90.7220\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 9459.9502 - MAE: 91.5398 - val_loss: 9379.9121 - val_MAE: 90.6482\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9446.0684 - MAE: 91.4688 - val_loss: 9365.7988 - val_MAE: 90.5752\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9432.4092 - MAE: 91.3979 - val_loss: 9351.5771 - val_MAE: 90.5017\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9418.8867 - MAE: 91.3269 - val_loss: 9337.1816 - val_MAE: 90.4273\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9405.1299 - MAE: 91.2554 - val_loss: 9322.8164 - val_MAE: 90.3529\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 9391.3145 - MAE: 91.1837 - val_loss: 9308.5430 - val_MAE: 90.2790\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 9377.6143 - MAE: 91.1127 - val_loss: 9294.3359 - val_MAE: 90.2054\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 9364.0029 - MAE: 91.0415 - val_loss: 9280.1123 - val_MAE: 90.1316\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 9350.3096 - MAE: 90.9705 - val_loss: 9265.9912 - val_MAE: 90.0583\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9336.8652 - MAE: 90.8997 - val_loss: 9251.8262 - val_MAE: 89.9846\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9323.1973 - MAE: 90.8287 - val_loss: 9237.7852 - val_MAE: 89.9116\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9309.5459 - MAE: 90.7584 - val_loss: 9223.8828 - val_MAE: 89.8391\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9296.2646 - MAE: 90.6880 - val_loss: 9209.7168 - val_MAE: 89.7653\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9282.8320 - MAE: 90.6171 - val_loss: 9195.5010 - val_MAE: 89.6912\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9269.0615 - MAE: 90.5460 - val_loss: 9181.6045 - val_MAE: 89.6187\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9255.5840 - MAE: 90.4757 - val_loss: 9167.6738 - val_MAE: 89.5459\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9242.3076 - MAE: 90.4055 - val_loss: 9153.5596 - val_MAE: 89.4721\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9228.9209 - MAE: 90.3345 - val_loss: 9139.4688 - val_MAE: 89.3984\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9215.3135 - MAE: 90.2635 - val_loss: 9125.5137 - val_MAE: 89.3254\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 9201.9043 - MAE: 90.1929 - val_loss: 9111.5547 - val_MAE: 89.2523\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 9188.5039 - MAE: 90.1226 - val_loss: 9097.6406 - val_MAE: 89.1793\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9175.0352 - MAE: 90.0521 - val_loss: 9083.7998 - val_MAE: 89.1066\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9161.5518 - MAE: 89.9816 - val_loss: 9069.9424 - val_MAE: 89.0338\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9148.2627 - MAE: 89.9113 - val_loss: 9055.9336 - val_MAE: 88.9602\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9135.0010 - MAE: 89.8404 - val_loss: 9041.7959 - val_MAE: 88.8859\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 9121.4658 - MAE: 89.7691 - val_loss: 9027.8486 - val_MAE: 88.8125\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9107.9346 - MAE: 89.6984 - val_loss: 9014.0957 - val_MAE: 88.7401\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 9094.6230 - MAE: 89.6282 - val_loss: 9000.3164 - val_MAE: 88.6674\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9081.4834 - MAE: 89.5579 - val_loss: 8986.3789 - val_MAE: 88.5939\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9068.0557 - MAE: 89.4871 - val_loss: 8972.5537 - val_MAE: 88.5209\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9054.8730 - MAE: 89.4166 - val_loss: 8958.6572 - val_MAE: 88.4475\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9041.3486 - MAE: 89.3460 - val_loss: 8944.9775 - val_MAE: 88.3751\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 9028.2559 - MAE: 89.2758 - val_loss: 8931.1250 - val_MAE: 88.3017\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9015.0225 - MAE: 89.2052 - val_loss: 8917.2578 - val_MAE: 88.2283\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9001.8760 - MAE: 89.1347 - val_loss: 8903.3477 - val_MAE: 88.1546\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8988.2666 - MAE: 89.0638 - val_loss: 8889.7500 - val_MAE: 88.0824\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8975.1855 - MAE: 88.9938 - val_loss: 8876.0078 - val_MAE: 88.0094\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8961.8975 - MAE: 88.9233 - val_loss: 8862.2744 - val_MAE: 87.9364\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8948.8789 - MAE: 88.8532 - val_loss: 8848.4375 - val_MAE: 87.8629\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8935.6641 - MAE: 88.7823 - val_loss: 8834.7246 - val_MAE: 87.7899\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8922.3848 - MAE: 88.7121 - val_loss: 8821.1904 - val_MAE: 87.7178\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8909.4307 - MAE: 88.6422 - val_loss: 8807.5146 - val_MAE: 87.6449\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8896.3037 - MAE: 88.5720 - val_loss: 8793.8486 - val_MAE: 87.5720\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8883.1758 - MAE: 88.5017 - val_loss: 8780.2539 - val_MAE: 87.4994\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8870.0898 - MAE: 88.4317 - val_loss: 8766.7354 - val_MAE: 87.4271\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 8857.0664 - MAE: 88.3621 - val_loss: 8753.1924 - val_MAE: 87.3547\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8844.2129 - MAE: 88.2922 - val_loss: 8739.5459 - val_MAE: 87.2816\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8831.0059 - MAE: 88.2219 - val_loss: 8726.1465 - val_MAE: 87.2097\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8817.9922 - MAE: 88.1525 - val_loss: 8712.7598 - val_MAE: 87.1379\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8805.2773 - MAE: 88.0830 - val_loss: 8699.0762 - val_MAE: 87.0646\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8791.9160 - MAE: 88.0123 - val_loss: 8685.7148 - val_MAE: 86.9928\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8779.0801 - MAE: 87.9429 - val_loss: 8672.2109 - val_MAE: 86.9202\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8766.1318 - MAE: 87.8729 - val_loss: 8658.6982 - val_MAE: 86.8475\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8753.1465 - MAE: 87.8028 - val_loss: 8645.1953 - val_MAE: 86.7749\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8740.1289 - MAE: 87.7326 - val_loss: 8631.6738 - val_MAE: 86.7020\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8727.2178 - MAE: 87.6626 - val_loss: 8618.2490 - val_MAE: 86.6297\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 8714.4111 - MAE: 87.5929 - val_loss: 8604.7861 - val_MAE: 86.5570\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8701.4111 - MAE: 87.5228 - val_loss: 8591.4766 - val_MAE: 86.4851\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8688.6357 - MAE: 87.4537 - val_loss: 8578.1338 - val_MAE: 86.4130\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8675.6533 - MAE: 87.3838 - val_loss: 8564.8213 - val_MAE: 86.3410\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8662.8926 - MAE: 87.3141 - val_loss: 8551.3525 - val_MAE: 86.2681\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8649.9199 - MAE: 87.2441 - val_loss: 8538.0117 - val_MAE: 86.1958\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8637.0215 - MAE: 87.1743 - val_loss: 8524.7080 - val_MAE: 86.1236\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8624.2432 - MAE: 87.1046 - val_loss: 8511.3877 - val_MAE: 86.0513\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8611.4619 - MAE: 87.0349 - val_loss: 8498.1006 - val_MAE: 85.9792\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8598.7422 - MAE: 86.9652 - val_loss: 8484.6943 - val_MAE: 85.9063\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8586.0215 - MAE: 86.8950 - val_loss: 8471.2354 - val_MAE: 85.8332\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8573.0049 - MAE: 86.8249 - val_loss: 8458.0557 - val_MAE: 85.7614\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8560.2871 - MAE: 86.7556 - val_loss: 8444.9238 - val_MAE: 85.6898\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 8547.6357 - MAE: 86.6864 - val_loss: 8431.8174 - val_MAE: 85.6183\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8534.8330 - MAE: 86.6173 - val_loss: 8418.7891 - val_MAE: 85.5471\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8522.2705 - MAE: 86.5485 - val_loss: 8405.6250 - val_MAE: 85.4752\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8509.7275 - MAE: 86.4791 - val_loss: 8392.3086 - val_MAE: 85.4024\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8496.8848 - MAE: 86.4090 - val_loss: 8379.1387 - val_MAE: 85.3304\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8484.1953 - MAE: 86.3394 - val_loss: 8365.9814 - val_MAE: 85.2583\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8471.4688 - MAE: 86.2700 - val_loss: 8352.8506 - val_MAE: 85.1864\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8458.8896 - MAE: 86.2005 - val_loss: 8339.6133 - val_MAE: 85.1139\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8446.2939 - MAE: 86.1306 - val_loss: 8326.4648 - val_MAE: 85.0417\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8433.7109 - MAE: 86.0612 - val_loss: 8313.3760 - val_MAE: 84.9698\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 8420.8760 - MAE: 85.9918 - val_loss: 8300.5010 - val_MAE: 84.8990\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8408.5898 - MAE: 85.9230 - val_loss: 8287.2578 - val_MAE: 84.8261\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8395.7578 - MAE: 85.8532 - val_loss: 8274.2207 - val_MAE: 84.7543\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8383.4404 - MAE: 85.7838 - val_loss: 8261.0459 - val_MAE: 84.6818\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8370.5518 - MAE: 85.7142 - val_loss: 8248.1982 - val_MAE: 84.6108\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8358.3076 - MAE: 85.6457 - val_loss: 8235.1846 - val_MAE: 84.5390\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8345.6582 - MAE: 85.5765 - val_loss: 8222.4014 - val_MAE: 84.4683\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=0, model__n_neurons=25, model__optimizer=adam; total time=   6.2s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 24ms/step - loss: 596.1724 - MAE: 22.6477 - val_loss: 410.8729 - val_MAE: 19.3070\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 534.3977 - MAE: 21.2413 - val_loss: 363.2057 - val_MAE: 18.0303\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 474.2222 - MAE: 19.7558 - val_loss: 301.5981 - val_MAE: 16.2322\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 395.4970 - MAE: 17.6529 - val_loss: 222.6779 - val_MAE: 13.5855\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 299.0105 - MAE: 14.7147 - val_loss: 141.6777 - val_MAE: 10.4064\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 207.0314 - MAE: 11.3756 - val_loss: 80.5685 - val_MAE: 7.5313\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 140.7322 - MAE: 8.7295 - val_loss: 48.6244 - val_MAE: 5.7569\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 104.8068 - MAE: 7.0301 - val_loss: 38.6871 - val_MAE: 5.1267\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 89.9001 - MAE: 6.4026 - val_loss: 38.8172 - val_MAE: 5.2686\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 84.9127 - MAE: 6.3102 - val_loss: 40.4251 - val_MAE: 5.3443\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 83.8671 - MAE: 6.3314 - val_loss: 42.1981 - val_MAE: 5.4146\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 83.5092 - MAE: 6.3674 - val_loss: 43.1695 - val_MAE: 5.4568\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 83.4769 - MAE: 6.3922 - val_loss: 44.2239 - val_MAE: 5.4981\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 83.7822 - MAE: 6.4492 - val_loss: 43.9555 - val_MAE: 5.4880\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 83.3813 - MAE: 6.4350 - val_loss: 43.8317 - val_MAE: 5.4832\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 83.6154 - MAE: 6.4457 - val_loss: 42.9828 - val_MAE: 5.4490\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 83.4865 - MAE: 6.3873 - val_loss: 43.6815 - val_MAE: 5.4773\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 83.4548 - MAE: 6.4079 - val_loss: 44.0951 - val_MAE: 5.4933\n",
            "Epoch 18: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.1, model__n_hidden=3, model__n_neurons=5, model__optimizer=nesterov; total time=   1.8s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 24ms/step - loss: 630.3605 - MAE: 22.9698 - val_loss: 434.5841 - val_MAE: 19.9116\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 613.7763 - MAE: 22.6044 - val_loss: 420.3672 - val_MAE: 19.5513\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 597.7112 - MAE: 22.2492 - val_loss: 406.4904 - val_MAE: 19.1932\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 582.0066 - MAE: 21.8914 - val_loss: 393.1120 - val_MAE: 18.8414\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 566.8327 - MAE: 21.5445 - val_loss: 380.2551 - val_MAE: 18.4971\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 552.2293 - MAE: 21.1962 - val_loss: 367.9045 - val_MAE: 18.1602\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 538.1367 - MAE: 20.8650 - val_loss: 355.9758 - val_MAE: 17.8287\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 524.5020 - MAE: 20.5436 - val_loss: 344.3621 - val_MAE: 17.5000\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 511.1781 - MAE: 20.2108 - val_loss: 333.0621 - val_MAE: 17.1741\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 498.1984 - MAE: 19.8861 - val_loss: 322.1698 - val_MAE: 16.8540\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 485.6625 - MAE: 19.5687 - val_loss: 311.7773 - val_MAE: 16.5428\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 473.6483 - MAE: 19.2629 - val_loss: 301.6292 - val_MAE: 16.2332\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 461.8950 - MAE: 18.9547 - val_loss: 291.8484 - val_MAE: 15.9291\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 450.5450 - MAE: 18.6523 - val_loss: 282.4687 - val_MAE: 15.6319\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 439.6059 - MAE: 18.3594 - val_loss: 273.3223 - val_MAE: 15.3365\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 428.9240 - MAE: 18.0604 - val_loss: 264.5447 - val_MAE: 15.0477\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 418.6323 - MAE: 17.7792 - val_loss: 255.9794 - val_MAE: 14.7603\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 408.5760 - MAE: 17.4914 - val_loss: 247.8089 - val_MAE: 14.4809\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 398.9377 - MAE: 17.2192 - val_loss: 239.8575 - val_MAE: 14.2037\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 389.5334 - MAE: 16.9391 - val_loss: 232.2211 - val_MAE: 13.9323\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 380.4738 - MAE: 16.6666 - val_loss: 224.8621 - val_MAE: 13.6656\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 371.7171 - MAE: 16.4086 - val_loss: 217.7899 - val_MAE: 13.4044\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 363.2643 - MAE: 16.1508 - val_loss: 210.9057 - val_MAE: 13.1549\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 355.0121 - MAE: 15.8941 - val_loss: 204.1885 - val_MAE: 12.9160\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 346.9375 - MAE: 15.6404 - val_loss: 197.8157 - val_MAE: 12.6848\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 339.2427 - MAE: 15.3988 - val_loss: 191.6268 - val_MAE: 12.4558\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 331.7643 - MAE: 15.1595 - val_loss: 185.7937 - val_MAE: 12.2358\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 324.6659 - MAE: 14.9313 - val_loss: 180.0938 - val_MAE: 12.0165\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 317.7016 - MAE: 14.7023 - val_loss: 174.5833 - val_MAE: 11.8003\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 310.9530 - MAE: 14.4860 - val_loss: 169.2456 - val_MAE: 11.5866\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 304.4005 - MAE: 14.2712 - val_loss: 164.1799 - val_MAE: 11.3798\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 298.1309 - MAE: 14.0673 - val_loss: 159.1279 - val_MAE: 11.1694\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 291.8691 - MAE: 13.8572 - val_loss: 154.3224 - val_MAE: 10.9651\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 285.8907 - MAE: 13.6559 - val_loss: 149.7006 - val_MAE: 10.7646\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 280.1066 - MAE: 13.4565 - val_loss: 145.1903 - val_MAE: 10.5649\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 274.4506 - MAE: 13.2586 - val_loss: 140.8802 - val_MAE: 10.3700\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 269.0192 - MAE: 13.0675 - val_loss: 136.7893 - val_MAE: 10.1813\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 263.8382 - MAE: 12.8822 - val_loss: 132.8365 - val_MAE: 10.0060\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 258.8019 - MAE: 12.7016 - val_loss: 128.9860 - val_MAE: 9.8357\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 253.8792 - MAE: 12.5278 - val_loss: 125.2609 - val_MAE: 9.6675\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 249.0989 - MAE: 12.3480 - val_loss: 121.6862 - val_MAE: 9.5027\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 244.4832 - MAE: 12.1769 - val_loss: 118.2532 - val_MAE: 9.3411\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 240.0262 - MAE: 12.0178 - val_loss: 114.9296 - val_MAE: 9.1813\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 235.6914 - MAE: 11.8546 - val_loss: 111.7118 - val_MAE: 9.0233\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 231.4873 - MAE: 11.6913 - val_loss: 108.6422 - val_MAE: 8.8751\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 227.4347 - MAE: 11.5432 - val_loss: 105.7116 - val_MAE: 8.7380\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 223.5556 - MAE: 11.3975 - val_loss: 102.9019 - val_MAE: 8.6037\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 219.8106 - MAE: 11.2549 - val_loss: 100.1203 - val_MAE: 8.4678\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 216.0750 - MAE: 11.1110 - val_loss: 97.4149 - val_MAE: 8.3327\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 212.4417 - MAE: 10.9670 - val_loss: 94.9009 - val_MAE: 8.2149\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 209.0270 - MAE: 10.8311 - val_loss: 92.4376 - val_MAE: 8.0986\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 205.6786 - MAE: 10.6963 - val_loss: 90.0697 - val_MAE: 7.9843\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 202.4141 - MAE: 10.5674 - val_loss: 87.7876 - val_MAE: 7.8717\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 199.2752 - MAE: 10.4417 - val_loss: 85.6376 - val_MAE: 7.7694\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 196.2735 - MAE: 10.3253 - val_loss: 83.4996 - val_MAE: 7.6706\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 193.2888 - MAE: 10.2069 - val_loss: 81.4532 - val_MAE: 7.5739\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 190.4090 - MAE: 10.0945 - val_loss: 79.5109 - val_MAE: 7.4832\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 187.6489 - MAE: 9.9846 - val_loss: 77.6437 - val_MAE: 7.4016\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 184.9829 - MAE: 9.8847 - val_loss: 75.8032 - val_MAE: 7.3193\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 182.3338 - MAE: 9.7771 - val_loss: 74.0478 - val_MAE: 7.2390\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 179.7987 - MAE: 9.6781 - val_loss: 72.4018 - val_MAE: 7.1618\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 177.3859 - MAE: 9.5868 - val_loss: 70.8194 - val_MAE: 7.0858\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 175.0539 - MAE: 9.4941 - val_loss: 69.2905 - val_MAE: 7.0107\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 172.7851 - MAE: 9.4104 - val_loss: 67.8175 - val_MAE: 6.9365\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 170.5927 - MAE: 9.3275 - val_loss: 66.4237 - val_MAE: 6.8646\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 168.4777 - MAE: 9.2428 - val_loss: 65.0452 - val_MAE: 6.7918\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 166.3875 - MAE: 9.1633 - val_loss: 63.7684 - val_MAE: 6.7226\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 164.4241 - MAE: 9.0897 - val_loss: 62.4873 - val_MAE: 6.6514\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 162.4471 - MAE: 9.0155 - val_loss: 61.2847 - val_MAE: 6.5829\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 160.5587 - MAE: 8.9459 - val_loss: 60.1262 - val_MAE: 6.5152\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 158.7395 - MAE: 8.8762 - val_loss: 59.0482 - val_MAE: 6.4506\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 157.0150 - MAE: 8.8143 - val_loss: 57.9746 - val_MAE: 6.3846\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 155.2883 - MAE: 8.7531 - val_loss: 56.9460 - val_MAE: 6.3196\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 153.6134 - MAE: 8.6908 - val_loss: 55.9503 - val_MAE: 6.2550\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 151.9799 - MAE: 8.6326 - val_loss: 55.0051 - val_MAE: 6.1920\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 150.4246 - MAE: 8.5783 - val_loss: 54.1264 - val_MAE: 6.1318\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 148.9328 - MAE: 8.5200 - val_loss: 53.2365 - val_MAE: 6.0692\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 147.4295 - MAE: 8.4649 - val_loss: 52.4400 - val_MAE: 6.0184\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 146.0553 - MAE: 8.4137 - val_loss: 51.6539 - val_MAE: 5.9677\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 144.6967 - MAE: 8.3656 - val_loss: 50.8955 - val_MAE: 5.9173\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 143.3606 - MAE: 8.3186 - val_loss: 50.1968 - val_MAE: 5.8696\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 142.1079 - MAE: 8.2716 - val_loss: 49.5109 - val_MAE: 5.8214\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 140.8680 - MAE: 8.2302 - val_loss: 48.8376 - val_MAE: 5.7727\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 139.6377 - MAE: 8.1842 - val_loss: 48.2199 - val_MAE: 5.7266\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 138.4870 - MAE: 8.1425 - val_loss: 47.6214 - val_MAE: 5.6806\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 137.3596 - MAE: 8.1018 - val_loss: 47.0578 - val_MAE: 5.6359\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 136.2807 - MAE: 8.0608 - val_loss: 46.5001 - val_MAE: 5.5903\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 135.1992 - MAE: 8.0257 - val_loss: 45.9633 - val_MAE: 5.5506\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 134.1417 - MAE: 7.9844 - val_loss: 45.4626 - val_MAE: 5.5185\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 133.1414 - MAE: 7.9489 - val_loss: 45.0045 - val_MAE: 5.4899\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 132.2037 - MAE: 7.9186 - val_loss: 44.5515 - val_MAE: 5.4606\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 131.2727 - MAE: 7.8854 - val_loss: 44.1270 - val_MAE: 5.4342\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 130.3657 - MAE: 7.8543 - val_loss: 43.7063 - val_MAE: 5.4115\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 129.4644 - MAE: 7.8219 - val_loss: 43.3252 - val_MAE: 5.3903\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 128.6251 - MAE: 7.7935 - val_loss: 42.9646 - val_MAE: 5.3694\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 127.8155 - MAE: 7.7615 - val_loss: 42.6200 - val_MAE: 5.3528\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 127.0284 - MAE: 7.7348 - val_loss: 42.2788 - val_MAE: 5.3376\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 126.2336 - MAE: 7.7047 - val_loss: 41.9712 - val_MAE: 5.3234\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 125.5026 - MAE: 7.6806 - val_loss: 41.6745 - val_MAE: 5.3092\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 124.7746 - MAE: 7.6554 - val_loss: 41.4059 - val_MAE: 5.2957\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.1, model__n_hidden=3, model__n_neurons=5, model__optimizer=nesterov; total time=   6.4s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 21ms/step - loss: 13972846.0000 - MAE: 1338.8156 - val_loss: 1759.6945 - val_MAE: 41.0841\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1748.3242 - MAE: 40.8730 - val_loss: 1706.3884 - val_MAE: 40.4301\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1695.6470 - MAE: 40.2215 - val_loss: 1654.7233 - val_MAE: 39.7861\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1644.5868 - MAE: 39.5855 - val_loss: 1604.7866 - val_MAE: 39.1535\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1595.2371 - MAE: 38.9536 - val_loss: 1556.4766 - val_MAE: 38.5316\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1547.4904 - MAE: 38.3352 - val_loss: 1509.6876 - val_MAE: 37.9196\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1501.2307 - MAE: 37.7291 - val_loss: 1464.1583 - val_MAE: 37.3144\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1456.2465 - MAE: 37.1227 - val_loss: 1420.2208 - val_MAE: 36.7210\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1412.8196 - MAE: 36.5375 - val_loss: 1377.7358 - val_MAE: 36.1378\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1370.8105 - MAE: 35.9593 - val_loss: 1336.4760 - val_MAE: 35.5624\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1330.0289 - MAE: 35.3875 - val_loss: 1296.5476 - val_MAE: 34.9965\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1290.5789 - MAE: 34.8253 - val_loss: 1258.0597 - val_MAE: 34.4422\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1252.5166 - MAE: 34.2749 - val_loss: 1220.6243 - val_MAE: 33.8944\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1215.5099 - MAE: 33.7309 - val_loss: 1184.3501 - val_MAE: 33.3550\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1179.6556 - MAE: 33.1906 - val_loss: 1149.2924 - val_MAE: 32.8253\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1144.9818 - MAE: 32.6656 - val_loss: 1115.1360 - val_MAE: 32.3008\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1111.2313 - MAE: 32.1411 - val_loss: 1082.2550 - val_MAE: 31.7878\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1078.7218 - MAE: 31.6334 - val_loss: 1050.4058 - val_MAE: 31.2828\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1047.2362 - MAE: 31.1351 - val_loss: 1019.6311 - val_MAE: 30.7870\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1016.7985 - MAE: 30.6424 - val_loss: 989.6973 - val_MAE: 30.2969\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 987.2121 - MAE: 30.1543 - val_loss: 960.8094 - val_MAE: 29.8164\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 958.6382 - MAE: 29.6798 - val_loss: 932.7567 - val_MAE: 29.3422\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 930.8965 - MAE: 29.2118 - val_loss: 905.5263 - val_MAE: 28.8744\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 903.9611 - MAE: 28.7436 - val_loss: 879.0620 - val_MAE: 28.4125\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 877.7971 - MAE: 28.2832 - val_loss: 853.4863 - val_MAE: 27.9588\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 852.5037 - MAE: 27.8341 - val_loss: 828.7510 - val_MAE: 27.5129\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 828.0471 - MAE: 27.3894 - val_loss: 804.8458 - val_MAE: 27.0750\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 804.4025 - MAE: 26.9536 - val_loss: 781.5964 - val_MAE: 26.6421\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 781.4136 - MAE: 26.5253 - val_loss: 759.1974 - val_MAE: 26.2184\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 759.2555 - MAE: 26.1020 - val_loss: 737.4166 - val_MAE: 25.7997\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 737.7177 - MAE: 25.6846 - val_loss: 716.3400 - val_MAE: 25.3879\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 716.8616 - MAE: 25.2808 - val_loss: 695.8951 - val_MAE: 24.9820\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 696.6564 - MAE: 24.8733 - val_loss: 676.2847 - val_MAE: 24.5864\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 677.2521 - MAE: 24.4804 - val_loss: 657.1975 - val_MAE: 24.1951\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 658.3593 - MAE: 24.0949 - val_loss: 638.6423 - val_MAE: 23.8086\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 640.0243 - MAE: 23.7088 - val_loss: 620.8920 - val_MAE: 23.4329\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 622.4684 - MAE: 23.3291 - val_loss: 603.6643 - val_MAE: 23.0623\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 605.4227 - MAE: 22.9641 - val_loss: 586.9763 - val_MAE: 22.6977\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 588.9039 - MAE: 22.6033 - val_loss: 570.7139 - val_MAE: 22.3365\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 572.8044 - MAE: 22.2476 - val_loss: 554.9601 - val_MAE: 21.9811\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 557.2234 - MAE: 21.8940 - val_loss: 539.7466 - val_MAE: 21.6322\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 542.1500 - MAE: 21.5506 - val_loss: 524.7454 - val_MAE: 21.2827\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 527.3202 - MAE: 21.2018 - val_loss: 510.4349 - val_MAE: 20.9438\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 513.1630 - MAE: 20.8632 - val_loss: 496.6224 - val_MAE: 20.6114\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 499.4801 - MAE: 20.5349 - val_loss: 483.1198 - val_MAE: 20.2812\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 486.1265 - MAE: 20.2055 - val_loss: 470.1626 - val_MAE: 19.9592\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 473.3082 - MAE: 19.8812 - val_loss: 457.6533 - val_MAE: 19.6433\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 460.9201 - MAE: 19.5729 - val_loss: 445.4830 - val_MAE: 19.3311\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 448.8723 - MAE: 19.2607 - val_loss: 433.6682 - val_MAE: 19.0230\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 437.1803 - MAE: 18.9534 - val_loss: 422.2452 - val_MAE: 18.7204\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 425.8745 - MAE: 18.6515 - val_loss: 411.2217 - val_MAE: 18.4236\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 414.9629 - MAE: 18.3582 - val_loss: 400.4942 - val_MAE: 18.1301\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 404.3311 - MAE: 18.0700 - val_loss: 390.0541 - val_MAE: 17.8399\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 393.9963 - MAE: 17.7788 - val_loss: 379.9559 - val_MAE: 17.5546\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 384.0019 - MAE: 17.4984 - val_loss: 370.2256 - val_MAE: 17.2752\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 374.3724 - MAE: 17.2195 - val_loss: 360.8301 - val_MAE: 17.0011\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 365.0591 - MAE: 16.9488 - val_loss: 351.6462 - val_MAE: 16.7368\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 355.9650 - MAE: 16.6781 - val_loss: 342.7678 - val_MAE: 16.4892\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 347.1721 - MAE: 16.4174 - val_loss: 334.1173 - val_MAE: 16.2439\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 338.6152 - MAE: 16.1537 - val_loss: 325.8797 - val_MAE: 16.0065\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 330.4443 - MAE: 15.9038 - val_loss: 317.7660 - val_MAE: 15.7690\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 322.4131 - MAE: 15.6472 - val_loss: 309.9901 - val_MAE: 15.5376\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 314.7150 - MAE: 15.4032 - val_loss: 302.5107 - val_MAE: 15.3115\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 307.3035 - MAE: 15.1685 - val_loss: 295.2155 - val_MAE: 15.0873\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 300.0793 - MAE: 14.9262 - val_loss: 288.1946 - val_MAE: 14.8681\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 293.1166 - MAE: 14.6968 - val_loss: 281.2762 - val_MAE: 14.6486\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 286.2644 - MAE: 14.4678 - val_loss: 274.6339 - val_MAE: 14.4344\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 279.6893 - MAE: 14.2388 - val_loss: 268.2788 - val_MAE: 14.2262\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 273.3786 - MAE: 14.0231 - val_loss: 261.9641 - val_MAE: 14.0160\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 267.1320 - MAE: 13.8002 - val_loss: 255.9936 - val_MAE: 13.8139\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 261.2062 - MAE: 13.5879 - val_loss: 250.1497 - val_MAE: 13.6130\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 255.4163 - MAE: 13.3784 - val_loss: 244.5193 - val_MAE: 13.4162\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 249.8419 - MAE: 13.1804 - val_loss: 239.0820 - val_MAE: 13.2231\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 244.4471 - MAE: 12.9807 - val_loss: 233.7897 - val_MAE: 13.0322\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 239.1995 - MAE: 12.7913 - val_loss: 228.6657 - val_MAE: 12.8443\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 234.1172 - MAE: 12.6033 - val_loss: 223.6826 - val_MAE: 12.6586\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 229.1817 - MAE: 12.4164 - val_loss: 218.8770 - val_MAE: 12.4767\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 224.4114 - MAE: 12.2327 - val_loss: 214.1709 - val_MAE: 12.2956\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 219.7581 - MAE: 12.0508 - val_loss: 209.7549 - val_MAE: 12.1229\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 215.3702 - MAE: 11.8827 - val_loss: 205.4271 - val_MAE: 11.9566\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 211.0711 - MAE: 11.7130 - val_loss: 201.1011 - val_MAE: 11.7959\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 206.7826 - MAE: 11.5396 - val_loss: 196.9681 - val_MAE: 11.6399\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 202.6799 - MAE: 11.3790 - val_loss: 192.9243 - val_MAE: 11.4847\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 198.6704 - MAE: 11.2152 - val_loss: 189.0763 - val_MAE: 11.3345\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 194.8591 - MAE: 11.0579 - val_loss: 185.3929 - val_MAE: 11.1885\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 191.2023 - MAE: 10.9088 - val_loss: 181.8098 - val_MAE: 11.0442\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 187.6468 - MAE: 10.7638 - val_loss: 178.3556 - val_MAE: 10.9028\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 184.2144 - MAE: 10.6221 - val_loss: 174.9892 - val_MAE: 10.7628\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 180.8744 - MAE: 10.4819 - val_loss: 171.7308 - val_MAE: 10.6251\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 177.6479 - MAE: 10.3476 - val_loss: 168.5902 - val_MAE: 10.4902\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 174.5194 - MAE: 10.2152 - val_loss: 165.4872 - val_MAE: 10.3548\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 171.4357 - MAE: 10.0846 - val_loss: 162.4346 - val_MAE: 10.2193\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 168.4101 - MAE: 9.9500 - val_loss: 159.5561 - val_MAE: 10.0895\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 165.5590 - MAE: 9.8250 - val_loss: 156.8114 - val_MAE: 9.9637\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 162.8273 - MAE: 9.7093 - val_loss: 154.0779 - val_MAE: 9.8413\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 160.1184 - MAE: 9.5904 - val_loss: 151.5151 - val_MAE: 9.7306\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 157.5661 - MAE: 9.4784 - val_loss: 148.9767 - val_MAE: 9.6191\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 155.0468 - MAE: 9.3649 - val_loss: 146.5175 - val_MAE: 9.5094\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 152.6055 - MAE: 9.2559 - val_loss: 144.1356 - val_MAE: 9.4014\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 150.2412 - MAE: 9.1547 - val_loss: 141.8677 - val_MAE: 9.2968\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.1, model__n_hidden=3, model__n_neurons=5, model__optimizer=nesterov; total time=   6.7s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 25ms/step - loss: 2383.3433 - MAE: 41.4853 - val_loss: 381.6716 - val_MAE: 17.9070\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 453.9774 - MAE: 18.4302 - val_loss: 362.3094 - val_MAE: 18.4737\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 216.7511 - MAE: 11.6068 - val_loss: 234.3449 - val_MAE: 13.7208\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 158.0123 - MAE: 10.2104 - val_loss: 21.1478 - val_MAE: 3.7116\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 101.2156 - MAE: 6.8302 - val_loss: 16.8060 - val_MAE: 3.2889\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 78.4131 - MAE: 6.8481 - val_loss: 40.3190 - val_MAE: 5.5610\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 67.2254 - MAE: 5.9299 - val_loss: 17.2741 - val_MAE: 3.1863\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 64.8308 - MAE: 5.4445 - val_loss: 24.6330 - val_MAE: 4.3656\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 65.7553 - MAE: 6.0792 - val_loss: 19.2627 - val_MAE: 3.7875\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 61.4496 - MAE: 5.4245 - val_loss: 16.5504 - val_MAE: 3.4585\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 11ms/step - loss: 60.7701 - MAE: 5.3973 - val_loss: 20.3597 - val_MAE: 3.9534\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 60.7336 - MAE: 5.5427 - val_loss: 17.2763 - val_MAE: 3.6042\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 60.5534 - MAE: 5.2513 - val_loss: 17.9631 - val_MAE: 3.6973\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 62.4619 - MAE: 5.9089 - val_loss: 16.7621 - val_MAE: 3.5277\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 61.2375 - MAE: 5.2234 - val_loss: 17.1302 - val_MAE: 3.5951\n",
            "Epoch 15: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.1, model__n_hidden=3, model__n_neurons=125, model__optimizer=adam; total time=   1.8s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 22ms/step - loss: 216.0649 - MAE: 11.4173 - val_loss: 32.5185 - val_MAE: 4.8344\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 96.0144 - MAE: 7.9759 - val_loss: 37.5348 - val_MAE: 4.8289\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 91.3267 - MAE: 7.1622 - val_loss: 14.4233 - val_MAE: 3.1889\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 71.7583 - MAE: 5.9791 - val_loss: 16.9751 - val_MAE: 3.5603\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 67.9604 - MAE: 5.9497 - val_loss: 17.0767 - val_MAE: 3.5951\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 65.1446 - MAE: 5.7099 - val_loss: 12.4336 - val_MAE: 3.0403\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 65.9793 - MAE: 5.8182 - val_loss: 12.0123 - val_MAE: 2.9609\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 67.3222 - MAE: 5.8432 - val_loss: 21.2824 - val_MAE: 4.0141\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 60.0210 - MAE: 5.2993 - val_loss: 38.5963 - val_MAE: 5.2469\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 65.2524 - MAE: 5.9388 - val_loss: 18.8095 - val_MAE: 3.7303\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 66.0465 - MAE: 6.1501 - val_loss: 11.6711 - val_MAE: 2.8388\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 56.7428 - MAE: 5.5149 - val_loss: 15.3064 - val_MAE: 3.2199\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 58.6272 - MAE: 5.5080 - val_loss: 26.8393 - val_MAE: 4.3552\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 55.7008 - MAE: 5.3260 - val_loss: 27.8433 - val_MAE: 4.4370\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 66.8278 - MAE: 5.9306 - val_loss: 33.7491 - val_MAE: 4.9054\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 59.4774 - MAE: 5.6812 - val_loss: 12.7167 - val_MAE: 2.8759\n",
            "Epoch 16: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.1, model__n_hidden=3, model__n_neurons=125, model__optimizer=adam; total time=   1.9s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 24ms/step - loss: 184.9302 - MAE: 10.5205 - val_loss: 68.7973 - val_MAE: 6.4804\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 96.5798 - MAE: 7.5046 - val_loss: 45.5121 - val_MAE: 5.4155\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 57.2975 - MAE: 5.7026 - val_loss: 44.8016 - val_MAE: 4.9887\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 55.5480 - MAE: 5.0815 - val_loss: 55.8828 - val_MAE: 5.7777\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 58.4570 - MAE: 5.4396 - val_loss: 37.9415 - val_MAE: 4.8945\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 52.5129 - MAE: 5.2086 - val_loss: 39.6164 - val_MAE: 4.6163\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 51.9557 - MAE: 5.0182 - val_loss: 36.5915 - val_MAE: 4.7150\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 49.1322 - MAE: 5.0093 - val_loss: 35.4331 - val_MAE: 4.4392\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 46.0117 - MAE: 4.6277 - val_loss: 32.0713 - val_MAE: 4.4125\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 51.3057 - MAE: 5.0262 - val_loss: 39.0071 - val_MAE: 4.7677\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 53.6837 - MAE: 5.2775 - val_loss: 41.1402 - val_MAE: 5.0638\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 54.8147 - MAE: 5.3481 - val_loss: 52.4734 - val_MAE: 5.8985\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 59.3040 - MAE: 5.6308 - val_loss: 47.1259 - val_MAE: 5.5729\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 55.8739 - MAE: 5.2685 - val_loss: 38.7270 - val_MAE: 4.9715\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 45.3366 - MAE: 4.8924 - val_loss: 25.4947 - val_MAE: 3.9526\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 40.1123 - MAE: 4.2722 - val_loss: 24.5406 - val_MAE: 3.7801\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 39.5169 - MAE: 4.4019 - val_loss: 33.5295 - val_MAE: 4.5664\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 45.7588 - MAE: 5.0670 - val_loss: 47.7111 - val_MAE: 5.5269\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 46.5780 - MAE: 5.1092 - val_loss: 33.3462 - val_MAE: 4.5759\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 43.9684 - MAE: 4.4685 - val_loss: 22.3187 - val_MAE: 3.7959\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 37.9292 - MAE: 4.1192 - val_loss: 21.9832 - val_MAE: 3.6982\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 36.0848 - MAE: 3.9302 - val_loss: 31.4282 - val_MAE: 4.4125\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 49.2575 - MAE: 4.9862 - val_loss: 36.4723 - val_MAE: 4.8721\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 40.9916 - MAE: 4.4704 - val_loss: 43.7345 - val_MAE: 5.4940\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 43.1919 - MAE: 4.6429 - val_loss: 22.7273 - val_MAE: 3.6707\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 34.9119 - MAE: 4.0363 - val_loss: 24.0564 - val_MAE: 3.8658\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 34.1972 - MAE: 4.1333 - val_loss: 18.3612 - val_MAE: 3.4576\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 32.1584 - MAE: 3.9129 - val_loss: 23.4007 - val_MAE: 3.9092\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 31.0525 - MAE: 3.8603 - val_loss: 25.4215 - val_MAE: 4.0038\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 33.8827 - MAE: 3.9598 - val_loss: 21.6883 - val_MAE: 3.7923\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 33.3322 - MAE: 4.0533 - val_loss: 37.8175 - val_MAE: 4.9685\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 39.0314 - MAE: 4.5103 - val_loss: 19.5300 - val_MAE: 3.2300\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 30.8893 - MAE: 3.9713 - val_loss: 23.2364 - val_MAE: 3.9398\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 30.5189 - MAE: 3.8241 - val_loss: 19.1402 - val_MAE: 3.3845\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 30.7477 - MAE: 3.9011 - val_loss: 16.2975 - val_MAE: 3.3889\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 27.0144 - MAE: 3.6276 - val_loss: 14.1694 - val_MAE: 3.0930\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 26.7823 - MAE: 3.5931 - val_loss: 14.5191 - val_MAE: 2.9880\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 27.4788 - MAE: 3.6307 - val_loss: 14.7194 - val_MAE: 3.1472\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 26.9746 - MAE: 3.6250 - val_loss: 17.3659 - val_MAE: 3.3395\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 29.1787 - MAE: 3.7312 - val_loss: 16.8437 - val_MAE: 3.2990\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 35.7789 - MAE: 4.3025 - val_loss: 18.3480 - val_MAE: 3.6146\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 32.0678 - MAE: 4.2063 - val_loss: 15.0331 - val_MAE: 3.1915\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 25.5959 - MAE: 3.6373 - val_loss: 13.7346 - val_MAE: 2.9977\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 26.1058 - MAE: 3.6352 - val_loss: 12.4155 - val_MAE: 2.8769\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 25.2551 - MAE: 3.6094 - val_loss: 14.3742 - val_MAE: 3.1584\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 23.5293 - MAE: 3.4241 - val_loss: 13.5930 - val_MAE: 2.8497\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 23.5977 - MAE: 3.5067 - val_loss: 16.3753 - val_MAE: 3.2221\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 24.6128 - MAE: 3.7175 - val_loss: 26.2624 - val_MAE: 4.0318\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 28.9526 - MAE: 3.9052 - val_loss: 17.2354 - val_MAE: 3.2977\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 28.2392 - MAE: 3.7395 - val_loss: 17.7575 - val_MAE: 3.4834\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 28.9299 - MAE: 3.9990 - val_loss: 18.1489 - val_MAE: 3.5978\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 31.2428 - MAE: 4.0347 - val_loss: 15.5507 - val_MAE: 3.1567\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 34.1174 - MAE: 4.2176 - val_loss: 26.0171 - val_MAE: 4.0033\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 27.7977 - MAE: 3.7962 - val_loss: 16.2843 - val_MAE: 3.1654\n",
            "Epoch 54: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.1, model__n_hidden=3, model__n_neurons=125, model__optimizer=adam; total time=   5.7s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 73ms/step - loss: 3864856064.0000 - MAE: 22326.4082 - val_loss: 2921.1360 - val_MAE: 53.6938\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 3228.5032 - MAE: 56.0825 - val_loss: 2911.5168 - val_MAE: 53.6041\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3218.4639 - MAE: 55.9926 - val_loss: 2901.9468 - val_MAE: 53.5148\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3208.4688 - MAE: 55.9034 - val_loss: 2892.3455 - val_MAE: 53.4250\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 3198.4478 - MAE: 55.8137 - val_loss: 2882.8184 - val_MAE: 53.3358\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3188.5029 - MAE: 55.7241 - val_loss: 2873.3396 - val_MAE: 53.2468\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3178.6047 - MAE: 55.6355 - val_loss: 2863.8826 - val_MAE: 53.1580\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3168.7278 - MAE: 55.5471 - val_loss: 2854.4363 - val_MAE: 53.0690\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3158.8616 - MAE: 55.4579 - val_loss: 2844.9966 - val_MAE: 52.9800\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 3149.0063 - MAE: 55.3689 - val_loss: 2835.6343 - val_MAE: 52.8916\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3139.2268 - MAE: 55.2808 - val_loss: 2826.2791 - val_MAE: 52.8031\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 3129.4548 - MAE: 55.1923 - val_loss: 2816.9592 - val_MAE: 52.7148\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3119.7180 - MAE: 55.1041 - val_loss: 2807.6470 - val_MAE: 52.6264\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 3109.9929 - MAE: 55.0156 - val_loss: 2798.3958 - val_MAE: 52.5384\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 3100.3262 - MAE: 54.9275 - val_loss: 2789.1682 - val_MAE: 52.4505\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 3090.6882 - MAE: 54.8396 - val_loss: 2780.0020 - val_MAE: 52.3631\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 3081.1074 - MAE: 54.7526 - val_loss: 2770.8333 - val_MAE: 52.2754\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 3071.5261 - MAE: 54.6651 - val_loss: 2761.6931 - val_MAE: 52.1879\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3061.9739 - MAE: 54.5781 - val_loss: 2752.5803 - val_MAE: 52.1006\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3052.4490 - MAE: 54.4904 - val_loss: 2743.4875 - val_MAE: 52.0132\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3042.9448 - MAE: 54.4028 - val_loss: 2734.4238 - val_MAE: 51.9260\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 3033.4736 - MAE: 54.3158 - val_loss: 2725.4255 - val_MAE: 51.8393\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3024.0645 - MAE: 54.2295 - val_loss: 2716.4263 - val_MAE: 51.7524\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 3014.6555 - MAE: 54.1424 - val_loss: 2707.4534 - val_MAE: 51.6657\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3005.2742 - MAE: 54.0554 - val_loss: 2698.5115 - val_MAE: 51.5791\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2995.9253 - MAE: 53.9690 - val_loss: 2689.5999 - val_MAE: 51.4926\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2986.6072 - MAE: 53.8829 - val_loss: 2680.7256 - val_MAE: 51.4063\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2977.3269 - MAE: 53.7965 - val_loss: 2671.8826 - val_MAE: 51.3203\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2968.0771 - MAE: 53.7107 - val_loss: 2663.0496 - val_MAE: 51.2341\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2958.8408 - MAE: 53.6241 - val_loss: 2654.2822 - val_MAE: 51.1485\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2949.6709 - MAE: 53.5386 - val_loss: 2645.5530 - val_MAE: 51.0631\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2940.5354 - MAE: 53.4535 - val_loss: 2636.7957 - val_MAE: 50.9773\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 2931.3750 - MAE: 53.3676 - val_loss: 2628.0803 - val_MAE: 50.8917\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2922.2559 - MAE: 53.2823 - val_loss: 2619.3894 - val_MAE: 50.8063\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2913.1638 - MAE: 53.1970 - val_loss: 2610.7358 - val_MAE: 50.7210\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2904.1106 - MAE: 53.1111 - val_loss: 2602.1370 - val_MAE: 50.6362\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2895.1108 - MAE: 53.0267 - val_loss: 2593.5630 - val_MAE: 50.5515\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2886.1399 - MAE: 52.9423 - val_loss: 2585.0388 - val_MAE: 50.4671\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2877.2141 - MAE: 52.8579 - val_loss: 2576.4907 - val_MAE: 50.3823\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2868.2678 - MAE: 52.7736 - val_loss: 2567.9739 - val_MAE: 50.2977\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2859.3530 - MAE: 52.6883 - val_loss: 2559.5005 - val_MAE: 50.2134\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2850.4827 - MAE: 52.6043 - val_loss: 2551.0605 - val_MAE: 50.1293\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2841.6475 - MAE: 52.5200 - val_loss: 2542.6599 - val_MAE: 50.0455\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2832.8496 - MAE: 52.4365 - val_loss: 2534.2646 - val_MAE: 49.9615\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2824.0588 - MAE: 52.3523 - val_loss: 2525.8972 - val_MAE: 49.8777\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2815.2981 - MAE: 52.2685 - val_loss: 2517.5769 - val_MAE: 49.7942\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2806.5850 - MAE: 52.1853 - val_loss: 2509.2854 - val_MAE: 49.7109\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2797.8970 - MAE: 52.1020 - val_loss: 2500.9827 - val_MAE: 49.6273\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2789.2012 - MAE: 52.0186 - val_loss: 2492.7239 - val_MAE: 49.5440\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2780.5510 - MAE: 51.9355 - val_loss: 2484.5022 - val_MAE: 49.4610\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2771.9365 - MAE: 51.8526 - val_loss: 2476.2888 - val_MAE: 49.3779\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2763.3320 - MAE: 51.7693 - val_loss: 2468.1108 - val_MAE: 49.2950\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2754.7627 - MAE: 51.6866 - val_loss: 2459.9497 - val_MAE: 49.2122\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2746.2148 - MAE: 51.6033 - val_loss: 2451.8650 - val_MAE: 49.1300\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2737.7393 - MAE: 51.5216 - val_loss: 2443.7700 - val_MAE: 49.0475\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2729.2549 - MAE: 51.4391 - val_loss: 2435.6958 - val_MAE: 48.9651\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2720.7944 - MAE: 51.3568 - val_loss: 2427.6746 - val_MAE: 48.8831\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2712.3840 - MAE: 51.2751 - val_loss: 2419.6433 - val_MAE: 48.8009\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2703.9670 - MAE: 51.1925 - val_loss: 2411.6655 - val_MAE: 48.7191\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2695.6033 - MAE: 51.1108 - val_loss: 2403.7065 - val_MAE: 48.6374\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2687.2598 - MAE: 51.0294 - val_loss: 2395.7874 - val_MAE: 48.5559\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2678.9553 - MAE: 50.9474 - val_loss: 2387.8855 - val_MAE: 48.4745\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2670.6694 - MAE: 50.8662 - val_loss: 2380.0115 - val_MAE: 48.3932\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2662.4099 - MAE: 50.7852 - val_loss: 2372.1355 - val_MAE: 48.3117\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2654.1543 - MAE: 50.7040 - val_loss: 2364.3381 - val_MAE: 48.2310\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2645.9736 - MAE: 50.6232 - val_loss: 2356.5413 - val_MAE: 48.1501\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2637.7942 - MAE: 50.5423 - val_loss: 2348.7610 - val_MAE: 48.0692\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2629.6309 - MAE: 50.4615 - val_loss: 2340.9834 - val_MAE: 47.9882\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2621.4736 - MAE: 50.3799 - val_loss: 2333.2490 - val_MAE: 47.9076\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2613.3586 - MAE: 50.2999 - val_loss: 2325.5542 - val_MAE: 47.8272\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2605.2866 - MAE: 50.2191 - val_loss: 2317.9111 - val_MAE: 47.7472\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2597.2627 - MAE: 50.1399 - val_loss: 2310.2454 - val_MAE: 47.6669\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2589.2207 - MAE: 50.0590 - val_loss: 2302.6433 - val_MAE: 47.5871\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2581.2383 - MAE: 49.9797 - val_loss: 2295.0173 - val_MAE: 47.5069\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2573.2363 - MAE: 49.8994 - val_loss: 2287.4473 - val_MAE: 47.4272\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2565.2920 - MAE: 49.8202 - val_loss: 2279.9248 - val_MAE: 47.3478\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2557.3911 - MAE: 49.7402 - val_loss: 2272.3823 - val_MAE: 47.2681\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2549.4731 - MAE: 49.6604 - val_loss: 2264.8770 - val_MAE: 47.1886\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2541.5928 - MAE: 49.5813 - val_loss: 2257.4001 - val_MAE: 47.1093\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2533.7407 - MAE: 49.5019 - val_loss: 2249.9255 - val_MAE: 47.0299\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2525.8962 - MAE: 49.4224 - val_loss: 2242.5359 - val_MAE: 46.9513\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2518.1309 - MAE: 49.3444 - val_loss: 2235.1260 - val_MAE: 46.8723\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2510.3481 - MAE: 49.2652 - val_loss: 2227.7388 - val_MAE: 46.7934\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2502.5886 - MAE: 49.1865 - val_loss: 2220.3787 - val_MAE: 46.7147\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2494.8586 - MAE: 49.1081 - val_loss: 2213.0564 - val_MAE: 46.6363\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2487.1636 - MAE: 49.0297 - val_loss: 2205.7288 - val_MAE: 46.5577\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2479.4663 - MAE: 48.9507 - val_loss: 2198.4565 - val_MAE: 46.4795\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 2471.8223 - MAE: 48.8730 - val_loss: 2191.1675 - val_MAE: 46.4010\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2464.1665 - MAE: 48.7946 - val_loss: 2183.9492 - val_MAE: 46.3232\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2456.5796 - MAE: 48.7162 - val_loss: 2176.7400 - val_MAE: 46.2453\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2449.0012 - MAE: 48.6389 - val_loss: 2169.5457 - val_MAE: 46.1674\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2441.4417 - MAE: 48.5608 - val_loss: 2162.3950 - val_MAE: 46.0899\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2433.9241 - MAE: 48.4837 - val_loss: 2155.2607 - val_MAE: 46.0125\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2426.4231 - MAE: 48.4063 - val_loss: 2148.1375 - val_MAE: 45.9350\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2418.9333 - MAE: 48.3285 - val_loss: 2141.0356 - val_MAE: 45.8576\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2411.4680 - MAE: 48.2515 - val_loss: 2133.9785 - val_MAE: 45.7806\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2404.0437 - MAE: 48.1747 - val_loss: 2126.8970 - val_MAE: 45.7032\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2396.5989 - MAE: 48.0965 - val_loss: 2119.8752 - val_MAE: 45.6263\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2389.2114 - MAE: 48.0201 - val_loss: 2112.8496 - val_MAE: 45.5493\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2381.8230 - MAE: 47.9437 - val_loss: 2105.8564 - val_MAE: 45.4724\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=1, model__n_neurons=5, model__optimizer=nesterov; total time=   6.7s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 31ms/step - loss: 1949323.3750 - MAE: 543.7821 - val_loss: 475.8193 - val_MAE: 20.9214\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 666.7801 - MAE: 23.7508 - val_loss: 474.2360 - val_MAE: 20.8836\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 664.9825 - MAE: 23.7132 - val_loss: 472.6431 - val_MAE: 20.8454\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 663.1750 - MAE: 23.6749 - val_loss: 471.0586 - val_MAE: 20.8073\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 661.3771 - MAE: 23.6371 - val_loss: 469.4873 - val_MAE: 20.7696\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 659.5948 - MAE: 23.5987 - val_loss: 467.9289 - val_MAE: 20.7320\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 657.8242 - MAE: 23.5616 - val_loss: 466.3766 - val_MAE: 20.6945\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 656.0610 - MAE: 23.5249 - val_loss: 464.8195 - val_MAE: 20.6569\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 654.2900 - MAE: 23.4867 - val_loss: 463.2576 - val_MAE: 20.6190\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 652.5154 - MAE: 23.4488 - val_loss: 461.7035 - val_MAE: 20.5813\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 650.7504 - MAE: 23.4111 - val_loss: 460.1704 - val_MAE: 20.5440\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 649.0063 - MAE: 23.3742 - val_loss: 458.6289 - val_MAE: 20.5065\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 647.2535 - MAE: 23.3366 - val_loss: 457.0951 - val_MAE: 20.4691\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 645.5102 - MAE: 23.2992 - val_loss: 455.5750 - val_MAE: 20.4319\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 643.7795 - MAE: 23.2623 - val_loss: 454.0486 - val_MAE: 20.3945\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 642.0429 - MAE: 23.2243 - val_loss: 452.5341 - val_MAE: 20.3573\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 640.3185 - MAE: 23.1878 - val_loss: 451.0129 - val_MAE: 20.3199\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 638.5881 - MAE: 23.1502 - val_loss: 449.5103 - val_MAE: 20.2829\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 636.8766 - MAE: 23.1137 - val_loss: 448.0043 - val_MAE: 20.2458\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 635.1613 - MAE: 23.0761 - val_loss: 446.5086 - val_MAE: 20.2088\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 633.4575 - MAE: 23.0387 - val_loss: 445.0197 - val_MAE: 20.1719\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 631.7617 - MAE: 23.0023 - val_loss: 443.5408 - val_MAE: 20.1352\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 630.0754 - MAE: 22.9657 - val_loss: 442.0581 - val_MAE: 20.0984\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 628.3847 - MAE: 22.9287 - val_loss: 440.5690 - val_MAE: 20.0613\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 626.6876 - MAE: 22.8914 - val_loss: 439.1018 - val_MAE: 20.0247\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 625.0140 - MAE: 22.8550 - val_loss: 437.6336 - val_MAE: 19.9880\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 623.3416 - MAE: 22.8183 - val_loss: 436.1930 - val_MAE: 19.9519\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 621.6970 - MAE: 22.7825 - val_loss: 434.7458 - val_MAE: 19.9156\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 620.0444 - MAE: 22.7460 - val_loss: 433.3016 - val_MAE: 19.8794\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 618.3959 - MAE: 22.7098 - val_loss: 431.8587 - val_MAE: 19.8430\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 616.7499 - MAE: 22.6734 - val_loss: 430.4354 - val_MAE: 19.8071\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 615.1220 - MAE: 22.6377 - val_loss: 428.9874 - val_MAE: 19.7705\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 613.4685 - MAE: 22.6009 - val_loss: 427.5564 - val_MAE: 19.7343\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 611.8339 - MAE: 22.5649 - val_loss: 426.1326 - val_MAE: 19.6982\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 610.2058 - MAE: 22.5291 - val_loss: 424.7037 - val_MAE: 19.6619\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 608.5733 - MAE: 22.4921 - val_loss: 423.2866 - val_MAE: 19.6258\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 606.9540 - MAE: 22.4564 - val_loss: 421.8873 - val_MAE: 19.5902\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 605.3538 - MAE: 22.4207 - val_loss: 420.4910 - val_MAE: 19.5545\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 603.7557 - MAE: 22.3848 - val_loss: 419.0914 - val_MAE: 19.5187\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 602.1544 - MAE: 22.3496 - val_loss: 417.6941 - val_MAE: 19.4828\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 600.5557 - MAE: 22.3132 - val_loss: 416.3044 - val_MAE: 19.4471\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 598.9649 - MAE: 22.2775 - val_loss: 414.9220 - val_MAE: 19.4116\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 597.3818 - MAE: 22.2423 - val_loss: 413.5411 - val_MAE: 19.3760\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 595.8002 - MAE: 22.2068 - val_loss: 412.1616 - val_MAE: 19.3403\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 594.2213 - MAE: 22.1702 - val_loss: 410.7930 - val_MAE: 19.3049\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 592.6530 - MAE: 22.1356 - val_loss: 409.4358 - val_MAE: 19.2697\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 591.0980 - MAE: 22.1003 - val_loss: 408.0862 - val_MAE: 19.2347\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 589.5502 - MAE: 22.0655 - val_loss: 406.7233 - val_MAE: 19.1992\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 587.9863 - MAE: 22.0304 - val_loss: 405.3584 - val_MAE: 19.1637\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 586.4233 - MAE: 21.9948 - val_loss: 404.0208 - val_MAE: 19.1287\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 584.8876 - MAE: 21.9596 - val_loss: 402.6767 - val_MAE: 19.0936\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 583.3466 - MAE: 21.9243 - val_loss: 401.3380 - val_MAE: 19.0585\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 581.8087 - MAE: 21.8894 - val_loss: 400.0030 - val_MAE: 19.0234\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 580.2780 - MAE: 21.8542 - val_loss: 398.6851 - val_MAE: 18.9887\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 578.7625 - MAE: 21.8197 - val_loss: 397.3525 - val_MAE: 18.9536\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 577.2328 - MAE: 21.7847 - val_loss: 396.0273 - val_MAE: 18.9186\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 575.7112 - MAE: 21.7498 - val_loss: 394.7142 - val_MAE: 18.8839\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 574.2019 - MAE: 21.7149 - val_loss: 393.4058 - val_MAE: 18.8492\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 572.6976 - MAE: 21.6808 - val_loss: 392.0883 - val_MAE: 18.8142\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 571.1830 - MAE: 21.6452 - val_loss: 390.7791 - val_MAE: 18.7794\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 569.6793 - MAE: 21.6103 - val_loss: 389.4873 - val_MAE: 18.7450\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 568.1928 - MAE: 21.5759 - val_loss: 388.2001 - val_MAE: 18.7106\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 566.7114 - MAE: 21.5416 - val_loss: 386.9144 - val_MAE: 18.6762\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 565.2321 - MAE: 21.5072 - val_loss: 385.6318 - val_MAE: 18.6419\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 563.7573 - MAE: 21.4734 - val_loss: 384.3612 - val_MAE: 18.6077\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 562.2925 - MAE: 21.4392 - val_loss: 383.0808 - val_MAE: 18.5733\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 560.8195 - MAE: 21.4042 - val_loss: 381.8213 - val_MAE: 18.5394\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 559.3674 - MAE: 21.3706 - val_loss: 380.5459 - val_MAE: 18.5049\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 557.8992 - MAE: 21.3358 - val_loss: 379.2845 - val_MAE: 18.4708\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 556.4447 - MAE: 21.3023 - val_loss: 378.0261 - val_MAE: 18.4367\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 554.9956 - MAE: 21.2675 - val_loss: 376.7848 - val_MAE: 18.4030\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 553.5631 - MAE: 21.2346 - val_loss: 375.5322 - val_MAE: 18.3690\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 552.1184 - MAE: 21.2003 - val_loss: 374.2838 - val_MAE: 18.3350\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 550.6779 - MAE: 21.1665 - val_loss: 373.0354 - val_MAE: 18.3009\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 549.2379 - MAE: 21.1322 - val_loss: 371.7950 - val_MAE: 18.2670\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 547.8085 - MAE: 21.0991 - val_loss: 370.5713 - val_MAE: 18.2334\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 546.3928 - MAE: 21.0650 - val_loss: 369.3289 - val_MAE: 18.1993\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 544.9606 - MAE: 21.0302 - val_loss: 368.1166 - val_MAE: 18.1660\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 543.5593 - MAE: 20.9973 - val_loss: 366.8972 - val_MAE: 18.1324\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 542.1512 - MAE: 20.9635 - val_loss: 365.6786 - val_MAE: 18.0988\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 540.7440 - MAE: 20.9302 - val_loss: 364.4781 - val_MAE: 18.0656\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 539.3551 - MAE: 20.8974 - val_loss: 363.2726 - val_MAE: 18.0322\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 537.9608 - MAE: 20.8640 - val_loss: 362.0613 - val_MAE: 17.9986\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 536.5612 - MAE: 20.8300 - val_loss: 360.8682 - val_MAE: 17.9654\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 535.1804 - MAE: 20.7972 - val_loss: 359.6744 - val_MAE: 17.9321\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 533.7995 - MAE: 20.7639 - val_loss: 358.4891 - val_MAE: 17.8991\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 532.4268 - MAE: 20.7310 - val_loss: 357.2958 - val_MAE: 17.8657\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 531.0453 - MAE: 20.6977 - val_loss: 356.1032 - val_MAE: 17.8323\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 529.6650 - MAE: 20.6639 - val_loss: 354.9212 - val_MAE: 17.7991\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 528.2972 - MAE: 20.6306 - val_loss: 353.7558 - val_MAE: 17.7663\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 526.9460 - MAE: 20.5981 - val_loss: 352.5830 - val_MAE: 17.7333\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 525.5881 - MAE: 20.5650 - val_loss: 351.4187 - val_MAE: 17.7004\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 524.2369 - MAE: 20.5328 - val_loss: 350.2459 - val_MAE: 17.6673\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 522.8785 - MAE: 20.4994 - val_loss: 349.0910 - val_MAE: 17.6346\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 521.5389 - MAE: 20.4660 - val_loss: 347.9408 - val_MAE: 17.6019\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 520.2045 - MAE: 20.4340 - val_loss: 346.7930 - val_MAE: 17.5693\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 518.8719 - MAE: 20.4013 - val_loss: 345.6366 - val_MAE: 17.5364\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 517.5309 - MAE: 20.3679 - val_loss: 344.4975 - val_MAE: 17.5038\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 516.2086 - MAE: 20.3356 - val_loss: 343.3573 - val_MAE: 17.4712\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 514.8852 - MAE: 20.3034 - val_loss: 342.2326 - val_MAE: 17.4390\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=1, model__n_neurons=5, model__optimizer=nesterov; total time=   6.7s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 28ms/step - loss: 11751142.0000 - MAE: 1242.2570 - val_loss: 471.5887 - val_MAE: 19.9949\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 479.4601 - MAE: 20.0403 - val_loss: 470.3067 - val_MAE: 19.9628\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 478.1763 - MAE: 20.0080 - val_loss: 469.0266 - val_MAE: 19.9307\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 476.8940 - MAE: 19.9763 - val_loss: 467.7557 - val_MAE: 19.8988\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 475.6211 - MAE: 19.9441 - val_loss: 466.4914 - val_MAE: 19.8670\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 474.3547 - MAE: 19.9123 - val_loss: 465.2310 - val_MAE: 19.8353\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 473.0918 - MAE: 19.8807 - val_loss: 463.9633 - val_MAE: 19.8033\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 471.8228 - MAE: 19.8482 - val_loss: 462.7077 - val_MAE: 19.7716\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 470.5653 - MAE: 19.8170 - val_loss: 461.4600 - val_MAE: 19.7400\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 469.3145 - MAE: 19.7855 - val_loss: 460.2100 - val_MAE: 19.7083\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 468.0626 - MAE: 19.7538 - val_loss: 458.9656 - val_MAE: 19.6767\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 466.8175 - MAE: 19.7222 - val_loss: 457.7355 - val_MAE: 19.6454\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 465.5844 - MAE: 19.6910 - val_loss: 456.5001 - val_MAE: 19.6140\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 464.3470 - MAE: 19.6596 - val_loss: 455.2676 - val_MAE: 19.5825\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 463.1125 - MAE: 19.6277 - val_loss: 454.0431 - val_MAE: 19.5512\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 461.8852 - MAE: 19.5966 - val_loss: 452.8102 - val_MAE: 19.5197\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 460.6512 - MAE: 19.5647 - val_loss: 451.5931 - val_MAE: 19.4885\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 459.4321 - MAE: 19.5337 - val_loss: 450.3797 - val_MAE: 19.4573\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 458.2170 - MAE: 19.5029 - val_loss: 449.1748 - val_MAE: 19.4263\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 457.0094 - MAE: 19.4719 - val_loss: 447.9656 - val_MAE: 19.3952\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 455.7988 - MAE: 19.4407 - val_loss: 446.7668 - val_MAE: 19.3642\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 454.5972 - MAE: 19.4100 - val_loss: 445.5670 - val_MAE: 19.3332\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 453.3954 - MAE: 19.3794 - val_loss: 444.3669 - val_MAE: 19.3022\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 452.1926 - MAE: 19.3480 - val_loss: 443.1642 - val_MAE: 19.2710\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 450.9883 - MAE: 19.3167 - val_loss: 441.9689 - val_MAE: 19.2400\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 449.7907 - MAE: 19.2858 - val_loss: 440.7797 - val_MAE: 19.2090\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 448.5999 - MAE: 19.2547 - val_loss: 439.5977 - val_MAE: 19.1782\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 447.4158 - MAE: 19.2239 - val_loss: 438.4124 - val_MAE: 19.1473\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 446.2287 - MAE: 19.1931 - val_loss: 437.2393 - val_MAE: 19.1167\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 445.0531 - MAE: 19.1623 - val_loss: 436.0634 - val_MAE: 19.0859\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 443.8756 - MAE: 19.1313 - val_loss: 434.8923 - val_MAE: 19.0552\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 442.7018 - MAE: 19.1012 - val_loss: 433.7226 - val_MAE: 19.0245\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 441.5314 - MAE: 19.0701 - val_loss: 432.5710 - val_MAE: 18.9942\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 440.3774 - MAE: 19.0398 - val_loss: 431.4157 - val_MAE: 18.9637\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 439.2191 - MAE: 19.0097 - val_loss: 430.2587 - val_MAE: 18.9332\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 438.0618 - MAE: 18.9790 - val_loss: 429.1229 - val_MAE: 18.9032\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 436.9243 - MAE: 18.9483 - val_loss: 427.9875 - val_MAE: 18.8731\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 435.7868 - MAE: 18.9186 - val_loss: 426.8557 - val_MAE: 18.8431\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 434.6525 - MAE: 18.8888 - val_loss: 425.7192 - val_MAE: 18.8129\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 433.5132 - MAE: 18.8589 - val_loss: 424.5861 - val_MAE: 18.7828\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 432.3787 - MAE: 18.8288 - val_loss: 423.4604 - val_MAE: 18.7528\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 431.2494 - MAE: 18.7991 - val_loss: 422.3150 - val_MAE: 18.7222\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 430.1031 - MAE: 18.7684 - val_loss: 421.1923 - val_MAE: 18.6922\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 428.9788 - MAE: 18.7382 - val_loss: 420.0773 - val_MAE: 18.6624\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 427.8604 - MAE: 18.7086 - val_loss: 418.9543 - val_MAE: 18.6323\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 426.7364 - MAE: 18.6783 - val_loss: 417.8460 - val_MAE: 18.6025\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 425.6264 - MAE: 18.6481 - val_loss: 416.7447 - val_MAE: 18.5729\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 424.5226 - MAE: 18.6191 - val_loss: 415.6419 - val_MAE: 18.5432\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 423.4177 - MAE: 18.5892 - val_loss: 414.5399 - val_MAE: 18.5134\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 422.3138 - MAE: 18.5593 - val_loss: 413.4431 - val_MAE: 18.4838\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 421.2153 - MAE: 18.5296 - val_loss: 412.3537 - val_MAE: 18.4543\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 420.1240 - MAE: 18.5003 - val_loss: 411.2626 - val_MAE: 18.4247\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 419.0299 - MAE: 18.4711 - val_loss: 410.1700 - val_MAE: 18.3950\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 417.9353 - MAE: 18.4411 - val_loss: 409.0818 - val_MAE: 18.3654\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 416.8457 - MAE: 18.4118 - val_loss: 408.0026 - val_MAE: 18.3360\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 415.7650 - MAE: 18.3823 - val_loss: 406.9296 - val_MAE: 18.3067\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 414.6890 - MAE: 18.3532 - val_loss: 405.8510 - val_MAE: 18.2773\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 413.6084 - MAE: 18.3235 - val_loss: 404.7773 - val_MAE: 18.2479\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 412.5328 - MAE: 18.2945 - val_loss: 403.7014 - val_MAE: 18.2184\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 411.4560 - MAE: 18.2648 - val_loss: 402.6446 - val_MAE: 18.1893\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 410.3957 - MAE: 18.2359 - val_loss: 401.5754 - val_MAE: 18.1599\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 409.3252 - MAE: 18.2061 - val_loss: 400.5190 - val_MAE: 18.1308\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 408.2673 - MAE: 18.1770 - val_loss: 399.4722 - val_MAE: 18.1019\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 407.2182 - MAE: 18.1486 - val_loss: 398.4227 - val_MAE: 18.0729\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 406.1670 - MAE: 18.1191 - val_loss: 397.3816 - val_MAE: 18.0441\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 405.1231 - MAE: 18.0905 - val_loss: 396.3293 - val_MAE: 18.0149\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 404.0692 - MAE: 18.0615 - val_loss: 395.2874 - val_MAE: 17.9860\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 403.0262 - MAE: 18.0323 - val_loss: 394.2588 - val_MAE: 17.9573\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 401.9939 - MAE: 18.0042 - val_loss: 393.2129 - val_MAE: 17.9282\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 400.9474 - MAE: 17.9748 - val_loss: 392.1892 - val_MAE: 17.8996\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 399.9207 - MAE: 17.9464 - val_loss: 391.1606 - val_MAE: 17.8709\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 398.8905 - MAE: 17.9175 - val_loss: 390.1391 - val_MAE: 17.8423\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 397.8677 - MAE: 17.8893 - val_loss: 389.1232 - val_MAE: 17.8138\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 396.8492 - MAE: 17.8605 - val_loss: 388.1067 - val_MAE: 17.7852\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 395.8307 - MAE: 17.8321 - val_loss: 387.0937 - val_MAE: 17.7567\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 394.8154 - MAE: 17.8034 - val_loss: 386.0808 - val_MAE: 17.7282\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 393.8010 - MAE: 17.7747 - val_loss: 385.0740 - val_MAE: 17.6998\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 392.7915 - MAE: 17.7463 - val_loss: 384.0627 - val_MAE: 17.6712\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 391.7800 - MAE: 17.7171 - val_loss: 383.0754 - val_MAE: 17.6432\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 390.7898 - MAE: 17.6897 - val_loss: 382.0839 - val_MAE: 17.6151\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 389.7951 - MAE: 17.6619 - val_loss: 381.0750 - val_MAE: 17.5864\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 388.7846 - MAE: 17.6329 - val_loss: 380.0777 - val_MAE: 17.5580\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 387.7850 - MAE: 17.6050 - val_loss: 379.0772 - val_MAE: 17.5295\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 386.7830 - MAE: 17.5765 - val_loss: 378.0907 - val_MAE: 17.5014\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 385.7953 - MAE: 17.5478 - val_loss: 377.1140 - val_MAE: 17.4734\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 384.8164 - MAE: 17.5200 - val_loss: 376.1381 - val_MAE: 17.4455\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 383.8386 - MAE: 17.4922 - val_loss: 375.1679 - val_MAE: 17.4177\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 382.8658 - MAE: 17.4644 - val_loss: 374.1973 - val_MAE: 17.3898\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 381.8934 - MAE: 17.4367 - val_loss: 373.2300 - val_MAE: 17.3620\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 380.9251 - MAE: 17.4089 - val_loss: 372.2687 - val_MAE: 17.3342\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 379.9603 - MAE: 17.3814 - val_loss: 371.2995 - val_MAE: 17.3063\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 378.9885 - MAE: 17.3535 - val_loss: 370.3246 - val_MAE: 17.2781\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 378.0127 - MAE: 17.3249 - val_loss: 369.3668 - val_MAE: 17.2503\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 377.0538 - MAE: 17.2968 - val_loss: 368.4199 - val_MAE: 17.2229\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 376.1039 - MAE: 17.2697 - val_loss: 367.4619 - val_MAE: 17.1950\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 375.1451 - MAE: 17.2416 - val_loss: 366.5230 - val_MAE: 17.1677\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 374.2031 - MAE: 17.2146 - val_loss: 365.5762 - val_MAE: 17.1401\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 373.2546 - MAE: 17.1867 - val_loss: 364.6321 - val_MAE: 17.1126\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 372.3087 - MAE: 17.1592 - val_loss: 363.6910 - val_MAE: 17.0850\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 371.3661 - MAE: 17.1314 - val_loss: 362.7606 - val_MAE: 17.0578\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=1, model__n_neurons=5, model__optimizer=nesterov; total time=   6.6s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 30ms/step - loss: 35216.8711 - MAE: 184.4696 - val_loss: 33578.3555 - val_MAE: 181.1010\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 32526.4629 - MAE: 177.2038 - val_loss: 30948.5078 - val_MAE: 173.8365\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 29945.2402 - MAE: 169.9716 - val_loss: 28460.1973 - val_MAE: 166.6667\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 27502.5859 - MAE: 162.8566 - val_loss: 26117.0352 - val_MAE: 159.6156\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 25233.3809 - MAE: 155.8867 - val_loss: 23911.3398 - val_MAE: 152.6753\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23068.4590 - MAE: 148.9938 - val_loss: 21859.1172 - val_MAE: 145.9160\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 21071.7969 - MAE: 142.3184 - val_loss: 19940.1113 - val_MAE: 139.2928\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 19183.5586 - MAE: 135.7616 - val_loss: 18165.4355 - val_MAE: 132.8678\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 17463.1914 - MAE: 129.3960 - val_loss: 16505.3926 - val_MAE: 126.5566\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 15851.3623 - MAE: 123.1636 - val_loss: 14977.0303 - val_MAE: 120.4471\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 14368.4121 - MAE: 117.1688 - val_loss: 13567.1543 - val_MAE: 114.5146\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 12997.1846 - MAE: 111.3060 - val_loss: 12275.4463 - val_MAE: 108.7895\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 11736.9424 - MAE: 105.6332 - val_loss: 11085.7383 - val_MAE: 103.2288\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 10587.7520 - MAE: 100.1630 - val_loss: 9989.8145 - val_MAE: 97.8179\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 9513.4609 - MAE: 94.8143 - val_loss: 8993.3760 - val_MAE: 92.6176\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 8550.2266 - MAE: 89.6542 - val_loss: 8075.1973 - val_MAE: 87.5443\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7662.6172 - MAE: 84.7032 - val_loss: 7244.1914 - val_MAE: 82.6737\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6863.5767 - MAE: 79.9148 - val_loss: 6488.5269 - val_MAE: 77.9728\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 6131.9741 - MAE: 75.2973 - val_loss: 5812.8184 - val_MAE: 73.5077\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5475.6401 - MAE: 70.8974 - val_loss: 5203.3564 - val_MAE: 69.2243\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4884.7715 - MAE: 66.6668 - val_loss: 4651.4546 - val_MAE: 65.0925\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4356.1689 - MAE: 62.5987 - val_loss: 4152.3647 - val_MAE: 61.1070\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 3876.8479 - MAE: 58.7184 - val_loss: 3708.3171 - val_MAE: 57.3714\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3440.9092 - MAE: 55.0289 - val_loss: 3319.0161 - val_MAE: 54.1021\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3071.6064 - MAE: 51.5037 - val_loss: 2962.0635 - val_MAE: 50.8921\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2728.4048 - MAE: 48.1259 - val_loss: 2648.8984 - val_MAE: 47.8704\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2420.4692 - MAE: 44.9436 - val_loss: 2375.4717 - val_MAE: 45.0368\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2156.7734 - MAE: 42.0100 - val_loss: 2132.3130 - val_MAE: 42.3289\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1926.1851 - MAE: 39.2636 - val_loss: 1915.4586 - val_MAE: 39.8333\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1719.9575 - MAE: 36.7221 - val_loss: 1725.5336 - val_MAE: 37.5869\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1535.0544 - MAE: 34.3592 - val_loss: 1563.0601 - val_MAE: 35.6833\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1380.2836 - MAE: 32.2016 - val_loss: 1418.6702 - val_MAE: 33.8535\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1243.3843 - MAE: 30.1944 - val_loss: 1293.0013 - val_MAE: 32.1275\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1121.1647 - MAE: 28.3228 - val_loss: 1186.9220 - val_MAE: 30.5964\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1019.1454 - MAE: 26.6705 - val_loss: 1094.8932 - val_MAE: 29.2381\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 933.2822 - MAE: 25.1850 - val_loss: 1013.6325 - val_MAE: 27.9358\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 853.8568 - MAE: 23.8602 - val_loss: 944.7735 - val_MAE: 26.7349\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 788.9078 - MAE: 22.6303 - val_loss: 884.9442 - val_MAE: 25.6012\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 732.4120 - MAE: 21.5523 - val_loss: 833.2073 - val_MAE: 24.5376\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 682.7215 - MAE: 20.6230 - val_loss: 790.1127 - val_MAE: 23.6792\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 642.8040 - MAE: 19.8575 - val_loss: 752.2003 - val_MAE: 22.8567\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 607.8577 - MAE: 19.1832 - val_loss: 719.5759 - val_MAE: 22.1437\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 577.7914 - MAE: 18.6787 - val_loss: 692.1705 - val_MAE: 21.6151\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 551.8203 - MAE: 18.2131 - val_loss: 669.2500 - val_MAE: 21.3736\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 530.8404 - MAE: 17.8652 - val_loss: 649.5353 - val_MAE: 21.1762\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 512.7866 - MAE: 17.5721 - val_loss: 632.2269 - val_MAE: 20.9860\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 497.2026 - MAE: 17.3673 - val_loss: 617.4827 - val_MAE: 20.8778\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 484.5349 - MAE: 17.1934 - val_loss: 604.3345 - val_MAE: 20.7712\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 472.9368 - MAE: 17.0455 - val_loss: 592.9598 - val_MAE: 20.6694\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 463.2711 - MAE: 16.9514 - val_loss: 582.9139 - val_MAE: 20.5698\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 455.0063 - MAE: 16.8764 - val_loss: 574.2694 - val_MAE: 20.4736\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 448.0130 - MAE: 16.8172 - val_loss: 566.5972 - val_MAE: 20.3800\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 441.9979 - MAE: 16.7649 - val_loss: 559.6918 - val_MAE: 20.2978\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 437.2402 - MAE: 16.7127 - val_loss: 553.2839 - val_MAE: 20.2422\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 432.6490 - MAE: 16.6837 - val_loss: 547.5585 - val_MAE: 20.1829\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 428.5300 - MAE: 16.6336 - val_loss: 542.4488 - val_MAE: 20.1232\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 425.0538 - MAE: 16.5926 - val_loss: 537.4207 - val_MAE: 20.0605\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 422.0511 - MAE: 16.5679 - val_loss: 532.5435 - val_MAE: 19.9943\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 419.3214 - MAE: 16.5404 - val_loss: 528.0561 - val_MAE: 19.9335\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 416.5207 - MAE: 16.5052 - val_loss: 523.9393 - val_MAE: 19.8723\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 414.2330 - MAE: 16.4750 - val_loss: 519.6459 - val_MAE: 19.8073\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 411.8896 - MAE: 16.4500 - val_loss: 515.7358 - val_MAE: 19.7482\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 409.8911 - MAE: 16.4272 - val_loss: 511.7220 - val_MAE: 19.6844\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 407.8175 - MAE: 16.4021 - val_loss: 507.7715 - val_MAE: 19.6186\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 405.7079 - MAE: 16.3727 - val_loss: 504.1730 - val_MAE: 19.5559\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 404.0494 - MAE: 16.3492 - val_loss: 500.2606 - val_MAE: 19.4883\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 402.1163 - MAE: 16.3159 - val_loss: 496.6861 - val_MAE: 19.4255\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 400.5024 - MAE: 16.2955 - val_loss: 492.8608 - val_MAE: 19.3572\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 398.7227 - MAE: 16.2671 - val_loss: 489.4439 - val_MAE: 19.2972\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 396.9763 - MAE: 16.2422 - val_loss: 485.7785 - val_MAE: 19.2300\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 395.3229 - MAE: 16.2137 - val_loss: 482.2520 - val_MAE: 19.1645\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 393.7283 - MAE: 16.1841 - val_loss: 478.6035 - val_MAE: 19.0964\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 392.1516 - MAE: 16.1545 - val_loss: 475.1169 - val_MAE: 19.0321\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 390.5439 - MAE: 16.1275 - val_loss: 471.4646 - val_MAE: 18.9629\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 388.8924 - MAE: 16.0964 - val_loss: 468.0639 - val_MAE: 18.8990\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 387.2726 - MAE: 16.0669 - val_loss: 464.5626 - val_MAE: 18.8319\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 385.7972 - MAE: 16.0348 - val_loss: 460.9032 - val_MAE: 18.7611\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 384.1331 - MAE: 16.0062 - val_loss: 457.4683 - val_MAE: 18.6960\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 382.5161 - MAE: 15.9796 - val_loss: 453.9766 - val_MAE: 18.6284\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 380.9677 - MAE: 15.9464 - val_loss: 450.5206 - val_MAE: 18.5607\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 379.5018 - MAE: 15.9172 - val_loss: 447.0428 - val_MAE: 18.4930\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 377.8910 - MAE: 15.8836 - val_loss: 443.4187 - val_MAE: 18.4210\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 376.2762 - MAE: 15.8480 - val_loss: 440.1327 - val_MAE: 18.3559\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 374.8452 - MAE: 15.8171 - val_loss: 436.6700 - val_MAE: 18.2867\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 373.3321 - MAE: 15.7832 - val_loss: 433.2982 - val_MAE: 18.2184\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 371.8212 - MAE: 15.7493 - val_loss: 430.0328 - val_MAE: 18.1525\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 370.3727 - MAE: 15.7195 - val_loss: 426.7818 - val_MAE: 18.0870\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 368.9279 - MAE: 15.6870 - val_loss: 423.3468 - val_MAE: 18.0171\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 367.5464 - MAE: 15.6544 - val_loss: 419.8878 - val_MAE: 17.9456\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 365.9385 - MAE: 15.6209 - val_loss: 416.6024 - val_MAE: 17.8784\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 364.4446 - MAE: 15.5898 - val_loss: 413.4933 - val_MAE: 17.8139\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 363.0883 - MAE: 15.5617 - val_loss: 410.0966 - val_MAE: 17.7433\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 361.6026 - MAE: 15.5296 - val_loss: 406.9152 - val_MAE: 17.6767\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 360.3264 - MAE: 15.5042 - val_loss: 403.6282 - val_MAE: 17.6093\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 358.7234 - MAE: 15.4781 - val_loss: 400.6494 - val_MAE: 17.5467\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 357.3776 - MAE: 15.4489 - val_loss: 397.4695 - val_MAE: 17.4788\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 355.9578 - MAE: 15.4137 - val_loss: 394.4258 - val_MAE: 17.4123\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 354.5508 - MAE: 15.3819 - val_loss: 391.4092 - val_MAE: 17.3490\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 353.2172 - MAE: 15.3554 - val_loss: 388.2362 - val_MAE: 17.2804\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 351.8313 - MAE: 15.3256 - val_loss: 385.2864 - val_MAE: 17.2163\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.9, model__n_hidden=0, model__n_neurons=25, model__optimizer=adam; total time=   6.5s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 28ms/step - loss: 5136.8149 - MAE: 67.0471 - val_loss: 3613.7043 - val_MAE: 56.7150\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4299.6538 - MAE: 61.0270 - val_loss: 2964.4998 - val_MAE: 51.2744\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 3585.7212 - MAE: 55.2603 - val_loss: 2437.5120 - val_MAE: 46.0321\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3000.8838 - MAE: 49.8690 - val_loss: 2026.2711 - val_MAE: 41.2625\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2527.1511 - MAE: 45.0482 - val_loss: 1720.4296 - val_MAE: 37.7334\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2163.0806 - MAE: 41.2466 - val_loss: 1503.2015 - val_MAE: 34.8385\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1886.3159 - MAE: 38.6702 - val_loss: 1353.9290 - val_MAE: 33.0827\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1675.5985 - MAE: 36.5678 - val_loss: 1255.9709 - val_MAE: 31.7917\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1537.7437 - MAE: 35.0433 - val_loss: 1188.1058 - val_MAE: 30.5734\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1424.9459 - MAE: 33.4863 - val_loss: 1142.7638 - val_MAE: 29.4702\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1339.3102 - MAE: 32.1989 - val_loss: 1107.2987 - val_MAE: 28.4770\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1273.4441 - MAE: 31.0997 - val_loss: 1078.9155 - val_MAE: 27.5738\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1219.6089 - MAE: 30.0858 - val_loss: 1051.7139 - val_MAE: 26.7531\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1171.9270 - MAE: 29.2847 - val_loss: 1020.4590 - val_MAE: 26.0193\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1129.1887 - MAE: 28.6013 - val_loss: 988.7249 - val_MAE: 25.3776\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1091.0375 - MAE: 27.9756 - val_loss: 958.4048 - val_MAE: 24.7489\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1051.6860 - MAE: 27.3674 - val_loss: 924.4390 - val_MAE: 24.1679\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1013.9670 - MAE: 26.8017 - val_loss: 891.4699 - val_MAE: 23.5956\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 978.7052 - MAE: 26.2953 - val_loss: 854.3437 - val_MAE: 23.0298\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 943.7161 - MAE: 25.7849 - val_loss: 818.5927 - val_MAE: 22.4600\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 909.4332 - MAE: 25.2800 - val_loss: 784.1763 - val_MAE: 21.9067\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 874.7803 - MAE: 24.8005 - val_loss: 748.9890 - val_MAE: 21.4005\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 842.6023 - MAE: 24.3360 - val_loss: 715.9753 - val_MAE: 20.9084\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 812.2833 - MAE: 23.8750 - val_loss: 682.1245 - val_MAE: 20.4064\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 780.5974 - MAE: 23.3953 - val_loss: 651.7353 - val_MAE: 19.9576\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 752.0690 - MAE: 22.9643 - val_loss: 620.3890 - val_MAE: 19.5048\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 723.1205 - MAE: 22.5080 - val_loss: 593.3352 - val_MAE: 19.0584\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 697.3572 - MAE: 22.0906 - val_loss: 561.1295 - val_MAE: 18.6052\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 668.2279 - MAE: 21.6333 - val_loss: 535.6906 - val_MAE: 18.1797\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 643.0212 - MAE: 21.2109 - val_loss: 509.5900 - val_MAE: 17.7489\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 619.2551 - MAE: 20.7890 - val_loss: 486.3588 - val_MAE: 17.3211\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 595.3286 - MAE: 20.3584 - val_loss: 463.8285 - val_MAE: 16.9356\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 572.2771 - MAE: 19.9397 - val_loss: 442.9098 - val_MAE: 16.5701\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 551.1257 - MAE: 19.5331 - val_loss: 421.4593 - val_MAE: 16.1877\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 531.0940 - MAE: 19.1535 - val_loss: 396.3733 - val_MAE: 15.7592\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 509.4107 - MAE: 18.7267 - val_loss: 379.6026 - val_MAE: 15.4219\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 490.6785 - MAE: 18.3435 - val_loss: 359.7784 - val_MAE: 15.0467\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 472.2469 - MAE: 17.9829 - val_loss: 340.3143 - val_MAE: 14.6731\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 454.9701 - MAE: 17.6394 - val_loss: 322.4910 - val_MAE: 14.3148\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 438.8358 - MAE: 17.3072 - val_loss: 303.9519 - val_MAE: 13.9396\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 423.0984 - MAE: 16.9595 - val_loss: 288.5464 - val_MAE: 13.5928\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 407.7054 - MAE: 16.6182 - val_loss: 274.3054 - val_MAE: 13.2817\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 393.7834 - MAE: 16.3048 - val_loss: 260.5359 - val_MAE: 12.9738\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 380.4330 - MAE: 15.9937 - val_loss: 246.0998 - val_MAE: 12.6488\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 367.6972 - MAE: 15.7028 - val_loss: 236.1137 - val_MAE: 12.4279\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 355.1879 - MAE: 15.4153 - val_loss: 224.5128 - val_MAE: 12.1684\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 343.7764 - MAE: 15.1335 - val_loss: 213.1018 - val_MAE: 11.9029\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 333.2647 - MAE: 14.8720 - val_loss: 201.8998 - val_MAE: 11.6352\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 323.1764 - MAE: 14.6058 - val_loss: 190.9374 - val_MAE: 11.3617\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 313.1037 - MAE: 14.3397 - val_loss: 182.8590 - val_MAE: 11.1689\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 303.8809 - MAE: 14.0890 - val_loss: 174.3294 - val_MAE: 10.9510\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 294.9356 - MAE: 13.8554 - val_loss: 166.5542 - val_MAE: 10.7474\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 286.5608 - MAE: 13.6235 - val_loss: 157.5612 - val_MAE: 10.4919\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 278.6270 - MAE: 13.3957 - val_loss: 151.2173 - val_MAE: 10.3104\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 270.8414 - MAE: 13.1778 - val_loss: 143.4239 - val_MAE: 10.0720\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 263.9717 - MAE: 12.9618 - val_loss: 136.6248 - val_MAE: 9.8573\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 257.3588 - MAE: 12.7542 - val_loss: 130.7098 - val_MAE: 9.6638\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 251.1284 - MAE: 12.5716 - val_loss: 125.3600 - val_MAE: 9.4812\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 245.1381 - MAE: 12.3958 - val_loss: 121.2781 - val_MAE: 9.3353\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 239.5081 - MAE: 12.2125 - val_loss: 116.2170 - val_MAE: 9.1500\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 234.3732 - MAE: 12.0540 - val_loss: 112.6926 - val_MAE: 9.0105\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 229.3966 - MAE: 11.8911 - val_loss: 108.0470 - val_MAE: 8.8275\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 224.7264 - MAE: 11.7267 - val_loss: 103.8929 - val_MAE: 8.6576\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 220.1042 - MAE: 11.5627 - val_loss: 99.1206 - val_MAE: 8.4603\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 216.0646 - MAE: 11.3930 - val_loss: 94.3592 - val_MAE: 8.2567\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 212.3260 - MAE: 11.2462 - val_loss: 89.8780 - val_MAE: 8.0558\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 208.3065 - MAE: 11.0878 - val_loss: 87.4283 - val_MAE: 7.9304\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 204.7177 - MAE: 10.9653 - val_loss: 84.7479 - val_MAE: 7.7978\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 201.5558 - MAE: 10.8344 - val_loss: 82.5299 - val_MAE: 7.6903\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 198.3172 - MAE: 10.7336 - val_loss: 80.9972 - val_MAE: 7.6089\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 195.2868 - MAE: 10.6321 - val_loss: 79.1254 - val_MAE: 7.5091\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 192.4999 - MAE: 10.5054 - val_loss: 75.1527 - val_MAE: 7.3124\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 189.6250 - MAE: 10.3821 - val_loss: 73.2100 - val_MAE: 7.2029\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 187.1444 - MAE: 10.2866 - val_loss: 71.2404 - val_MAE: 7.0901\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 184.5609 - MAE: 10.1731 - val_loss: 69.5599 - val_MAE: 6.9983\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 182.4027 - MAE: 10.0785 - val_loss: 67.7804 - val_MAE: 6.8977\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 180.2203 - MAE: 10.0045 - val_loss: 66.9646 - val_MAE: 6.8445\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 178.2166 - MAE: 9.9514 - val_loss: 67.5244 - val_MAE: 6.8572\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 176.0535 - MAE: 9.8652 - val_loss: 64.3778 - val_MAE: 6.6805\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 174.1176 - MAE: 9.7716 - val_loss: 62.6375 - val_MAE: 6.5718\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 172.2811 - MAE: 9.6901 - val_loss: 61.1347 - val_MAE: 6.4734\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 170.6731 - MAE: 9.6104 - val_loss: 58.8916 - val_MAE: 6.3317\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 168.8790 - MAE: 9.5371 - val_loss: 57.6946 - val_MAE: 6.2562\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 167.3374 - MAE: 9.4989 - val_loss: 58.4087 - val_MAE: 6.2970\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 165.7302 - MAE: 9.4667 - val_loss: 58.1648 - val_MAE: 6.2754\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 164.3660 - MAE: 9.4221 - val_loss: 57.4218 - val_MAE: 6.2221\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 163.0006 - MAE: 9.3863 - val_loss: 57.0519 - val_MAE: 6.1892\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 161.6525 - MAE: 9.3402 - val_loss: 55.4494 - val_MAE: 6.0829\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 160.4328 - MAE: 9.3167 - val_loss: 56.2433 - val_MAE: 6.1320\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 159.1472 - MAE: 9.2839 - val_loss: 55.3481 - val_MAE: 6.0703\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 157.8936 - MAE: 9.2234 - val_loss: 52.7717 - val_MAE: 5.8978\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 156.6719 - MAE: 9.1657 - val_loss: 51.6453 - val_MAE: 5.8379\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 155.5532 - MAE: 9.1173 - val_loss: 50.2667 - val_MAE: 5.7594\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 154.6811 - MAE: 9.0783 - val_loss: 49.3348 - val_MAE: 5.7064\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 153.4817 - MAE: 9.0548 - val_loss: 49.8157 - val_MAE: 5.7391\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 152.4624 - MAE: 9.0246 - val_loss: 48.9801 - val_MAE: 5.6893\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 151.5005 - MAE: 8.9996 - val_loss: 48.7457 - val_MAE: 5.6752\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 150.6547 - MAE: 8.9999 - val_loss: 49.5042 - val_MAE: 5.7204\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 149.6628 - MAE: 8.9814 - val_loss: 48.8129 - val_MAE: 5.6772\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 148.8694 - MAE: 8.9348 - val_loss: 46.6004 - val_MAE: 5.5370\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.9, model__n_hidden=0, model__n_neurons=25, model__optimizer=adam; total time=   6.4s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 25ms/step - loss: 70106.0547 - MAE: 260.6117 - val_loss: 73232.5547 - val_MAE: 266.5215\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 66255.5859 - MAE: 253.4221 - val_loss: 69223.6641 - val_MAE: 259.1418\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 62628.8125 - MAE: 246.3430 - val_loss: 65325.2969 - val_MAE: 251.7577\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 59072.0938 - MAE: 239.2550 - val_loss: 61590.9414 - val_MAE: 244.4744\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 55686.1016 - MAE: 232.2716 - val_loss: 58004.1172 - val_MAE: 237.2673\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 52399.8516 - MAE: 225.3545 - val_loss: 54595.6562 - val_MAE: 230.2074\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 49293.6055 - MAE: 218.5830 - val_loss: 51337.5703 - val_MAE: 223.2498\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 46335.1094 - MAE: 211.9093 - val_loss: 48222.3203 - val_MAE: 216.3875\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 43490.5352 - MAE: 205.3322 - val_loss: 45260.9336 - val_MAE: 209.6538\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 40803.0469 - MAE: 198.8782 - val_loss: 42427.4414 - val_MAE: 203.0025\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 38235.4727 - MAE: 192.5103 - val_loss: 39733.8750 - val_MAE: 196.4695\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 35790.4688 - MAE: 186.2597 - val_loss: 37182.8203 - val_MAE: 190.0729\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 33502.9023 - MAE: 180.1497 - val_loss: 34738.9922 - val_MAE: 183.7372\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 31284.7695 - MAE: 174.0889 - val_loss: 32436.0273 - val_MAE: 177.5580\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 29195.8789 - MAE: 168.1871 - val_loss: 30263.9395 - val_MAE: 171.5234\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 27217.5332 - MAE: 162.4095 - val_loss: 28220.6387 - val_MAE: 165.6438\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 25350.5273 - MAE: 156.7596 - val_loss: 26285.0098 - val_MAE: 159.8741\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 23614.2793 - MAE: 151.2442 - val_loss: 24434.0762 - val_MAE: 154.1554\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 21949.2520 - MAE: 145.7970 - val_loss: 22690.8652 - val_MAE: 148.5668\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 20370.9219 - MAE: 140.4569 - val_loss: 21059.0176 - val_MAE: 143.1364\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 18901.4297 - MAE: 135.2770 - val_loss: 19523.1836 - val_MAE: 137.8268\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 17510.5000 - MAE: 130.2088 - val_loss: 18083.3066 - val_MAE: 132.6557\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 16208.8262 - MAE: 125.2634 - val_loss: 16733.6035 - val_MAE: 127.6166\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 14996.2588 - MAE: 120.4564 - val_loss: 15464.4238 - val_MAE: 122.6865\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 13843.3662 - MAE: 115.7474 - val_loss: 14282.8486 - val_MAE: 117.9095\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 12777.1377 - MAE: 111.1755 - val_loss: 13175.9375 - val_MAE: 113.2493\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 11779.2051 - MAE: 106.7304 - val_loss: 12139.4141 - val_MAE: 108.7045\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 10858.3213 - MAE: 102.4034 - val_loss: 11162.2061 - val_MAE: 104.2362\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 9973.3086 - MAE: 98.1484 - val_loss: 10264.5889 - val_MAE: 99.9529\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 9162.1846 - MAE: 94.0471 - val_loss: 9426.8428 - val_MAE: 95.7814\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 8415.8828 - MAE: 90.0792 - val_loss: 8640.6455 - val_MAE: 91.6923\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 7714.0894 - MAE: 86.1968 - val_loss: 7912.7036 - val_MAE: 87.7348\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 7058.2266 - MAE: 82.4281 - val_loss: 7241.8726 - val_MAE: 83.9195\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6456.3472 - MAE: 78.7883 - val_loss: 6620.3291 - val_MAE: 80.2214\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5901.5537 - MAE: 75.2700 - val_loss: 6044.3403 - val_MAE: 76.6329\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 5383.2715 - MAE: 71.8518 - val_loss: 5515.4189 - val_MAE: 73.1797\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4908.2583 - MAE: 68.5388 - val_loss: 5025.2153 - val_MAE: 69.8252\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4471.4692 - MAE: 65.3562 - val_loss: 4571.2944 - val_MAE: 66.5666\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4068.7363 - MAE: 62.2778 - val_loss: 4153.3486 - val_MAE: 63.4156\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 3697.7773 - MAE: 59.2799 - val_loss: 3770.5574 - val_MAE: 60.3832\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 3353.0247 - MAE: 56.4041 - val_loss: 3424.0251 - val_MAE: 57.4968\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3043.6746 - MAE: 53.6474 - val_loss: 3105.0295 - val_MAE: 54.7034\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2762.1653 - MAE: 50.9893 - val_loss: 2810.0283 - val_MAE: 51.9837\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2499.1396 - MAE: 48.4151 - val_loss: 2543.5308 - val_MAE: 49.3949\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2263.6624 - MAE: 45.9585 - val_loss: 2299.3313 - val_MAE: 46.8950\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2045.7773 - MAE: 43.5841 - val_loss: 2078.8157 - val_MAE: 44.5133\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1851.3197 - MAE: 41.3222 - val_loss: 1876.4442 - val_MAE: 42.2072\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1673.2684 - MAE: 39.1403 - val_loss: 1692.5610 - val_MAE: 39.9940\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1512.7162 - MAE: 37.0530 - val_loss: 1526.0948 - val_MAE: 37.8754\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1363.9703 - MAE: 35.0424 - val_loss: 1377.6512 - val_MAE: 35.8773\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1232.2814 - MAE: 33.1506 - val_loss: 1243.0837 - val_MAE: 33.9616\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1112.6715 - MAE: 31.3342 - val_loss: 1121.6443 - val_MAE: 32.1317\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1007.6646 - MAE: 29.6044 - val_loss: 1010.6890 - val_MAE: 30.3603\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 910.6161 - MAE: 27.9267 - val_loss: 911.4135 - val_MAE: 28.6790\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 822.7002 - MAE: 26.3367 - val_loss: 822.9377 - val_MAE: 27.0896\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 745.6392 - MAE: 24.8611 - val_loss: 743.1708 - val_MAE: 25.5686\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 675.7773 - MAE: 23.4463 - val_loss: 671.9997 - val_MAE: 24.1272\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 613.6772 - MAE: 22.1104 - val_loss: 608.3087 - val_MAE: 22.7560\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 557.8628 - MAE: 20.8472 - val_loss: 551.6161 - val_MAE: 21.4587\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 509.1971 - MAE: 19.6868 - val_loss: 500.3181 - val_MAE: 20.2093\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 464.4944 - MAE: 18.5858 - val_loss: 455.3909 - val_MAE: 19.0442\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 425.4424 - MAE: 17.5509 - val_loss: 415.1098 - val_MAE: 17.9320\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 390.6848 - MAE: 16.6054 - val_loss: 379.2854 - val_MAE: 16.8774\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 359.7486 - MAE: 15.7144 - val_loss: 347.7303 - val_MAE: 15.8869\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 332.6501 - MAE: 14.9063 - val_loss: 319.5455 - val_MAE: 14.9611\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 308.3468 - MAE: 14.1734 - val_loss: 294.8054 - val_MAE: 14.1350\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 287.0356 - MAE: 13.4922 - val_loss: 273.0050 - val_MAE: 13.3686\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 268.9075 - MAE: 12.8794 - val_loss: 253.2269 - val_MAE: 12.6887\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 251.5128 - MAE: 12.2889 - val_loss: 236.5002 - val_MAE: 12.1907\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 237.5219 - MAE: 11.8052 - val_loss: 221.3001 - val_MAE: 11.7506\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 224.5802 - MAE: 11.3762 - val_loss: 208.2226 - val_MAE: 11.4103\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 213.4693 - MAE: 11.0251 - val_loss: 196.6898 - val_MAE: 11.1178\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 203.7296 - MAE: 10.7274 - val_loss: 186.5540 - val_MAE: 10.8570\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 195.1216 - MAE: 10.4948 - val_loss: 177.7191 - val_MAE: 10.6602\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 188.0243 - MAE: 10.3258 - val_loss: 169.7148 - val_MAE: 10.4690\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 181.1848 - MAE: 10.1430 - val_loss: 163.0840 - val_MAE: 10.2992\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 175.9595 - MAE: 10.0334 - val_loss: 156.8275 - val_MAE: 10.1474\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 170.6305 - MAE: 9.9269 - val_loss: 151.6407 - val_MAE: 10.0344\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 166.6884 - MAE: 9.8489 - val_loss: 146.7298 - val_MAE: 9.9190\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 162.5018 - MAE: 9.7611 - val_loss: 142.9043 - val_MAE: 9.8223\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 159.2360 - MAE: 9.6983 - val_loss: 139.6424 - val_MAE: 9.7340\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 156.6699 - MAE: 9.6454 - val_loss: 136.5744 - val_MAE: 9.6452\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 154.2087 - MAE: 9.6065 - val_loss: 133.9231 - val_MAE: 9.5628\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 152.1353 - MAE: 9.5744 - val_loss: 131.4899 - val_MAE: 9.4874\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 150.3763 - MAE: 9.5420 - val_loss: 129.3706 - val_MAE: 9.4287\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 148.7122 - MAE: 9.5218 - val_loss: 127.6327 - val_MAE: 9.3812\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 147.1997 - MAE: 9.4904 - val_loss: 126.2571 - val_MAE: 9.3550\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 146.1177 - MAE: 9.4753 - val_loss: 124.8898 - val_MAE: 9.3279\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 145.0257 - MAE: 9.4579 - val_loss: 123.6970 - val_MAE: 9.3022\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 144.1099 - MAE: 9.4409 - val_loss: 122.6274 - val_MAE: 9.2776\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 143.2924 - MAE: 9.4331 - val_loss: 121.6625 - val_MAE: 9.2543\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 142.5523 - MAE: 9.4187 - val_loss: 120.8066 - val_MAE: 9.2318\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 141.9302 - MAE: 9.4081 - val_loss: 120.0235 - val_MAE: 9.2096\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 141.4612 - MAE: 9.4053 - val_loss: 119.3012 - val_MAE: 9.1886\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 140.8871 - MAE: 9.3947 - val_loss: 118.7962 - val_MAE: 9.1790\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 140.5072 - MAE: 9.3897 - val_loss: 118.3111 - val_MAE: 9.1691\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 140.1715 - MAE: 9.3860 - val_loss: 117.8489 - val_MAE: 9.1598\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 139.8663 - MAE: 9.3809 - val_loss: 117.4162 - val_MAE: 9.1503\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 139.5181 - MAE: 9.3728 - val_loss: 117.0660 - val_MAE: 9.1415\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 139.2892 - MAE: 9.3705 - val_loss: 116.7020 - val_MAE: 9.1326\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.9, model__n_hidden=0, model__n_neurons=25, model__optimizer=adam; total time=  10.8s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 29ms/step - loss: 11087929968301853610772267008.0000 - MAE: 28757299560448.0000 - val_loss: 591132384608027335871638324379648.0000 - val_MAE: 23797825376616448.0000\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: inf - MAE: 5531418246701752516758994944.0000 - val_loss: inf - val_MAE: 4498915887926994844818358665216.0000\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=0, model__n_neurons=125, model__optimizer=momentum; total time=   1.8s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 1s 26ms/step - loss: 2602689490587353953368801280.0000 - MAE: 13984581287936.0000 - val_loss: 122528722822916242761598143299584.0000 - val_MAE: 10866788023664640.0000\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: inf - MAE: 2102136499851434911562989568.0000 - val_loss: inf - val_MAE: 1708063938586840779957278343168.0000\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 4ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=0, model__n_neurons=125, model__optimizer=momentum; total time=   1.8s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 19ms/step - loss: 1019940254745048125487972352.0000 - MAE: 9078515433472.0000 - val_loss: 43044795126550955761336655544320.0000 - val_MAE: 6415248768630784.0000\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: inf - MAE: 1139654680056011361315979264.0000 - val_loss: inf - val_MAE: 885269252430715341395225214976.0000\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 11: early stopping\n",
            "5/5 [==============================] - 0s 2ms/step\n",
            "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=0, model__n_neurons=125, model__optimizer=momentum; total time=   1.7s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 21ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: nan - MAE: nan - val_loss: nan - val_MAE: nan\n",
            "Epoch 10: early stopping\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.1, model__n_hidden=2, model__n_neurons=25, model__optimizer=momentum; total time=   1.8s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 761, in _score\n",
            "    scores = scorer(estimator, X_test, y_test)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
            "    return estimator.score(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1100, in score\n",
            "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
            "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 790, in r2_score\n",
            "    y_true, y_pred, multioutput\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_regression.py\", line 96, in _check_reg_targets\n",
            "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 800, in check_array\n",
            "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\", line 116, in _assert_all_finite\n",
            "    type_err, msg_dtype if msg_dtype is not None else X.dtype\n",
            "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
            "\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 0s 22ms/step - loss: 61161627080196096.0000 - MAE: 109609832.0000 - val_loss: 1190103482368.0000 - val_MAE: 1090918.6250\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 1172853882880.0000 - MAE: 1082970.8750 - val_loss: 1148506996736.0000 - val_MAE: 1071684.1250\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 1131834900480.0000 - MAE: 1063864.6250 - val_loss: 1108335001600.0000 - val_MAE: 1052775.0000\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1092246110208.0000 - MAE: 1045093.4375 - val_loss: 1069567901696.0000 - val_MAE: 1034199.1250\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1054041833472.0000 - MAE: 1026653.2500 - val_loss: 1032156807168.0000 - val_MAE: 1015951.1875\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1017174097920.0000 - MAE: 1008538.4375 - val_loss: 996054597632.0000 - val_MAE: 998025.3125\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 981595848704.0000 - MAE: 990743.5000 - val_loss: 961214873600.0000 - val_MAE: 980415.6250\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 947261997056.0000 - MAE: 973262.2500 - val_loss: 927593791488.0000 - val_MAE: 963116.7500\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 914128961536.0000 - MAE: 956089.5625 - val_loss: 895148687360.0000 - val_MAE: 946122.9375\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 882154930176.0000 - MAE: 939219.8750 - val_loss: 863838404608.0000 - val_MAE: 929429.1250\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 851299270656.0000 - MAE: 922647.8750 - val_loss: 833623293952.0000 - val_MAE: 913029.6875\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 821522923520.0000 - MAE: 906368.2500 - val_loss: 804465016832.0000 - val_MAE: 896919.7500\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 792788008960.0000 - MAE: 890375.8125 - val_loss: 776326742016.0000 - val_MAE: 881094.1250\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 765058154496.0000 - MAE: 874665.6250 - val_loss: 749172621312.0000 - val_MAE: 865547.6250\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 738298298368.0000 - MAE: 859232.6875 - val_loss: 722968379392.0000 - val_MAE: 850275.5000\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 712474492928.0000 - MAE: 844072.0625 - val_loss: 697680658432.0000 - val_MAE: 835272.8125\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 687554035712.0000 - MAE: 829178.9375 - val_loss: 673277542400.0000 - val_MAE: 820534.9375\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 663505076224.0000 - MAE: 814548.5625 - val_loss: 649727836160.0000 - val_MAE: 806056.9375\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 640297336832.0000 - MAE: 800176.2500 - val_loss: 627001786368.0000 - val_MAE: 791834.4375\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 617901195264.0000 - MAE: 786057.5625 - val_loss: 605070557184.0000 - val_MAE: 777862.8125\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 596288471040.0000 - MAE: 772187.9375 - val_loss: 583906754560.0000 - val_MAE: 764137.9375\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 575431835648.0000 - MAE: 758563.2500 - val_loss: 563483181056.0000 - val_MAE: 750655.1250\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 555304681472.0000 - MAE: 745178.8125 - val_loss: 543773818880.0000 - val_MAE: 737410.2500\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 535881515008.0000 - MAE: 732030.6250 - val_loss: 524753928192.0000 - val_MAE: 724399.0625\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 517137694720.0000 - MAE: 719114.3125 - val_loss: 506399064064.0000 - val_MAE: 711617.1875\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 499049398272.0000 - MAE: 706425.9375 - val_loss: 488686354432.0000 - val_MAE: 699061.1250\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 481593917440.0000 - MAE: 693961.3750 - val_loss: 471593451520.0000 - val_MAE: 686726.5625\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 464748904448.0000 - MAE: 681716.8750 - val_loss: 455098007552.0000 - val_MAE: 674609.5625\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 448492994560.0000 - MAE: 669688.2500 - val_loss: 439179640832.0000 - val_MAE: 662706.3125\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 432805806080.0000 - MAE: 657872.0000 - val_loss: 423818100736.0000 - val_MAE: 651013.1250\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 417667317760.0000 - MAE: 646264.1250 - val_loss: 408993923072.0000 - val_MAE: 639526.3750\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 403058262016.0000 - MAE: 634861.2500 - val_loss: 394688200704.0000 - val_MAE: 628242.1250\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 388960223232.0000 - MAE: 623659.3750 - val_loss: 380882878464.0000 - val_MAE: 617157.0625\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 375355342848.0000 - MAE: 612655.2500 - val_loss: 367560491008.0000 - val_MAE: 606267.6250\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 362226253824.0000 - MAE: 601845.2500 - val_loss: 354703933440.0000 - val_MAE: 595570.3125\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 349556473856.0000 - MAE: 591226.0000 - val_loss: 342297280512.0000 - val_MAE: 585061.7500\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 337329913856.0000 - MAE: 580794.1875 - val_loss: 330324508672.0000 - val_MAE: 574738.6250\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 325530910720.0000 - MAE: 570546.3750 - val_loss: 318770577408.0000 - val_MAE: 564597.6875\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 314144718848.0000 - MAE: 560479.5000 - val_loss: 307620708352.0000 - val_MAE: 554635.6250\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 303156690944.0000 - MAE: 550590.1250 - val_loss: 296860811264.0000 - val_MAE: 544849.3125\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 292553031680.0000 - MAE: 540875.3125 - val_loss: 286477352960.0000 - val_MAE: 535235.7500\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 282320207872.0000 - MAE: 531331.7500 - val_loss: 276456931328.0000 - val_MAE: 525791.6875\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 272445259776.0000 - MAE: 521956.6875 - val_loss: 266787061760.0000 - val_MAE: 516514.3438\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 262915801088.0000 - MAE: 512747.0312 - val_loss: 257455423488.0000 - val_MAE: 507400.6250\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 253719609344.0000 - MAE: 503699.9062 - val_loss: 248450203648.0000 - val_MAE: 498447.7812\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 244845150208.0000 - MAE: 494812.4062 - val_loss: 239759982592.0000 - val_MAE: 489652.9375\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 236281036800.0000 - MAE: 486081.7188 - val_loss: 231373717504.0000 - val_MAE: 481013.2188\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 228016504832.0000 - MAE: 477505.0938 - val_loss: 223280824320.0000 - val_MAE: 472525.9688\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 220041068544.0000 - MAE: 469079.8125 - val_loss: 215470931968.0000 - val_MAE: 464188.4375\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 212344553472.0000 - MAE: 460803.0938 - val_loss: 207934259200.0000 - val_MAE: 455998.0312\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 204917243904.0000 - MAE: 452672.5000 - val_loss: 200661123072.0000 - val_MAE: 447952.1562\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 197749735424.0000 - MAE: 444685.3125 - val_loss: 193642446848.0000 - val_MAE: 440048.2188\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 190832951296.0000 - MAE: 436839.0938 - val_loss: 186869252096.0000 - val_MAE: 432283.7812\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 184158027776.0000 - MAE: 429131.2812 - val_loss: 180332937216.0000 - val_MAE: 424656.2188\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 177716609024.0000 - MAE: 421559.4688 - val_loss: 174025277440.0000 - val_MAE: 417163.3750\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 171500503040.0000 - MAE: 414121.2812 - val_loss: 167938244608.0000 - val_MAE: 409802.6562\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 165501829120.0000 - MAE: 406814.2812 - val_loss: 162064138240.0000 - val_MAE: 402571.8750\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 159712968704.0000 - MAE: 399636.3125 - val_loss: 156395487232.0000 - val_MAE: 395468.6562\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 154126598144.0000 - MAE: 392584.9688 - val_loss: 150925131776.0000 - val_MAE: 388490.8438\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 148735655936.0000 - MAE: 385658.0625 - val_loss: 145646125056.0000 - val_MAE: 381636.1562\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 143533244416.0000 - MAE: 378853.3125 - val_loss: 140551716864.0000 - val_MAE: 374902.2812\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 138512793600.0000 - MAE: 372168.6250 - val_loss: 135635542016.0000 - val_MAE: 368287.2812\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 133667954688.0000 - MAE: 365601.9375 - val_loss: 130891276288.0000 - val_MAE: 361788.9688\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 128992534528.0000 - MAE: 359151.0312 - val_loss: 126312947712.0000 - val_MAE: 355405.3438\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 124480700416.0000 - MAE: 352814.0312 - val_loss: 121894805504.0000 - val_MAE: 349134.3750\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 120126668800.0000 - MAE: 346588.8750 - val_loss: 117631205376.0000 - val_MAE: 342974.0312\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 115924951040.0000 - MAE: 340473.4688 - val_loss: 113516724224.0000 - val_MAE: 336922.4062\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 111870181376.0000 - MAE: 334466.0312 - val_loss: 109546151936.0000 - val_MAE: 330977.5625\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 107957215232.0000 - MAE: 328564.5312 - val_loss: 105714434048.0000 - val_MAE: 325137.5625\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 104181121024.0000 - MAE: 322767.1250 - val_loss: 102016786432.0000 - val_MAE: 319400.6562\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 100537139200.0000 - MAE: 317072.1250 - val_loss: 98448474112.0000 - val_MAE: 313765.0000\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 97020608512.0000 - MAE: 311477.6250 - val_loss: 95004958720.0000 - val_MAE: 308228.7500\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 93627064320.0000 - MAE: 305981.7500 - val_loss: 91681890304.0000 - val_MAE: 302790.1562\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 90352238592.0000 - MAE: 300582.9062 - val_loss: 88475058176.0000 - val_MAE: 297447.5625\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 87191937024.0000 - MAE: 295279.2812 - val_loss: 85380399104.0000 - val_MAE: 292199.2500\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 84142161920.0000 - MAE: 290069.2188 - val_loss: 82393939968.0000 - val_MAE: 287043.4375\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 81199054848.0000 - MAE: 284951.0938 - val_loss: 79511977984.0000 - val_MAE: 281978.6562\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 78358913024.0000 - MAE: 279923.2812 - val_loss: 76730802176.0000 - val_MAE: 277003.2500\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 75618099200.0000 - MAE: 274984.1875 - val_loss: 74046898176.0000 - val_MAE: 272115.5938\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 72973131776.0000 - MAE: 270132.1875 - val_loss: 71456874496.0000 - val_MAE: 267314.1562\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 70420692992.0000 - MAE: 265365.8438 - val_loss: 68957429760.0000 - val_MAE: 262597.4688\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 67957538816.0000 - MAE: 260683.5625 - val_loss: 66545438720.0000 - val_MAE: 257964.0312\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 65580539904.0000 - MAE: 256083.9531 - val_loss: 64217804800.0000 - val_MAE: 253412.3281\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 63286689792.0000 - MAE: 251565.4688 - val_loss: 61971619840.0000 - val_MAE: 248940.9844\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 61073088512.0000 - MAE: 247126.7656 - val_loss: 59803979776.0000 - val_MAE: 244548.5000\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 58936901632.0000 - MAE: 242766.3438 - val_loss: 57712156672.0000 - val_MAE: 240233.5312\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 56875433984.0000 - MAE: 238482.8750 - val_loss: 55693504512.0000 - val_MAE: 235994.7031\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 54886068224.0000 - MAE: 234274.9844 - val_loss: 53745451008.0000 - val_MAE: 231830.6719\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 52966277120.0000 - MAE: 230141.3281 - val_loss: 51865530368.0000 - val_MAE: 227740.0781\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 51113644032.0000 - MAE: 226080.5781 - val_loss: 50051383296.0000 - val_MAE: 223721.6719\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 49325826048.0000 - MAE: 222091.5312 - val_loss: 48300695552.0000 - val_MAE: 219774.1875\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 47600529408.0000 - MAE: 218172.8594 - val_loss: 46611230720.0000 - val_MAE: 215896.3281\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 45935570944.0000 - MAE: 214323.3125 - val_loss: 44980850688.0000 - val_MAE: 212086.9219\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 44328853504.0000 - MAE: 210541.6719 - val_loss: 43407515648.0000 - val_MAE: 208344.7031\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 42778337280.0000 - MAE: 206826.7812 - val_loss: 41889198080.0000 - val_MAE: 204668.5000\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 41282056192.0000 - MAE: 203177.4531 - val_loss: 40424001536.0000 - val_MAE: 201057.2188\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 39838113792.0000 - MAE: 199592.5000 - val_loss: 39010045952.0000 - val_MAE: 197509.6094\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 38444666880.0000 - MAE: 196070.7812 - val_loss: 37645549568.0000 - val_MAE: 194024.6094\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 37099962368.0000 - MAE: 192611.2031 - val_loss: 36328775680.0000 - val_MAE: 190601.0938\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 35802292224.0000 - MAE: 189212.7031 - val_loss: 35058069504.0000 - val_MAE: 187238.0000\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.1, model__n_hidden=2, model__n_neurons=25, model__optimizer=momentum; total time=   6.2s\n",
            "Epoch 1/100\n",
            "8/8 [==============================] - 1s 21ms/step - loss: 177903246121819413938176.0000 - MAE: 154580402176.0000 - val_loss: 6530667510675537920.0000 - val_MAE: 2555517184.0000\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6435409121779908608.0000 - MAE: 2536780544.0000 - val_loss: 6302253315853910016.0000 - val_MAE: 2510428928.0000\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 6210315452074164224.0000 - MAE: 2492020736.0000 - val_loss: 6081817177403424768.0000 - val_MAE: 2466134016.0000\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5993094285623296000.0000 - MAE: 2448050432.0000 - val_loss: 5869088615463649280.0000 - val_MAE: 2422620160.0000\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5783470744520359936.0000 - MAE: 2404855808.0000 - val_loss: 5663803197488103424.0000 - val_MAE: 2379874560.0000\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5581178746951434240.0000 - MAE: 2362423296.0000 - val_loss: 5465696490930307072.0000 - val_MAE: 2337882624.0000\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5385962656463060992.0000 - MAE: 2320739840.0000 - val_loss: 5274520006162382848.0000 - val_MAE: 2296632320.0000\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 5197575632694804480.0000 - MAE: 2279792128.0000 - val_loss: 5090030201358778368.0000 - val_MAE: 2256109568.0000\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 5015776332844367872.0000 - MAE: 2239566080.0000 - val_loss: 4911993980054405120.0000 - val_MAE: 2216301824.0000\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 4840338257516429312.0000 - MAE: 2200050432.0000 - val_loss: 4740184842853941248.0000 - val_MAE: 2177196544.0000\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 4671035457071480832.0000 - MAE: 2161232128.0000 - val_loss: 4574385086455087104.0000 - val_MAE: 2138781184.0000\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4507653801620013056.0000 - MAE: 2123097856.0000 - val_loss: 4414383879503216640.0000 - val_MAE: 2101043456.0000\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 4349987132731817984.0000 - MAE: 2085637248.0000 - val_loss: 4259979461614632960.0000 - val_MAE: 2063971712.0000\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4197834239779012608.0000 - MAE: 2048837248.0000 - val_loss: 4110975295085871104.0000 - val_MAE: 2027554176.0000\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 4051004632372084736.0000 - MAE: 2012686592.0000 - val_loss: 3967183913184395264.0000 - val_MAE: 1991778944.0000\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3909310294022684672.0000 - MAE: 1977174016.0000 - val_loss: 3828421972346273792.0000 - val_MAE: 1956635264.0000\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 3772572828969205760.0000 - MAE: 1942288000.0000 - val_loss: 3694512176321527808.0000 - val_MAE: 1922111360.0000\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 3640617414862831616.0000 - MAE: 1908017280.0000 - val_loss: 3565288223976456192.0000 - val_MAE: 1888196992.0000\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 3513277475691954176.0000 - MAE: 1874351488.0000 - val_loss: 3440582714665730048.0000 - val_MAE: 1854880768.0000\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3390391658125197312.0000 - MAE: 1841279616.0000 - val_loss: 3320239792616112128.0000 - val_MAE: 1822152320.0000\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3271804106389323776.0000 - MAE: 1808791168.0000 - val_loss: 3204105801077620736.0000 - val_MAE: 1790001664.0000\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 3157364462269235200.0000 - MAE: 1776876160.0000 - val_loss: 3092034230125854720.0000 - val_MAE: 1758418048.0000\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 3046927040474251264.0000 - MAE: 1745524096.0000 - val_loss: 2983881593493389312.0000 - val_MAE: 1727391488.0000\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2940352752783458304.0000 - MAE: 1714725120.0000 - val_loss: 2879512926127915008.0000 - val_MAE: 1696912768.0000\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2837506084388732928.0000 - MAE: 1684469760.0000 - val_loss: 2778793812732936192.0000 - val_MAE: 1666971392.0000\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2738257018040090624.0000 - MAE: 1654748160.0000 - val_loss: 2681598634104979456.0000 - val_MAE: 1637558784.0000\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2642479109900337152.0000 - MAE: 1625550848.0000 - val_loss: 2587802320796385280.0000 - val_MAE: 1608664704.0000\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2550051688568324096.0000 - MAE: 1596868992.0000 - val_loss: 2497287499940888576.0000 - val_MAE: 1580280832.0000\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2460857381177786368.0000 - MAE: 1568692992.0000 - val_loss: 2409938722817572864.0000 - val_MAE: 1552397696.0000\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2374782388275249152.0000 - MAE: 1541014272.0000 - val_loss: 2325645213629939712.0000 - val_MAE: 1525006592.0000\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2291718545404329984.0000 - MAE: 1513824256.0000 - val_loss: 2244300044872187904.0000 - val_MAE: 1498098816.0000\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2211560162009808896.0000 - MAE: 1487113728.0000 - val_loss: 2165799862451306496.0000 - val_MAE: 1471665664.0000\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 2134205120949256192.0000 - MAE: 1460874496.0000 - val_loss: 2090045572881842176.0000 - val_MAE: 1445698944.0000\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 2059555840565706752.0000 - MAE: 1435097984.0000 - val_loss: 2016940968896364544.0000 - val_MAE: 1420190464.0000\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1987517900298125312.0000 - MAE: 1409776768.0000 - val_loss: 1946393416640233472.0000 - val_MAE: 1395132032.0000\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1917999628364546048.0000 - MAE: 1384902016.0000 - val_loss: 1878313305915785216.0000 - val_MAE: 1370515712.0000\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1850912101762072576.0000 - MAE: 1360466048.0000 - val_loss: 1812614050182332416.0000 - val_MAE: 1346333568.0000\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1786171482729086976.0000 - MAE: 1336461184.0000 - val_loss: 1749213048628838400.0000 - val_MAE: 1322578176.0000\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1723695582771412992.0000 - MAE: 1312880128.0000 - val_loss: 1688029899467522048.0000 - val_MAE: 1299241984.0000\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1663404549857083392.0000 - MAE: 1289714944.0000 - val_loss: 1628986262494904320.0000 - val_MAE: 1276317440.0000\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1605222517683781632.0000 - MAE: 1266958720.0000 - val_loss: 1572008195554017280.0000 - val_MAE: 1253797632.0000\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1549075544094539776.0000 - MAE: 1244603776.0000 - val_loss: 1517022855999520768.0000 - val_MAE: 1231674880.0000\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1494892435711459328.0000 - MAE: 1222643328.0000 - val_loss: 1463960974598864896.0000 - val_MAE: 1209942528.0000\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1442604747935711232.0000 - MAE: 1201070464.0000 - val_loss: 1412755481142755328.0000 - val_MAE: 1188593920.0000\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1392146097752768512.0000 - MAE: 1179878272.0000 - val_loss: 1363340542372478976.0000 - val_MAE: 1167621888.0000\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 1343452301171359744.0000 - MAE: 1159060096.0000 - val_loss: 1315654310758973440.0000 - val_MAE: 1147019776.0000\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1296461922979282944.0000 - MAE: 1138609152.0000 - val_loss: 1269636175723757568.0000 - val_MAE: 1126781184.0000\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1251114627475963904.0000 - MAE: 1118518912.0000 - val_loss: 1225226626199977984.0000 - val_MAE: 1106899584.0000\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1207353240056758272.0000 - MAE: 1098783104.0000 - val_loss: 1182371098923106304.0000 - val_MAE: 1087368960.0000\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1165122647701323776.0000 - MAE: 1079395456.0000 - val_loss: 1141014549592276992.0000 - val_MAE: 1068182848.0000\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1124369524095713280.0000 - MAE: 1060350208.0000 - val_loss: 1101104888844124160.0000 - val_MAE: 1049335424.0000\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1085041986034991104.0000 - MAE: 1041640960.0000 - val_loss: 1062590920668479488.0000 - val_MAE: 1030820544.0000\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 1047089799581663232.0000 - MAE: 1023261760.0000 - val_loss: 1025424197834244096.0000 - val_MAE: 1012632320.0000\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 1010465342138351616.0000 - MAE: 1005207040.0000 - val_loss: 989557372621946880.0000 - val_MAE: 994764928.0000\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 975121609582968832.0000 - MAE: 987470656.0000 - val_loss: 954944884018511872.0000 - val_MAE: 977212864.0000\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 941014209133543424.0000 - MAE: 970047232.0000 - val_loss: 921543232595165184.0000 - val_MAE: 959970496.0000\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 908099641361301504.0000 - MAE: 952931200.0000 - val_loss: 889309743556853760.0000 - val_MAE: 943032256.0000\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 876336605860724736.0000 - MAE: 936117312.0000 - val_loss: 858204009851256832.0000 - val_MAE: 926393024.0000\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 845684558140538880.0000 - MAE: 919600064.0000 - val_loss: 828186105462390784.0000 - val_MAE: 910047360.0000\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 816104602976911360.0000 - MAE: 903374272.0000 - val_loss: 799218234678050816.0000 - val_MAE: 893990080.0000\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 787559219535544320.0000 - MAE: 887434688.0000 - val_loss: 771263563858706432.0000 - val_MAE: 878216256.0000\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 760012467530104832.0000 - MAE: 871776448.0000 - val_loss: 744286702473838592.0000 - val_MAE: 862720512.0000\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 733429025149550592.0000 - MAE: 856394432.0000 - val_loss: 718253290785079296.0000 - val_MAE: 847498176.0000\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 707775494728187904.0000 - MAE: 841283840.0000 - val_loss: 693130687040978944.0000 - val_MAE: 832544704.0000\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 683019165795090432.0000 - MAE: 826439744.0000 - val_loss: 668886524367994880.0000 - val_MAE: 817854848.0000\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 659128702268866560.0000 - MAE: 811857664.0000 - val_loss: 645490360037933056.0000 - val_MAE: 803424192.0000\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 636073936299229184.0000 - MAE: 797532800.0000 - val_loss: 622912575956320256.0000 - val_MAE: 789248128.0000\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 613825524669612032.0000 - MAE: 783460736.0000 - val_loss: 601124516101357568.0000 - val_MAE: 775322304.0000\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 592355223675076608.0000 - MAE: 769636992.0000 - val_loss: 580098692682350592.0000 - val_MAE: 761642176.0000\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 571636198359957504.0000 - MAE: 756057216.0000 - val_loss: 559808202024157184.0000 - val_MAE: 748203328.0000\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 551641716847804416.0000 - MAE: 742716928.0000 - val_loss: 540227514841169920.0000 - val_MAE: 735001792.0000\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 532346730889347072.0000 - MAE: 729612224.0000 - val_loss: 521331685963333632.0000 - val_MAE: 722033024.0000\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 513726501472960512.0000 - MAE: 716738496.0000 - val_loss: 503096869732220928.0000 - val_MAE: 709293184.0000\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 495757663976554496.0000 - MAE: 704092096.0000 - val_loss: 485499632806264832.0000 - val_MAE: 696778048.0000\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 478417094296207360.0000 - MAE: 691668672.0000 - val_loss: 468517950593171456.0000 - val_MAE: 684483712.0000\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 461683111437008896.0000 - MAE: 679464576.0000 - val_loss: 452130245177245696.0000 - val_MAE: 672406336.0000\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 445534549800124416.0000 - MAE: 667475712.0000 - val_loss: 436315900715466752.0000 - val_MAE: 660542144.0000\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 429950965341224960.0000 - MAE: 655698624.0000 - val_loss: 421054610602459136.0000 - val_MAE: 648887168.0000\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 414912360692580352.0000 - MAE: 644129088.0000 - val_loss: 406327305183428608.0000 - val_MAE: 637438016.0000\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 400399769278611456.0000 - MAE: 632763904.0000 - val_loss: 392114880443842560.0000 - val_MAE: 626190784.0000\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 386394705560076288.0000 - MAE: 621599104.0000 - val_loss: 378399709837918208.0000 - val_MAE: 615141952.0000\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 372879611710668800.0000 - MAE: 610631424.0000 - val_loss: 365164235539349504.0000 - val_MAE: 604288192.0000\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 359837204781989888.0000 - MAE: 599857152.0000 - val_loss: 352391724355551232.0000 - val_MAE: 593625920.0000\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 347251026459361280.0000 - MAE: 589273024.0000 - val_loss: 340065924130275328.0000 - val_MAE: 583151680.0000\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 335105065104703488.0000 - MAE: 578875712.0000 - val_loss: 328171304261779456.0000 - val_MAE: 572862400.0000\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 323383893195489280.0000 - MAE: 568661760.0000 - val_loss: 316692609026228224.0000 - val_MAE: 562754496.0000\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 312072667324743680.0000 - MAE: 558627968.0000 - val_loss: 305615441693245440.0000 - val_MAE: 552824896.0000\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 301157128201043968.0000 - MAE: 548771200.0000 - val_loss: 294925817849315328.0000 - val_MAE: 543070720.0000\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 290623428849827840.0000 - MAE: 539088576.0000 - val_loss: 284610010778959872.0000 - val_MAE: 533488512.0000\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 280458083073785856.0000 - MAE: 529576640.0000 - val_loss: 274655066860814336.0000 - val_MAE: 524075424.0000\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 270648394949591040.0000 - MAE: 520232576.0000 - val_loss: 265048307351420928.0000 - val_MAE: 514828448.0000\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 261181805992869888.0000 - MAE: 511053408.0000 - val_loss: 255777551723528192.0000 - val_MAE: 505744576.0000\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 252046307475062784.0000 - MAE: 502036064.0000 - val_loss: 246831048946614272.0000 - val_MAE: 496820992.0000\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 243230320164339712.0000 - MAE: 493177952.0000 - val_loss: 238197529026494464.0000 - val_MAE: 488054848.0000\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 234722694325600256.0000 - MAE: 484476064.0000 - val_loss: 229865910947545088.0000 - val_MAE: 479443360.0000\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 226512641000996864.0000 - MAE: 475927744.0000 - val_loss: 221825800888909824.0000 - val_MAE: 470983872.0000\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 218589783549542400.0000 - MAE: 467530240.0000 - val_loss: 214066890929078272.0000 - val_MAE: 462673664.0000\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 210944088927633408.0000 - MAE: 459280992.0000 - val_loss: 206579405722484736.0000 - val_MAE: 454510048.0000\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 203565816149442560.0000 - MAE: 451177312.0000 - val_loss: 199353741722255360.0000 - val_MAE: 446490464.0000\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 6ms/step - loss: 196445550646657024.0000 - MAE: 443216544.0000 - val_loss: 192380810777591808.0000 - val_MAE: 438612352.0000\n",
            "5/5 [==============================] - 0s 3ms/step\n",
            "[CV] END model__learning_rate=0.001, model__momentum=0.1, model__n_hidden=2, model__n_neurons=25, model__optimizer=momentum; total time=   6.3s\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [            nan -1.61242174e+01 -3.65112926e+00 -1.55598281e+00\n",
            "             nan  3.83550266e-01             nan             nan\n",
            " -2.17883474e+02             nan             nan  1.15425323e-01\n",
            " -1.72328740e+02             nan -5.20974478e+00             nan\n",
            "             nan -1.77950869e+00             nan             nan\n",
            "             nan             nan             nan -3.34794787e+02\n",
            " -3.90162278e-01  4.74257241e-01 -1.10358453e+01 -1.61519724e+00\n",
            "             nan             nan]\n",
            "  category=UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 1s 16ms/step - loss: 190.7958 - MAE: 10.9464 - val_loss: 126.3142 - val_MAE: 10.1553\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 110.0656 - MAE: 8.0743 - val_loss: 35.3425 - val_MAE: 4.9979\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 91.9119 - MAE: 7.0951 - val_loss: 32.1051 - val_MAE: 4.6004\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 75.5544 - MAE: 5.9281 - val_loss: 24.1520 - val_MAE: 3.9960\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 65.2266 - MAE: 5.6695 - val_loss: 25.4245 - val_MAE: 4.2178\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 59.7797 - MAE: 5.4925 - val_loss: 37.4658 - val_MAE: 5.0029\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 63.4842 - MAE: 5.6391 - val_loss: 19.3956 - val_MAE: 3.6097\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 64.3277 - MAE: 5.6556 - val_loss: 26.7792 - val_MAE: 4.3373\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 61.7723 - MAE: 5.6430 - val_loss: 35.9751 - val_MAE: 4.8825\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 60.9072 - MAE: 5.3122 - val_loss: 18.4716 - val_MAE: 3.5768\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 56.5990 - MAE: 5.2910 - val_loss: 17.9743 - val_MAE: 3.5184\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 52.1926 - MAE: 4.9613 - val_loss: 21.3483 - val_MAE: 3.8457\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 58.1337 - MAE: 5.5091 - val_loss: 19.8131 - val_MAE: 3.5080\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 48.8520 - MAE: 4.7882 - val_loss: 17.7143 - val_MAE: 3.5768\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 46.0300 - MAE: 4.7284 - val_loss: 17.5667 - val_MAE: 3.4349\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 44.1975 - MAE: 4.6232 - val_loss: 17.4972 - val_MAE: 3.4873\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 44.5980 - MAE: 4.5895 - val_loss: 49.4096 - val_MAE: 6.0391\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 54.2795 - MAE: 5.7458 - val_loss: 35.4223 - val_MAE: 4.7150\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 49.1245 - MAE: 4.9350 - val_loss: 20.6072 - val_MAE: 3.8543\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 41.4768 - MAE: 4.3712 - val_loss: 18.6045 - val_MAE: 3.6927\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 41.5671 - MAE: 4.6543 - val_loss: 27.4417 - val_MAE: 4.2939\n",
            "Epoch 21: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3,\n",
              "                   estimator=KerasRegressor(callbacks=[<keras.callbacks.EarlyStopping object at 0x7fdb6c6dc790>], model=<function build_model at 0x7fdb6cbbaef0>),\n",
              "                   n_iter=30,\n",
              "                   param_distributions={'model__learning_rate': [1e-05, 0.0001,\n",
              "                                                                 0.001],\n",
              "                                        'model__momentum': [0.1, 0.5, 0.9],\n",
              "                                        'model__n_hidden': [0, 1, 2, 3],\n",
              "                                        'model__n_neurons': [5, 25, 125],\n",
              "                                        'model__optimizer': ['sgd', 'nesterov',\n",
              "                                                             'momentum',\n",
              "                                                             'adam']},\n",
              "                   verbose=2)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnd_search_cv.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jub_QULpgVyO",
        "outputId": "136049ae-4c96-44e2-ac40-135ec48a85ff"
      },
      "id": "Jub_QULpgVyO",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model__learning_rate': 0.001,\n",
              " 'model__momentum': 0.1,\n",
              " 'model__n_hidden': 3,\n",
              " 'model__n_neurons': 125,\n",
              " 'model__optimizer': 'adam'}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(rnd_search_cv.best_params_, open(\"rnd_search.pkl\", \"wb\"))"
      ],
      "metadata": {
        "id": "xESBU_AqlygN"
      },
      "id": "xESBU_AqlygN",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "name": "lab11.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}